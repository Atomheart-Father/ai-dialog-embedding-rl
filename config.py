"""
é¡¹ç›®é…ç½®æ–‡ä»¶
"""
import torch
from dataclasses import dataclass
from typing import Optional

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    # æ¨¡å‹ç›¸å…³
    model_name: str = "Qwen/Qwen2.5-0.5B-Instruct"  # ç›®å‰ä½¿ç”¨å¯ç”¨æ¨¡å‹ï¼Œåç»­æ›¿æ¢ä¸ºQwen3-0.6B
    model_cache_dir: str = "./models"
    use_quantization: bool = True
    quantization_bits: int = 4  # 4bité‡åŒ–èŠ‚çœå†…å­˜
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"  # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    use_mps: bool = True  # M4 Proä½¿ç”¨MPSåŠ é€Ÿ
    
    # æ¨ç†å‚æ•°
    max_length: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True
    
    # å‹ç¼©ç›¸å…³
    max_history_tokens: int = 1500  # å†å²æœ€å¤§tokenæ•°
    compression_ratio: float = 0.3  # å‹ç¼©æ¯”ä¾‹
    min_compression_length: int = 200  # æœ€å°å‹ç¼©é•¿åº¦

@dataclass 
class DialogConfig:
    """å¯¹è¯é…ç½®"""
    # ç§»é™¤æœ€å¤§å¯¹è¯è½®æ•°é™åˆ¶ - å®ç°æ— è½®æ¬¡æ•°é™åˆ¶çš„å¯¹è¯
    save_logs: bool = True
    log_dir: str = "./logs"
    
    # å‹ç¼©è§¦å‘æ¡ä»¶
    trigger_compression_tokens: int = 1200  # è§¦å‘å‹ç¼©çš„tokenæ•°
    keep_recent_turns: int = 3  # ä¿ç•™æœ€è¿‘å‡ è½®å¯¹è¯ä¸å‹ç¼©

@dataclass
class RLConfig:
    """å¼ºåŒ–å­¦ä¹ é…ç½®"""
    # è®­ç»ƒç›¸å…³
    enable_rl_training: bool = True
    training_episodes: int = 1000
    learning_rate: float = 1e-4
    discount_factor: float = 0.95
    
    # å¥–åŠ±æƒé‡
    quality_reward_weight: float = 0.4  # å›ç­”è´¨é‡å¥–åŠ±æƒé‡
    compression_reward_weight: float = 0.3  # å‹ç¼©æ•ˆç‡å¥–åŠ±æƒé‡
    coherence_reward_weight: float = 0.3  # è¿è´¯æ€§å¥–åŠ±æƒé‡
    
    # è¯„ä¼°ç›¸å…³
    evaluation_interval: int = 100  # æ¯Nä¸ªepisodeè¯„ä¼°ä¸€æ¬¡
    save_checkpoint_interval: int = 500  # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
    
    # æ¢ç´¢ç›¸å…³
    epsilon_start: float = 1.0  # åˆå§‹æ¢ç´¢ç‡
    epsilon_end: float = 0.1   # æœ€ç»ˆæ¢ç´¢ç‡
    epsilon_decay: int = 500   # æ¢ç´¢ç‡è¡°å‡æ­¥æ•°

def get_device():
    """è·å–æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

# å…¨å±€é…ç½®å®ä¾‹
model_config = ModelConfig()
dialog_config = DialogConfig()
rl_config = RLConfig()

# æ›´æ–°è®¾å¤‡é…ç½®
if model_config.device == "auto":
    model_config.device = get_device()
    
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {model_config.device}")
print(f"ğŸ“± æ¨¡å‹: {model_config.model_name}")
print(f"ğŸ”§ é‡åŒ–: {'å¯ç”¨' if model_config.use_quantization else 'ç¦ç”¨'}")
print(f"ğŸ¤– RLè®­ç»ƒ: {'å¯ç”¨' if rl_config.enable_rl_training else 'ç¦ç”¨'}") 