"""
åŸºäºEmbeddingçš„ä¸Šä¸‹æ–‡å‹ç¼©å™¨
ä½¿ç”¨æ¨¡å‹å†…éƒ¨å‘é‡è¡¨ç¤ºæ¥å‹ç¼©å’Œè¡¨ç¤ºå†å²å¯¹è¯
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional
import json
import logging
from datetime import datetime
try:
    from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
    from sklearn.cluster import KMeans  # type: ignore
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    cosine_similarity = None
    KMeans = None
    print("Warning: sklearn not available. Some clustering features will be disabled.")

from config import model_config, dialog_config
from models import model_manager

logger = logging.getLogger(__name__)

class EmbeddingCompressor:
    """åŸºäºEmbeddingçš„å†å²å‹ç¼©å™¨"""
    
    def __init__(self):
        self.embedding_dim = 768  # Qwenæ¨¡å‹çš„éšå±‚ç»´åº¦
        self.max_history_embeddings = 20  # æœ€å¤§ä¿å­˜çš„å†å²embeddingæ•°é‡
        self.similarity_threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # å†å²embeddingå­˜å‚¨
        self.history_embeddings = []  # List of (embedding, metadata)
        self.current_session_embeddings = []
        
        # ç‰¹æ®Štokenå®šä¹‰
        self.history_summary_token = "<HIST_EMB>"
        self.context_separator_token = "<CTX_SEP>"
        
        logger.info("ğŸ§  Embeddingå‹ç¼©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_text_embedding(self, text: str, use_pooling: str = 'mean') -> torch.Tensor:
        """æå–æ–‡æœ¬çš„embeddingè¡¨ç¤º"""
        if not model_manager.dialog_model or not model_manager.tokenizer:
            logger.warning("æ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›é›¶å‘é‡")
            return torch.zeros(self.embedding_dim)
        
        try:
            # Tokenizeè¾“å…¥
            tokenizer = model_manager.tokenizer
            inputs = tokenizer(  # type: ignore
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # è·å–æ¨¡å‹è¾“å‡º
            with torch.no_grad():
                model = model_manager.dialog_model
                outputs = model(  # type: ignore
                    **inputs,
                    output_hidden_states=True
                )
                
                # è·å–æœ€åä¸€å±‚éšçŠ¶æ€
                last_hidden_state = outputs.hidden_states[-1]  # [batch, seq_len, hidden_dim]
                
                # åº”ç”¨ä¸åŒçš„poolingç­–ç•¥
                if use_pooling == 'mean':
                    # å¹³å‡pooling
                    embedding = last_hidden_state.mean(dim=1)  # [batch, hidden_dim]
                elif use_pooling == 'cls':
                    # ä½¿ç”¨[CLS] token (ç¬¬ä¸€ä¸ªtoken)
                    embedding = last_hidden_state[:, 0, :]
                elif use_pooling == 'max':
                    # æœ€å¤§å€¼pooling
                    embedding = last_hidden_state.max(dim=1)[0]
                else:
                    embedding = last_hidden_state.mean(dim=1)
                
                return embedding.squeeze(0)  # ç§»é™¤batchç»´åº¦
                
        except Exception as e:
            logger.error(f"æå–embeddingå¤±è´¥: {e}")
            return torch.zeros(self.embedding_dim)
    
    def compress_dialog_turn(self, user_input: str, assistant_response: str) -> Dict:
        """å‹ç¼©å•è½®å¯¹è¯ä¸ºembedding"""
        # æ„å»ºå¯¹è¯æ–‡æœ¬
        dialog_text = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹: {assistant_response}"
        
        # æå–embedding
        embedding = self.extract_text_embedding(dialog_text)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'user_input': user_input,
            'assistant_response': assistant_response,
            'timestamp': datetime.now().isoformat(),
            'token_count': len(user_input) + len(assistant_response),
            'turn_summary': user_input[:50] + "..." if len(user_input) > 50 else user_input
        }
        
        # å­˜å‚¨åˆ°å½“å‰ä¼šè¯
        embedding_data = {
            'embedding': embedding,
            'metadata': metadata
        }
        
        self.current_session_embeddings.append(embedding_data)
        
        return embedding_data
    
    def compress_history_to_embeddings(self, history: List[Dict]) -> List[Dict]:
        """å°†å®Œæ•´å†å²å‹ç¼©ä¸ºembeddingåˆ—è¡¨"""
        compressed_embeddings = []
        
        # æŒ‰å¯¹è¯è½®æ¬¡å¤„ç†
        for i in range(0, len(history), 2):
            if i + 1 < len(history):
                user_turn = history[i]
                assistant_turn = history[i + 1]
                
                if user_turn['role'] == 'user' and assistant_turn['role'] == 'assistant':
                    embedding_data = self.compress_dialog_turn(
                        user_turn['content'],
                        assistant_turn['content']
                    )
                    compressed_embeddings.append(embedding_data)
        
        return compressed_embeddings
    
    def retrieve_relevant_embeddings(self, query_text: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å†å²embedding"""
        if not self.history_embeddings:
            return []
        
        # è·å–æŸ¥è¯¢çš„embedding
        query_embedding = self.extract_text_embedding(query_text)
        query_np = query_embedding.detach().cpu().numpy().reshape(1, -1)
        
        # è®¡ç®—ä¸æ‰€æœ‰å†å²embeddingçš„ç›¸ä¼¼åº¦
        similarities = []
        for hist_data in self.history_embeddings:
            hist_embedding = hist_data['embedding'].detach().cpu().numpy().reshape(1, -1)
            if SKLEARN_AVAILABLE and cosine_similarity is not None:
                similarity = cosine_similarity(query_np, hist_embedding)[0][0]
            else:
                # ä½¿ç”¨PyTorchçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºfallback
                query_tensor = torch.from_numpy(query_np[0])
                hist_tensor = torch.from_numpy(hist_embedding[0])
                similarity = torch.nn.functional.cosine_similarity(
                    query_tensor.unsqueeze(0), hist_tensor.unsqueeze(0)
                ).item()
            similarities.append((similarity, hist_data))
        
        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(key=lambda x: x[0], reverse=True)
        
        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        relevant = [
            data for sim, data in similarities[:top_k] 
            if sim > self.similarity_threshold
        ]
        
        logger.info(f"æ£€ç´¢åˆ° {len(relevant)} ä¸ªç›¸å…³çš„å†å²embedding")
        return relevant
    
    def cluster_embeddings(self, embeddings: List[Dict], n_clusters: int = 3) -> Dict:
        """å¯¹embeddingè¿›è¡Œèšç±»ä»¥å‘ç°ä¸»é¢˜"""
        if not SKLEARN_AVAILABLE or KMeans is None:
            logger.warning("sklearnæœªå®‰è£…ï¼Œè·³è¿‡èšç±»åˆ†æ")
            return {'clusters': [], 'centroids': []}
            
        if len(embeddings) < n_clusters:
            return {'clusters': [], 'centroids': []}
        
        # æå–embeddingçŸ©é˜µ
        embedding_matrix = np.stack([
            emb['embedding'].detach().cpu().numpy() 
            for emb in embeddings
        ])
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embedding_matrix)
        
        # ç»„ç»‡èšç±»ç»“æœ
        clusters = {}
        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(embeddings[i])
        
        return {
            'clusters': clusters,
            'centroids': kmeans.cluster_centers_,
            'labels': cluster_labels
        }
    
    def embedding_to_natural_language(self, embeddings: List[Dict]) -> str:
        """å°†embeddingè½¬æ¢å›è‡ªç„¶è¯­è¨€æè¿°"""
        if not embeddings:
            return ""
        
        # æ–¹æ¡ˆ1: ç›´æ¥ä½¿ç”¨metadataä¸­çš„æ‘˜è¦
        summaries = []
        for emb_data in embeddings:
            metadata = emb_data['metadata']
            summary = f"- {metadata['turn_summary']}"
            summaries.append(summary)
        
        # æ–¹æ¡ˆ2: åŸºäºèšç±»çš„ä¸»é¢˜æ‘˜è¦
        if len(embeddings) > 3:
            cluster_result = self.cluster_embeddings(embeddings, n_clusters=min(3, len(embeddings)))
            topic_summaries = []
            
            for cluster_id, cluster_items in cluster_result['clusters'].items():
                topics = [item['metadata']['turn_summary'] for item in cluster_items]
                topic_summary = f"ä¸»é¢˜{cluster_id + 1}: " + "; ".join(topics[:2])
                topic_summaries.append(topic_summary)
            
            return "\n".join(topic_summaries)
        
        return "\n".join(summaries)
    
    def generate_context_with_embeddings(self, current_input: str, max_context_length: int = 1500) -> str:
        """ç”ŸæˆåŒ…å«embeddingä¿¡æ¯çš„ä¸Šä¸‹æ–‡"""
        # æ£€ç´¢ç›¸å…³å†å²
        relevant_embeddings = self.retrieve_relevant_embeddings(current_input, top_k=5)
        
        if not relevant_embeddings:
            return current_input
        
        # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
        history_summary = self.embedding_to_natural_language(relevant_embeddings)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = f"""ç›¸å…³å†å²æ‘˜è¦:
{history_summary}

å½“å‰å¯¹è¯:
ç”¨æˆ·: {current_input}
åŠ©æ‰‹:"""
        
        # æ£€æŸ¥é•¿åº¦é™åˆ¶
        if model_manager.count_tokens(context) > max_context_length:
            # æˆªæ–­å†å²æ‘˜è¦
            lines = history_summary.split('\n')
            truncated_summary = '\n'.join(lines[:3])  # åªä¿ç•™å‰3è¡Œ
            context = f"""ç›¸å…³å†å²æ‘˜è¦:
{truncated_summary}

å½“å‰å¯¹è¯:
ç”¨æˆ·: {current_input}
åŠ©æ‰‹:"""
        
        return context
    
    def update_history_embeddings(self):
        """æ›´æ–°å†å²embeddingå­˜å‚¨"""
        # å°†å½“å‰ä¼šè¯çš„embeddingåˆå¹¶åˆ°å†å²ä¸­
        self.history_embeddings.extend(self.current_session_embeddings)
        
        # ä¿æŒæœ€å¤§æ•°é‡é™åˆ¶
        if len(self.history_embeddings) > self.max_history_embeddings:
            # ç§»é™¤æœ€æ—§çš„embedding
            self.history_embeddings = self.history_embeddings[-self.max_history_embeddings:]
        
        # æ¸…ç©ºå½“å‰ä¼šè¯
        self.current_session_embeddings = []
        
        logger.info(f"å†å²embeddingå·²æ›´æ–°ï¼Œå½“å‰å­˜å‚¨ {len(self.history_embeddings)} ä¸ª")
    
    def get_compression_stats(self) -> Dict:
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        if not self.history_embeddings:
            return {}
        
        # è®¡ç®—åŸå§‹tokenæ•°
        original_tokens = sum(
            emb['metadata']['token_count'] 
            for emb in self.history_embeddings
        )
        
        # embeddingå ç”¨çš„"ç­‰æ•ˆtokenæ•°" (å‡è®¾1ä¸ªembedding = 10ä¸ªtoken)
        embedding_equivalent_tokens = len(self.history_embeddings) * 10
        
        compression_ratio = embedding_equivalent_tokens / original_tokens if original_tokens > 0 else 0
        
        return {
            'total_embeddings': len(self.history_embeddings),
            'original_tokens': original_tokens,
            'embedding_equivalent_tokens': embedding_equivalent_tokens,
            'compression_ratio': compression_ratio,
            'memory_efficiency': f"{compression_ratio:.1%}",
            'average_similarity': self._calculate_average_similarity()
        }
    
    def _calculate_average_similarity(self) -> float:
        """è®¡ç®—å†å²embeddingä¹‹é—´çš„å¹³å‡ç›¸ä¼¼åº¦"""
        if len(self.history_embeddings) < 2:
            return 0.0
        
        embeddings = [emb['embedding'].detach().cpu().numpy() for emb in self.history_embeddings]
        embedding_matrix = np.stack(embeddings)
        
        # è®¡ç®—æ‰€æœ‰pairçš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(embedding_matrix)):
            for j in range(i + 1, len(embedding_matrix)):
                if SKLEARN_AVAILABLE and cosine_similarity is not None:
                    sim = cosine_similarity(
                        embedding_matrix[i:i+1], 
                        embedding_matrix[j:j+1]
                    )[0][0]
                else:
                    # ä½¿ç”¨PyTorchçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºfallback
                    vec_i = torch.from_numpy(embedding_matrix[i])
                    vec_j = torch.from_numpy(embedding_matrix[j])
                    sim = torch.nn.functional.cosine_similarity(
                        vec_i.unsqueeze(0), vec_j.unsqueeze(0)
                    ).item()
                similarities.append(sim)
        
        return float(np.mean(similarities)) if similarities else 0.0
    
    def save_embeddings(self, filepath: str):
        """ä¿å­˜embeddingåˆ°æ–‡ä»¶"""
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_data = []
        for emb_data in self.history_embeddings:
            serializable_data.append({
                'embedding': emb_data['embedding'].detach().cpu().numpy().tolist(),
                'metadata': emb_data['metadata']
            })
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Embeddingå·²ä¿å­˜åˆ° {filepath}")
    
    def load_embeddings(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½embedding"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è½¬æ¢å›tensoræ ¼å¼
            self.history_embeddings = []
            for item in data:
                embedding_tensor = torch.tensor(item['embedding'])
                self.history_embeddings.append({
                    'embedding': embedding_tensor,
                    'metadata': item['metadata']
                })
            
            logger.info(f"ä» {filepath} åŠ è½½äº† {len(self.history_embeddings)} ä¸ªembedding")
        
        except Exception as e:
            logger.error(f"åŠ è½½embeddingå¤±è´¥: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
embedding_compressor = EmbeddingCompressor() 