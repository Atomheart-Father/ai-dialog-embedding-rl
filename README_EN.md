# ğŸ¤– Reinforcement Learning Dialog System

An intelligent multi-turn dialog system based on reinforcement learning that jointly optimizes historical compression and dialog generation.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/transformers/)

**Language**: [ğŸ‡¨ğŸ‡³ ä¸­æ–‡](README.md) | [ğŸ‡ºğŸ‡¸ English](README_EN.md)

## âœ¨ Key Features

### ğŸ¯ New: Direct Embedding Compression Solution
- ğŸ§  **Direct Model Internal States**: Extract hidden states for context compression
- ğŸ’¾ **Smart State Bank**: Store and retrieve vector representations of historical dialogs
- ğŸ”§ **Four Fusion Strategies**: attention, weighted_sum, concatenation, interpolation
- âš¡ **Zero Dependencies**: No additional embedding models, directly utilize main model
- ğŸ“ **Efficient Compression**: Fixed-length vectors vs variable-length text

### ğŸ“‹ Legacy Features  
- ğŸ§  **Smart Historical Compression**: Adaptive compression strategies based on reinforcement learning
- ğŸ¯ **Multi-Strategy Compression**: Supports 4 compression strategies and dynamic compression ratios
- ğŸ† **Multi-Dimensional Reward System**: Comprehensive optimization of quality + compression + coherence
- ğŸ“Š **Complete Experimental Framework**: Scientific evaluation system with four control groups
- ğŸš€ **MPS/CUDA Acceleration**: Supports Apple Silicon and NVIDIA GPU

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the project
git clone https://github.com/Atomheart-Father/ai-dialog-embedding-rl.git
cd ai-dialog-embedding-rl

# Install dependencies
pip install -r requirements.txt
```

### 2. ğŸ¯ Recommended: Experience Direct Embedding Compression

**This is the core solution designed based on your requirements: directly using large model internal states!**

```bash
# Direct Embedding compression demo (Recommended!)
python quick_start_direct_embedding.py

# Complete testing of all features
python test_direct_embedding.py

# Jupyter experimental comparison
jupyter notebook rl_experiment.ipynb
```

### 3. ğŸ“‹ Traditional RL Solution Experience

```bash
# Quick dialog experience
python quick_start.py

# Complete system demonstration
python main.py

# Reinforcement learning training
python rl_main.py
```

## ğŸ—ï¸ System Architecture

### ğŸ¯ Direct Embedding Compression Architecture
```
ğŸ“¦ Core Components
â”œâ”€â”€ ğŸ§  Direct Embedding Compressor  # Direct embedding compressor
â”‚   â”œâ”€â”€ ğŸ¦ Hidden State Bank        # State bank
â”‚   â”œâ”€â”€ ğŸ”§ Fusion Strategies        # Fusion strategies
â”‚   â””â”€â”€ ğŸ” Similarity Retrieval     # Similarity retrieval
â”œâ”€â”€ ğŸ¯ Dialog Manager              # Dialog manager
â””â”€â”€ ğŸ”§ Model Manager               # Model manager
```

### ğŸ“‹ Traditional RL Architecture
```
ğŸ“¦ Core Components
â”œâ”€â”€ ğŸ¯ Dialog Manager    # Dialog manager
â”œâ”€â”€ ğŸ—œï¸ Compressor        # Historical compressor  
â”œâ”€â”€ ğŸ¤– RL Trainer        # Reinforcement learning trainer
â”œâ”€â”€ ğŸ† Reward Calculator # Reward calculator
â””â”€â”€ ğŸ”§ Model Manager     # Model manager
```

## ğŸ“Š Four Control Group Experiments

| Group | Historical Compression | RL Training | Expected Effect |
|-------|:---------------------:|:-----------:|----------------|
| Baseline | âŒ | âŒ | Benchmark performance |
| Compression | âœ… | âŒ | Compression effect (+15-25%) |
| Training | âŒ | âœ… | RL optimization (+10-20%) |
| Complete | âœ… | âœ… | Optimal performance (+25-40%) |

## ğŸ’¡ Core Algorithms

### Compression Strategy Space
- **Compression Ratio**: [0.2, 0.3, 0.4, 0.5]
- **Strategy Types**: Recent focus, Topic focus, Entity focus, Balanced strategy
- **Action Space**: 16 strategy combinations (4Ã—4)

### Reward Function Design
```python
Total Reward = Quality RewardÃ—0.4 + Compression RewardÃ—0.3 + Coherence RewardÃ—0.3
```

## ğŸ“ Project Structure

```
vest.ai1/
ğŸ¯ Direct Embedding Solution
â”œâ”€â”€ ğŸ direct_embedding_compressor.py    # Direct embedding compressor
â”œâ”€â”€ ğŸ test_direct_embedding.py          # Direct embedding tests
â”œâ”€â”€ ğŸ quick_start_direct_embedding.py   # Quick start script
â”œâ”€â”€ ğŸ rl_trainer_embedding.py           # Embedding-enhanced RL trainer

ğŸ“‹ Traditional Solution
â”œâ”€â”€ ğŸ main.py                   # Main program entry
â”œâ”€â”€ ğŸ compressor.py             # Historical compressor
â”œâ”€â”€ ğŸ rl_trainer.py             # RL trainer
â”œâ”€â”€ ğŸ embedding_compressor.py   # Traditional embedding compressor

ğŸ”§ Shared Components
â”œâ”€â”€ ğŸ config.py                 # Configuration file
â”œâ”€â”€ ğŸ models.py                 # Model manager
â”œâ”€â”€ ğŸ dialog_manager.py         # Dialog manager
â”œâ”€â”€ ğŸ reward_calculator.py      # Reward calculator
â”œâ”€â”€ ğŸ““ rl_experiment.ipynb       # Complete experimental framework

ğŸ“ Data and Logs
â”œâ”€â”€ ğŸ“ models/                   # Model storage
â”œâ”€â”€ ğŸ“ rl_training/              # Training records
â””â”€â”€ ğŸ“ logs/                     # Log files
```

## ğŸ›ï¸ Key Configuration

```python
# Compression trigger conditions
trigger_compression_tokens = 1500  # Token threshold
keep_recent_turns = 3              # Keep recent turns

# RL training parameters  
learning_rate = 0.001              # Learning rate
epsilon_start = 1.0                # Initial exploration rate
epsilon_decay = 1000               # Exploration decay

# Model settings
model_name = 'Qwen/Qwen2.5-0.5B-Instruct'  # Base model
```

## ğŸ“ˆ Expected Results

### Performance Ranking
1. ğŸ¥‡ **Complete Group** (Compression + RL) - Optimal performance
2. ğŸ¥ˆ **Training Group** (RL only) - Medium-high performance  
3. ğŸ¥‰ **Compression Group** (Compression only) - Medium performance
4. ğŸ“Š **Baseline Group** (No optimization) - Benchmark performance

### Evaluation Metrics
- **Automated Metrics**: BLEU, ROUGE, BERTScore
- **Dialog Metrics**: Coherence, Information retention, Appropriateness
- **Compression Metrics**: Compression ratio, Information loss, Compression quality

## ğŸ”§ Troubleshooting

### Common Issues
```bash
# Model loading failure
python -c "from models import model_manager; model_manager.download_model()"

# Out of memory
# Set in config.py: torch_dtype = torch.float16

# Training not converging  
# Adjust learning rate: learning_rate = 0.0001
```

## ğŸ“š Detailed Documentation

- ğŸ“– [Complete Project Documentation](PROJECT_OVERVIEW.md) - Detailed system introduction and configuration
- ğŸ““ [Experiment Guide](RL_EXPERIMENT_GUIDE.md) - Experimental design and evaluation methods
- ğŸš€ [Quick Start Guide](QUICKSTART.md) - Beginner-friendly usage guide

## ğŸ¤ Contributing

We welcome Issues and Pull Requests! Please check the [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE)

---

**ğŸ‰ Start your reinforcement learning dialog system journey!**

For questions, please check the [detailed documentation](PROJECT_OVERVIEW.md) or submit an Issue. 