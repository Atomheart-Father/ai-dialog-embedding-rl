"""
å¢å¼ºç‰ˆEmbeddingæ³¨å…¥å™¨
å®ç°é«˜çº§çš„embeddingä¸queryç»“åˆæŠ€æœ¯ï¼Œç”¨äºå°†å†å²è®°å½•å‹ç¼©å‘é‡ä¸æ–°æŸ¥è¯¢ä¸€èµ·è¾“å…¥Qwenæ¨¡å‹
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional, Union
import logging
from datetime import datetime
import json

logger = logging.getLogger(__name__)

class AdvancedEmbeddingInjector:
    """é«˜çº§embeddingæ³¨å…¥å™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.embedding_dim = 896  # Qwen2.5-0.5Bçš„hidden_size
        
        # åˆå§‹åŒ–å¯å­¦ä¹ çš„æŠ•å½±å±‚
        self.history_projector = nn.Linear(self.embedding_dim, self.embedding_dim)
        self.query_projector = nn.Linear(self.embedding_dim, self.embedding_dim)
        self.fusion_attention = nn.MultiheadAttention(
            embed_dim=self.embedding_dim, 
            num_heads=8, 
            batch_first=True
        )
        
        # å¯å­¦ä¹ çš„embeddingæ± 
        self.special_embeddings = nn.Embedding(10, self.embedding_dim)  # ç‰¹æ®Štokençš„embedding
        
        # èåˆç­–ç•¥æ˜ å°„
        self.injection_strategies = {
            'direct_concatenation': self._direct_concatenation,
            'weighted_fusion': self._weighted_fusion,
            'attention_based': self._attention_based_fusion,
            'layered_injection': self._layered_injection,
            'adaptive_routing': self._adaptive_routing,
        }
        
        logger.info("ğŸš€ é«˜çº§Embeddingæ³¨å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_layered_embeddings(self, text: str, layers: List[int] = [-3, -2, -1]) -> Dict[str, torch.Tensor]:
        """æå–å¤šå±‚embeddingè¡¨ç¤º"""
        if not self.model_manager.dialog_model or not self.model_manager.tokenizer:
            return {f'layer_{i}': torch.zeros(self.embedding_dim) for i in layers}
        
        try:
            inputs = self.model_manager.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.model_manager.device)
            
            with torch.no_grad():
                outputs = self.model_manager.dialog_model(
                    **inputs,
                    output_hidden_states=True
                )
                
                layered_embeddings = {}
                for layer_idx in layers:
                    hidden_state = outputs.hidden_states[layer_idx]  # [batch, seq_len, hidden_dim]
                    
                    # å¤šç§poolingç­–ç•¥
                    mean_pooled = hidden_state.mean(dim=1)  # å¹³å‡pooling
                    max_pooled = hidden_state.max(dim=1)[0]  # æœ€å¤§pooling
                    cls_token = hidden_state[:, 0, :]  # CLS token
                    
                    # ç»„åˆpooling
                    combined = (0.5 * mean_pooled + 0.3 * max_pooled + 0.2 * cls_token)
                    layered_embeddings[f'layer_{layer_idx}'] = combined.squeeze(0).cpu()
                
                return layered_embeddings
                
        except Exception as e:
            logger.error(f"æå–å¤šå±‚embeddingå¤±è´¥: {e}")
            return {f'layer_{i}': torch.zeros(self.embedding_dim) for i in layers}
    
    def _direct_concatenation(self, history_emb: torch.Tensor, query_emb: torch.Tensor) -> torch.Tensor:
        """ç›´æ¥æ‹¼æ¥èåˆ"""
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        hist_proj = self.history_projector(history_emb)
        query_proj = self.query_projector(query_emb)
        
        # ç®€å•æ‹¼æ¥å¹¶é™ç»´
        concatenated = torch.cat([hist_proj, query_proj], dim=0)
        # ä½¿ç”¨çº¿æ€§å±‚é™ç»´åˆ°åŸå§‹ç»´åº¦
        reduced = concatenated[:self.embedding_dim] + concatenated[self.embedding_dim:]
        
        return reduced
    
    def _weighted_fusion(self, history_emb: torch.Tensor, query_emb: torch.Tensor) -> torch.Tensor:
        """åŠ æƒèåˆ"""
        # è®¡ç®—æƒé‡
        similarity = F.cosine_similarity(history_emb, query_emb, dim=0)
        history_weight = torch.sigmoid(similarity)
        query_weight = 1 - history_weight
        
        # æŠ•å½±å¹¶èåˆ
        hist_proj = self.history_projector(history_emb)
        query_proj = self.query_projector(query_emb)
        
        fused = history_weight * hist_proj + query_weight * query_proj
        return fused
    
    def _attention_based_fusion(self, history_emb: torch.Tensor, query_emb: torch.Tensor) -> torch.Tensor:
        """åŸºäºæ³¨æ„åŠ›çš„èåˆ"""
        # å‡†å¤‡è¾“å…¥ [batch_size, seq_len, embed_dim]
        hist_expanded = history_emb.unsqueeze(0).unsqueeze(0)  # [1, 1, embed_dim]
        query_expanded = query_emb.unsqueeze(0).unsqueeze(0)  # [1, 1, embed_dim]
        
        # æ³¨æ„åŠ›èåˆ
        combined_input = torch.cat([hist_expanded, query_expanded], dim=1)  # [1, 2, embed_dim]
        
        try:
            attn_output, attn_weights = self.fusion_attention(
                combined_input, combined_input, combined_input
            )
            
            # èšåˆè¾“å‡º
            fused = attn_output.mean(dim=1).squeeze(0)  # [embed_dim]
            return fused
            
        except Exception as e:
            logger.warning(f"æ³¨æ„åŠ›èåˆå¤±è´¥ï¼Œå›é€€åˆ°åŠ æƒèåˆ: {e}")
            return self._weighted_fusion(history_emb, query_emb)
    
    def _layered_injection(self, history_emb: torch.Tensor, query_emb: torch.Tensor) -> torch.Tensor:
        """åˆ†å±‚æ³¨å…¥èåˆ"""
        # å°†embeddingåˆ†æˆå¤šä¸ªå±‚è¿›è¡Œå¤„ç†
        chunk_size = self.embedding_dim // 4
        fused_chunks = []
        
        for i in range(4):
            start_idx = i * chunk_size
            end_idx = start_idx + chunk_size
            
            hist_chunk = history_emb[start_idx:end_idx]
            query_chunk = query_emb[start_idx:end_idx]
            
            # ä¸åŒçš„èåˆç­–ç•¥
            if i == 0:  # ç¬¬ä¸€å±‚ï¼šä¿ç•™æ›´å¤šå†å²ä¿¡æ¯
                chunk_fused = 0.7 * hist_chunk + 0.3 * query_chunk
            elif i == 1:  # ç¬¬äºŒå±‚ï¼šå¹³è¡¡èåˆ
                chunk_fused = 0.5 * hist_chunk + 0.5 * query_chunk
            elif i == 2:  # ç¬¬ä¸‰å±‚ï¼šä¾§é‡æŸ¥è¯¢
                chunk_fused = 0.3 * hist_chunk + 0.7 * query_chunk
            else:  # ç¬¬å››å±‚ï¼šä¸»è¦æ˜¯æŸ¥è¯¢ä¿¡æ¯
                chunk_fused = 0.1 * hist_chunk + 0.9 * query_chunk
            
            fused_chunks.append(chunk_fused)
        
        return torch.cat(fused_chunks, dim=0)
    
    def _adaptive_routing(self, history_emb: torch.Tensor, query_emb: torch.Tensor) -> torch.Tensor:
        """è‡ªé€‚åº”è·¯ç”±èåˆ"""
        # è®¡ç®—å†å²å’ŒæŸ¥è¯¢çš„ç‰¹å¾
        hist_norm = torch.norm(history_emb)
        query_norm = torch.norm(query_emb)
        similarity = F.cosine_similarity(history_emb, query_emb, dim=0)
        
        # åŸºäºç‰¹å¾é€‰æ‹©èåˆç­–ç•¥
        if similarity > 0.8:  # é«˜ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨ç®€å•åŠ æƒ
            return self._weighted_fusion(history_emb, query_emb)
        elif hist_norm > query_norm * 2:  # å†å²ä¿¡æ¯ä¸°å¯Œï¼Œä½¿ç”¨åˆ†å±‚æ³¨å…¥
            return self._layered_injection(history_emb, query_emb)
        else:  # ä½¿ç”¨æ³¨æ„åŠ›èåˆ
            return self._attention_based_fusion(history_emb, query_emb)
    
    def create_context_with_embedding(self, 
                                    history_embedding: torch.Tensor,
                                    query_text: str,
                                    injection_strategy: str = "adaptive_routing") -> Tuple[str, Dict]:
        """åˆ›å»ºåŒ…å«embeddingä¿¡æ¯çš„ä¸Šä¸‹æ–‡"""
        
        # æå–æŸ¥è¯¢çš„embedding
        query_embeddings = self.extract_layered_embeddings(query_text)
        query_emb = query_embeddings['layer_-1']  # ä½¿ç”¨æœ€åä¸€å±‚
        
        # é€‰æ‹©èåˆç­–ç•¥
        if injection_strategy in self.injection_strategies:
            fusion_func = self.injection_strategies[injection_strategy]
            fused_embedding = fusion_func(history_embedding, query_emb)
        else:
            logger.warning(f"æœªçŸ¥çš„æ³¨å…¥ç­–ç•¥: {injection_strategy}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            fused_embedding = self._adaptive_routing(history_embedding, query_emb)
        
        # å°†èåˆçš„embeddingè½¬æ¢ä¸ºä¸Šä¸‹æ–‡æç¤º
        context_prompt = self._embedding_to_context_prompt(fused_embedding, query_text)
        
        # å…ƒæ•°æ®
        metadata = {
            'injection_strategy': injection_strategy,
            'history_embedding_norm': torch.norm(history_embedding).item(),
            'query_embedding_norm': torch.norm(query_emb).item(),
            'fused_embedding_norm': torch.norm(fused_embedding).item(),
            'embedding_similarity': F.cosine_similarity(
                history_embedding, query_emb, dim=0
            ).item()
        }
        
        return context_prompt, metadata
    
    def _embedding_to_context_prompt(self, embedding: torch.Tensor, query_text: str) -> str:
        """å°†èåˆçš„embeddingè½¬æ¢ä¸ºä¸Šä¸‹æ–‡æç¤º"""
        
        # æ–¹æ³•1: åŸºäºembeddingçš„æ¿€æ´»æ¨¡å¼ç”Ÿæˆæç¤º
        activation_threshold = embedding.mean() + embedding.std()
        high_activation_dims = (embedding > activation_threshold).sum().item()
        
        if high_activation_dims > embedding.shape[0] * 0.3:
            context_prefix = "åŸºäºä¸°å¯Œçš„å†å²å¯¹è¯ä¿¡æ¯"
        elif high_activation_dims > embedding.shape[0] * 0.1:
            context_prefix = "å‚è€ƒç›¸å…³çš„å†å²èƒŒæ™¯"
        else:
            context_prefix = "ç»“åˆä¹‹å‰çš„å¯¹è¯å†…å®¹"
        
        # æ–¹æ³•2: å°†embeddingçš„å…³é”®ç»´åº¦æ˜ å°„ä¸ºç‰¹æ®Štoken
        top_k = 5
        top_values, top_indices = torch.topk(embedding, top_k)
        context_tokens = []
        
        for val, idx in zip(top_values, top_indices):
            if val > 0:
                token_id = int(idx.item() % 10)  # æ˜ å°„åˆ°0-9
                context_tokens.append(f"<CTX_{token_id}>")
        
        context_token_str = " ".join(context_tokens)
        
        # æ„å»ºæœ€ç»ˆæç¤º
        enhanced_prompt = f"""{context_prefix}ï¼Œ{context_token_str}

ç”¨æˆ·: {query_text}
åŠ©æ‰‹:"""
        
        return enhanced_prompt
    
    def generate_with_enhanced_context(self, 
                                     history_text: str,
                                     query_text: str,
                                     injection_strategy: str = "adaptive_routing",
                                     max_new_tokens: int = 512) -> Dict:
        """ä½¿ç”¨å¢å¼ºä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤"""
        
        # æå–å†å²embedding
        history_embeddings = self.extract_layered_embeddings(history_text)
        history_emb = history_embeddings['layer_-1']
        
        # åˆ›å»ºå¢å¼ºä¸Šä¸‹æ–‡
        enhanced_prompt, metadata = self.create_context_with_embedding(
            history_emb, query_text, injection_strategy
        )
        
        # ç”Ÿæˆå›å¤
        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        
        if start_time:
            start_time.record()
        
        response = self.model_manager.generate_text(
            model=self.model_manager.dialog_model,
            prompt=enhanced_prompt,
            max_new_tokens=max_new_tokens
        )
        
        if end_time:
            end_time.record()
            torch.cuda.synchronize()
            generation_time = start_time.elapsed_time(end_time) / 1000.0  # è½¬æ¢ä¸ºç§’
        else:
            generation_time = 0.0
        
        return {
            'response': response,
            'enhanced_prompt': enhanced_prompt,
            'metadata': metadata,
            'generation_time': generation_time,
            'input_tokens': self.model_manager.count_tokens(enhanced_prompt),
            'output_tokens': self.model_manager.count_tokens(response)
        }
    
    def compare_injection_strategies(self, 
                                   history_text: str,
                                   query_text: str) -> Dict:
        """å¯¹æ¯”ä¸åŒæ³¨å…¥ç­–ç•¥çš„æ•ˆæœ"""
        
        results = {}
        
        for strategy in self.injection_strategies.keys():
            try:
                result = self.generate_with_enhanced_context(
                    history_text, query_text, strategy
                )
                results[strategy] = result
                logger.info(f"âœ… ç­–ç•¥ {strategy} å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ ç­–ç•¥ {strategy} å¤±è´¥: {e}")
                results[strategy] = {'error': str(e)}
        
        return results
    
    def save_injector_state(self, filepath: str):
        """ä¿å­˜æ³¨å…¥å™¨çš„å¯å­¦ä¹ å‚æ•°"""
        state_dict = {
            'history_projector': self.history_projector.state_dict(),
            'query_projector': self.query_projector.state_dict(),
            'fusion_attention': self.fusion_attention.state_dict(),
            'special_embeddings': self.special_embeddings.state_dict(),
        }
        
        torch.save(state_dict, filepath)
        logger.info(f"ğŸ’¾ æ³¨å…¥å™¨çŠ¶æ€å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_injector_state(self, filepath: str):
        """åŠ è½½æ³¨å…¥å™¨çš„å¯å­¦ä¹ å‚æ•°"""
        try:
            state_dict = torch.load(filepath, map_location='cpu')
            
            self.history_projector.load_state_dict(state_dict['history_projector'])
            self.query_projector.load_state_dict(state_dict['query_projector'])
            self.fusion_attention.load_state_dict(state_dict['fusion_attention'])
            self.special_embeddings.load_state_dict(state_dict['special_embeddings'])
            
            logger.info(f"ğŸ“¥ æ³¨å…¥å™¨çŠ¶æ€å·²ä» {filepath} åŠ è½½")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ³¨å…¥å™¨çŠ¶æ€å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_enhanced_injector():
    """æµ‹è¯•å¢å¼ºç‰ˆæ³¨å…¥å™¨çš„åŠŸèƒ½"""
    
    # è¿™é‡Œéœ€è¦åœ¨æœ‰model_managerçš„ç¯å¢ƒä¸­è¿è¡Œ
    # enhanced_injector = AdvancedEmbeddingInjector(model_manager)
    
    # æµ‹è¯•å†å²å’ŒæŸ¥è¯¢
    test_history = """
    ç”¨æˆ·: æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹
    åŠ©æ‰‹: Pythonæ˜¯å¾ˆå¥½çš„ç¼–ç¨‹è¯­è¨€ï¼Œå»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹
    ç”¨æˆ·: å¦‚ä½•å­¦ä¹ æ•°æ®ç»“æ„ï¼Ÿ
    åŠ©æ‰‹: å¯ä»¥ä»åˆ—è¡¨ã€å­—å…¸ç­‰åŸºç¡€æ•°æ®ç»“æ„å¼€å§‹å­¦ä¹ 
    """
    
    test_query = "ç°åœ¨æˆ‘æƒ³å­¦ä¹ é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ"
    
    print("ğŸ§ª æµ‹è¯•å†å²æ–‡æœ¬:")
    print(test_history)
    print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")
    
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦å…ˆåˆå§‹åŒ–model_manager
    # result = enhanced_injector.compare_injection_strategies(test_history, test_query)
    # return result

if __name__ == "__main__":
    test_enhanced_injector() 