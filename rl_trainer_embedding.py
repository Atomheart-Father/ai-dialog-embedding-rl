"""
å¢å¼ºç‰ˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ - æ”¯æŒEmbeddingå‹ç¼©
ç»“åˆå‘é‡è¡¨ç¤ºå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¯¹è¯ç³»ç»Ÿæ€§èƒ½
"""
import torch
import torch.nn as nn
import numpy as np
import random
from collections import deque, namedtuple
from typing import List, Dict, Tuple, Optional
import logging
import json
import os
from datetime import datetime

from config import rl_config, model_config, dialog_config
from models import model_manager
from embedding_compressor import embedding_compressor
from reward_calculator import RewardCalculator

logger = logging.getLogger(__name__)

# å®šä¹‰ç»éªŒå›æ”¾ä¸­çš„è½¬æ¢ï¼ˆå¢å¼ºç‰ˆï¼‰
EmbeddingTransition = namedtuple('EmbeddingTransition', 
    ('state_embedding', 'action', 'next_state_embedding', 'reward', 'metadata'))

class EmbeddingRLTrainer:
    """åŸºäºEmbeddingçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.reward_calculator = RewardCalculator()
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆä¸“é—¨å­˜å‚¨embeddingï¼‰
        self.embedding_memory = deque(maxlen=5000)
        
        # åŠ¨ä½œç©ºé—´ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸“æ³¨äºembeddingç›¸å…³å†³ç­–ï¼‰
        self.action_space = {
            'use_embedding': 0,      # ä½¿ç”¨embeddingå‹ç¼©
            'use_full_context': 1,   # ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
            'hybrid_approach': 2     # æ··åˆæ–¹æ³•
        }
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.embedding_efficiency = []
        self.context_length_savings = []
        self.current_episode = 0
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = 0.3
        
        # åˆ›å»ºè®­ç»ƒç›®å½•
        self.training_dir = "rl_embedding_training"
        os.makedirs(self.training_dir, exist_ok=True)
        
        logger.info("ğŸ§  Embedding RLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_state_embedding(self, dialog_history: List[Dict]) -> torch.Tensor:
        """æå–å¯¹è¯çŠ¶æ€çš„embeddingè¡¨ç¤º"""
        if not dialog_history:
            return torch.zeros(embedding_compressor.embedding_dim)
        
        # åˆå¹¶æœ€è¿‘å‡ è½®å¯¹è¯
        recent_context = ""
        for turn in dialog_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
            role = "ç”¨æˆ·" if turn['role'] == 'user' else "åŠ©æ‰‹"
            recent_context += f"{role}: {turn['content']}\n"
        
        # æå–çŠ¶æ€embedding
        state_embedding = embedding_compressor.extract_text_embedding(recent_context)
        return state_embedding
    
    def select_compression_strategy(self, state_embedding: torch.Tensor, training: bool = True) -> int:
        """é€‰æ‹©å‹ç¼©ç­–ç•¥"""
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©ç­–ç•¥
            return random.choice(list(self.action_space.values()))
        else:
            # åˆ©ç”¨ï¼šåŸºäºçŠ¶æ€embeddingé€‰æ‹©æœ€ä¼˜ç­–ç•¥
            return self._select_optimal_strategy(state_embedding)
    
    def _select_optimal_strategy(self, state_embedding: torch.Tensor) -> int:
        """åŸºäºçŠ¶æ€embeddingé€‰æ‹©æœ€ä¼˜ç­–ç•¥"""
        # è®¡ç®—embeddingçš„"ä¿¡æ¯å¯†åº¦"
        embedding_norm = torch.norm(state_embedding).item()
        embedding_variance = torch.var(state_embedding).item()
        
        # åŸºäºä¿¡æ¯å¯†åº¦å†³ç­–
        if embedding_norm > 2.0 and embedding_variance > 0.1:
            # é«˜ä¿¡æ¯å¯†åº¦ï¼Œé€‚åˆembeddingå‹ç¼©
            return self.action_space['use_embedding']
        elif embedding_norm < 1.0:
            # ä½ä¿¡æ¯å¯†åº¦ï¼Œä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
            return self.action_space['use_full_context']
        else:
            # ä¸­ç­‰ä¿¡æ¯å¯†åº¦ï¼Œæ··åˆæ–¹æ³•
            return self.action_space['hybrid_approach']
    
    def execute_compression_strategy(self, 
                                   action: int, 
                                   dialog_history: List[Dict], 
                                   current_input: str) -> Tuple[str, Dict]:
        """æ‰§è¡Œå‹ç¼©ç­–ç•¥"""
        if action == self.action_space['use_embedding']:
            # ç­–ç•¥1: çº¯embeddingå‹ç¼©
            context = embedding_compressor.generate_context_with_embeddings(current_input)
            
            # å‹ç¼©å†å²ä¸ºembedding
            if len(dialog_history) > 2:
                compressed_data = embedding_compressor.compress_history_to_embeddings(dialog_history)
                embedding_compressor.current_session_embeddings.extend(compressed_data)
                embedding_compressor.update_history_embeddings()
            
            metadata = {
                'strategy': 'embedding_only',
                'compression_used': True,
                'context_length': len(context)
            }
            
        elif action == self.action_space['use_full_context']:
            # ç­–ç•¥2: å®Œæ•´ä¸Šä¸‹æ–‡
            context = ""
            for turn in dialog_history:
                role = "ç”¨æˆ·" if turn['role'] == 'user' else "åŠ©æ‰‹"
                context += f"{role}: {turn['content']}\n"
            context += f"ç”¨æˆ·: {current_input}\nåŠ©æ‰‹:"
            
            metadata = {
                'strategy': 'full_context',
                'compression_used': False,
                'context_length': len(context)
            }
            
        else:  # hybrid_approach
            # ç­–ç•¥3: æ··åˆæ–¹æ³•
            # ä¿ç•™æœ€è¿‘2è½® + embeddingæ‘˜è¦
            recent_context = ""
            for turn in dialog_history[-4:]:  # æœ€è¿‘2è½®
                role = "ç”¨æˆ·" if turn['role'] == 'user' else "åŠ©æ‰‹"
                recent_context += f"{role}: {turn['content']}\n"
            
            # è·å–embeddingä¿¡æ¯
            if len(dialog_history) > 4:
                embedding_context = embedding_compressor.generate_context_with_embeddings(current_input)
                context = embedding_context + "\n" + recent_context + f"ç”¨æˆ·: {current_input}\nåŠ©æ‰‹:"
            else:
                context = recent_context + f"ç”¨æˆ·: {current_input}\nåŠ©æ‰‹:"
            
            metadata = {
                'strategy': 'hybrid',
                'compression_used': len(dialog_history) > 4,
                'context_length': len(context)
            }
        
        return context, metadata
    
    def calculate_embedding_reward(self, 
                                 original_state: torch.Tensor,
                                 action: int,
                                 context: str,
                                 response: str,
                                 metadata: Dict,
                                 user_input: str) -> float:
        """è®¡ç®—åŸºäºembeddingçš„å¥–åŠ±"""
        reward = 0.0
        
        # 1. æ•ˆç‡å¥–åŠ± (30%)
        if metadata['compression_used']:
            # å¥–åŠ±å‹ç¼©æ•ˆç‡
            estimated_full_length = len(user_input) * 10  # ä¼°ç®—å®Œæ•´å¯¹è¯é•¿åº¦
            actual_length = metadata['context_length']
            efficiency = max(0, (estimated_full_length - actual_length) / estimated_full_length)
            reward += efficiency * 0.3
        
        # 2. è´¨é‡å¥–åŠ± (40%)
        # åŸºäºå›å¤é•¿åº¦å’Œç›¸å…³æ€§çš„ç®€åŒ–è¯„ä¼°
        response_quality = min(1.0, len(response) / 200)  # é•¿åº¦é€‚ä¸­æ€§
        reward += response_quality * 0.4
        
        # 3. ç­–ç•¥é€‰æ‹©å¥–åŠ± (30%)
        # å¥–åŠ±åˆé€‚çš„ç­–ç•¥é€‰æ‹©
        embedding_norm = torch.norm(original_state).item()
        if action == self.action_space['use_embedding'] and embedding_norm > 2.0:
            reward += 0.3  # å¥–åŠ±åœ¨é«˜ä¿¡æ¯å¯†åº¦æ—¶ä½¿ç”¨embedding
        elif action == self.action_space['use_full_context'] and embedding_norm < 1.0:
            reward += 0.3  # å¥–åŠ±åœ¨ä½ä¿¡æ¯å¯†åº¦æ—¶ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
        elif action == self.action_space['hybrid_approach']:
            reward += 0.15  # æ··åˆæ–¹æ³•å¾—åˆ°ä¸­ç­‰å¥–åŠ±
        
        return reward
    
    def store_embedding_transition(self, 
                                 state_embedding: torch.Tensor,
                                 action: int,
                                 next_state_embedding: torch.Tensor,
                                 reward: float,
                                 metadata: Dict):
        """å­˜å‚¨embeddingè½¬æ¢åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº"""
        transition = EmbeddingTransition(
            state_embedding, action, next_state_embedding, reward, metadata
        )
        self.embedding_memory.append(transition)
    
    def train_embedding_step(self) -> float:
        """æ‰§è¡Œä¸€æ­¥embeddingè®­ç»ƒ"""
        if len(self.embedding_memory) < 50:
            return 0.0
        
        # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·
        batch_size = min(16, len(self.embedding_memory))
        transitions = random.sample(self.embedding_memory, batch_size)
        batch = EmbeddingTransition(*zip(*transitions))
        
        # è®¡ç®—ç®€åŒ–çš„ç­–ç•¥æŸå¤±
        rewards = torch.tensor(batch.reward, dtype=torch.float32)
        actions = torch.tensor(batch.action, dtype=torch.long)
        
        # ç­–ç•¥æ¢¯åº¦çš„ç®€åŒ–ç‰ˆæœ¬
        policy_loss = -torch.mean(rewards)  # æœ€å¤§åŒ–å¥–åŠ±
        
        # æ›´æ–°epsilon
        self.epsilon = max(0.1, self.epsilon * 0.995)
        
        return policy_loss.item()
    
    def train_embedding_episode(self, test_inputs: List[str]) -> Dict:
        """è®­ç»ƒä¸€ä¸ªembedding episode"""
        episode_reward = 0.0
        episode_steps = 0
        context_savings = []
        
        dialog_history = []
        
        logger.info(f"ğŸ® å¼€å§‹Embedding RL Episode {self.current_episode}")
        
        for i, user_input in enumerate(test_inputs):
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
            dialog_history.append({
                'role': 'user',
                'content': user_input,
                'timestamp': datetime.now().isoformat()
            })
            
            # æå–å½“å‰çŠ¶æ€embedding
            current_state_embedding = self.extract_state_embedding(dialog_history)
            
            # é€‰æ‹©å‹ç¼©ç­–ç•¥
            action = self.select_compression_strategy(current_state_embedding, training=True)
            
            # æ‰§è¡Œå‹ç¼©ç­–ç•¥
            context, metadata = self.execute_compression_strategy(
                action, dialog_history, user_input
            )
            
            # ç”Ÿæˆå›å¤
            if model_manager.dialog_model:
                response = model_manager.generate_text(
                    model=model_manager.dialog_model,
                    prompt=context,
                    max_new_tokens=256
                )
            else:
                response = f"Embeddingå›å¤{i+1}: å…³äº'{user_input[:20]}...'çš„æ™ºèƒ½å›ç­”"
            
            # æ·»åŠ å›å¤åˆ°å†å²
            dialog_history.append({
                'role': 'assistant',
                'content': response,
                'timestamp': datetime.now().isoformat()
            })
            
            # æå–ä¸‹ä¸€çŠ¶æ€embedding
            next_state_embedding = self.extract_state_embedding(dialog_history)
            
            # è®¡ç®—å¥–åŠ±
            reward = self.calculate_embedding_reward(
                current_state_embedding, action, context, response, metadata, user_input
            )
            
            # å­˜å‚¨ç»éªŒ
            self.store_embedding_transition(
                current_state_embedding, action, next_state_embedding, reward, metadata
            )
            
            episode_reward += reward
            episode_steps += 1
            
            # è®°å½•ä¸Šä¸‹æ–‡èŠ‚çœ
            if metadata['compression_used']:
                estimated_full = len(' '.join([t['content'] for t in dialog_history]))
                actual = metadata['context_length']
                saving = max(0, (estimated_full - actual) / estimated_full)
                context_savings.append(saving)
        
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        loss = self.train_embedding_step()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards.append(episode_reward)
        if context_savings:
            self.context_length_savings.append(np.mean(context_savings))
        self.current_episode += 1
        
        episode_stats = {
            'episode': self.current_episode,
            'total_reward': episode_reward,
            'steps': episode_steps,
            'loss': loss,
            'epsilon': self.epsilon,
            'avg_context_saving': np.mean(context_savings) if context_savings else 0.0,
            'memory_size': len(self.embedding_memory)
        }
        
        logger.info(f"âœ… Embedding Episode {self.current_episode} å®Œæˆ: "
                   f"å¥–åŠ±={episode_reward:.3f}, ä¸Šä¸‹æ–‡èŠ‚çœ={episode_stats['avg_context_saving']:.1%}")
        
        return episode_stats
    
    def evaluate_embedding_performance(self, test_inputs: List[str]) -> Dict:
        """è¯„ä¼°embeddingå‹ç¼©æ€§èƒ½"""
        print("ğŸ“Š è¯„ä¼°Embeddingå‹ç¼©æ€§èƒ½...")
        
        total_compression_ratio = 0.0
        total_response_quality = 0.0
        strategy_usage = {strategy: 0 for strategy in ['embedding_only', 'full_context', 'hybrid']}
        
        dialog_history = []
        
        for user_input in test_inputs:
            dialog_history.append({'role': 'user', 'content': user_input})
            
            # æå–çŠ¶æ€
            state_embedding = self.extract_state_embedding(dialog_history)
            
            # é€‰æ‹©ç­–ç•¥ï¼ˆè¯„ä¼°æ¨¡å¼ï¼Œä¸æ¢ç´¢ï¼‰
            action = self.select_compression_strategy(state_embedding, training=False)
            
            # æ‰§è¡Œç­–ç•¥
            context, metadata = self.execute_compression_strategy(
                action, dialog_history, user_input
            )
            
            # ç»Ÿè®¡ç­–ç•¥ä½¿ç”¨
            strategy_usage[metadata['strategy']] += 1
            
            # æ¨¡æ‹Ÿå›å¤
            response = f"è¯„ä¼°å›å¤: å…³äº{user_input[:20]}çš„å›ç­”"
            dialog_history.append({'role': 'assistant', 'content': response})
            
            # è®¡ç®—å‹ç¼©æ¯”
            if metadata['compression_used']:
                estimated_full = len(' '.join([t['content'] for t in dialog_history]))
                compression_ratio = metadata['context_length'] / estimated_full
                total_compression_ratio += compression_ratio
        
        avg_compression = total_compression_ratio / len(test_inputs)
        
        performance_report = {
            'average_compression_ratio': avg_compression,
            'strategy_distribution': strategy_usage,
            'total_episodes_trained': self.current_episode,
            'current_epsilon': self.epsilon,
            'embedding_memory_size': len(self.embedding_memory)
        }
        
        return performance_report
    
    def save_embedding_checkpoint(self):
        """ä¿å­˜embeddingè®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'episode': self.current_episode,
            'epsilon': self.epsilon,
            'episode_rewards': self.episode_rewards,
            'context_length_savings': self.context_length_savings,
            'memory_size': len(self.embedding_memory),
            'action_space': self.action_space,
            'training_timestamp': datetime.now().isoformat()
        }
        
        checkpoint_path = os.path.join(
            self.training_dir,
            f"embedding_checkpoint_episode_{self.current_episode}.json"
        )
        
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Embeddingæ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

# åˆ›å»ºå…¨å±€embedding RLè®­ç»ƒå™¨å®ä¾‹
embedding_rl_trainer = EmbeddingRLTrainer() 