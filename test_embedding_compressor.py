"""
Embeddingå‹ç¼©å™¨æµ‹è¯•è„šæœ¬
æµ‹è¯•åŸºäºå‘é‡ç©ºé—´çš„ä¸Šä¸‹æ–‡å‹ç¼©æ€§èƒ½
"""
import sys
import time
import numpy as np
from typing import List, Dict

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from embedding_compressor import embedding_compressor
from models import model_manager
from config import model_config

def test_embedding_extraction():
    """æµ‹è¯•embeddingæå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•1: Embeddingæå–åŠŸèƒ½")
    
    test_texts = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£Pythonç¼–ç¨‹",
        "å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æº"
    ]
    
    print(f"  æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    
    for i, text in enumerate(test_texts):
        start_time = time.time()
        embedding = embedding_compressor.extract_text_embedding(text)
        end_time = time.time()
        
        print(f"  æ–‡æœ¬{i+1}: ç»´åº¦={embedding.shape}, æ—¶é—´={end_time-start_time:.3f}s")
        print(f"    å†…å®¹: {text[:30]}...")
        print(f"    å‘é‡èŒƒå›´: [{embedding.min():.3f}, {embedding.max():.3f}]")
    
    print("âœ… Embeddingæå–æµ‹è¯•å®Œæˆ\n")

def test_dialog_compression():
    """æµ‹è¯•å¯¹è¯å‹ç¼©åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•2: å¯¹è¯å‹ç¼©åŠŸèƒ½")
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    dialog_history = [
        ("æˆ‘æƒ³å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œä»å“ªé‡Œå¼€å§‹æ¯”è¾ƒå¥½ï¼Ÿ", "å»ºè®®ä»PythonåŸºç¡€å¼€å§‹ï¼Œç„¶åå­¦ä¹ numpyã€pandasç­‰åº“"),
        ("æˆ‘å·²ç»ä¼šPythonäº†ï¼Œå¯ä»¥ç›´æ¥å­¦ä¹ ç®—æ³•å—ï¼Ÿ", "å¯ä»¥çš„ï¼Œå»ºè®®å…ˆå­¦ä¹ çº¿æ€§å›å½’ã€å†³ç­–æ ‘ç­‰åŸºç¡€ç®—æ³•"),
        ("ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ", "ç›‘ç£å­¦ä¹ æœ‰æ ‡ç­¾æ•°æ®ï¼Œæ— ç›‘ç£å­¦ä¹ æ²¡æœ‰æ ‡ç­¾ï¼Œç”¨äºå‘ç°æ•°æ®æ¨¡å¼"),
        ("æ·±åº¦å­¦ä¹ éœ€è¦ä»€ä¹ˆåŸºç¡€ï¼Ÿ", "éœ€è¦çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºåŸºç¡€ï¼Œä»¥åŠå¯¹ç¥ç»ç½‘ç»œçš„ç†è§£"),
        ("å¼ºåŒ–å­¦ä¹ é€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ", "é€‚åˆå†³ç­–ä¼˜åŒ–åœºæ™¯ï¼Œå¦‚æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿç­‰")
    ]
    
    print(f"  å¯¹è¯è½®æ•°: {len(dialog_history)}")
    
    # é€è½®å‹ç¼©
    compressed_data = []
    for i, (user_input, assistant_response) in enumerate(dialog_history):
        embedding_data = embedding_compressor.compress_dialog_turn(user_input, assistant_response)
        compressed_data.append(embedding_data)
        
        metadata = embedding_data['metadata']
        print(f"  è½®æ¬¡{i+1}: {metadata['turn_summary']}")
        print(f"    Tokenæ•°: {metadata['token_count']}, å‘é‡ç»´åº¦: {embedding_data['embedding'].shape}")
    
    # è®¡ç®—å‹ç¼©ç»Ÿè®¡
    stats = embedding_compressor.get_compression_stats()
    if stats:
        print(f"\nğŸ“Š å‹ç¼©ç»Ÿè®¡:")
        print(f"  æ€»embeddingæ•°: {stats['total_embeddings']}")
        print(f"  åŸå§‹tokenæ•°: {stats['original_tokens']}")
        print(f"  ç­‰æ•ˆtokenæ•°: {stats['embedding_equivalent_tokens']}")
        print(f"  å‹ç¼©æ¯”: {stats['memory_efficiency']}")
    
    print("âœ… å¯¹è¯å‹ç¼©æµ‹è¯•å®Œæˆ\n")
    return compressed_data

def test_similarity_retrieval(compressed_data: List[Dict]):
    """æµ‹è¯•ç›¸ä¼¼åº¦æ£€ç´¢åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•3: ç›¸ä¼¼åº¦æ£€ç´¢åŠŸèƒ½")
    
    # æ›´æ–°å†å²embedding
    embedding_compressor.current_session_embeddings = compressed_data
    embedding_compressor.update_history_embeddings()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„å…³ç³»",
        "æ¨èç³»ç»Ÿæ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿ",
        "æˆ‘æƒ³äº†è§£ç®—æ³•åŸºç¡€çŸ¥è¯†"
    ]
    
    for query in test_queries:
        print(f"\n  æŸ¥è¯¢: {query}")
        relevant = embedding_compressor.retrieve_relevant_embeddings(query, top_k=3)
        
        if relevant:
            for i, emb_data in enumerate(relevant):
                metadata = emb_data['metadata']
                print(f"    ç›¸å…³{i+1}: {metadata['turn_summary']}")
        else:
            print("    æœªæ‰¾åˆ°ç›¸å…³å†å²")
    
    print("\nâœ… ç›¸ä¼¼åº¦æ£€ç´¢æµ‹è¯•å®Œæˆ\n")

def test_context_generation():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•4: ä¸Šä¸‹æ–‡ç”ŸæˆåŠŸèƒ½")
    
    test_inputs = [
        "æˆ‘æƒ³æ·±å…¥äº†è§£ç¥ç»ç½‘ç»œ",
        "å¼ºåŒ–å­¦ä¹ æœ‰å“ªäº›å…·ä½“åº”ç”¨ï¼Ÿ",
        "æ•°æ®é¢„å¤„ç†æœ‰ä»€ä¹ˆæŠ€å·§ï¼Ÿ"
    ]
    
    for input_text in test_inputs:
        print(f"\n  è¾“å…¥: {input_text}")
        context = embedding_compressor.generate_context_with_embeddings(input_text)
        
        print(f"  ç”Ÿæˆçš„ä¸Šä¸‹æ–‡:")
        lines = context.split('\n')
        for line in lines[:8]:  # åªæ˜¾ç¤ºå‰8è¡Œ
            print(f"    {line}")
        if len(lines) > 8:
            print(f"    ... (å…±{len(lines)}è¡Œ)")
    
    print("\nâœ… ä¸Šä¸‹æ–‡ç”Ÿæˆæµ‹è¯•å®Œæˆ\n")

def test_clustering_analysis(compressed_data: List[Dict]):
    """æµ‹è¯•èšç±»åˆ†æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•5: èšç±»åˆ†æåŠŸèƒ½")
    
    try:
        from sklearn.cluster import KMeans
        
        if len(compressed_data) >= 3:
            cluster_result = embedding_compressor.cluster_embeddings(compressed_data, n_clusters=2)
            
            print(f"  èšç±»ç»“æœ:")
            for cluster_id, items in cluster_result['clusters'].items():
                print(f"    ç°‡{cluster_id + 1} ({len(items)}ä¸ªå¯¹è¯):")
                for item in items:
                    print(f"      - {item['metadata']['turn_summary']}")
        else:
            print("  å¯¹è¯æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»æµ‹è¯•")
    
    except ImportError:
        print("  Sklearnæœªå®‰è£…ï¼Œè·³è¿‡èšç±»æµ‹è¯•")
    
    print("âœ… èšç±»åˆ†ææµ‹è¯•å®Œæˆ\n")

def test_performance_comparison():
    """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
    print("ğŸ§ª æµ‹è¯•6: æ€§èƒ½å¯¹æ¯”")
    
    test_text = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹æ¥è®©è®¡ç®—æœºç³»ç»Ÿé€æ­¥æ”¹å–„ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æå’Œè¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"
    
    # æµ‹è¯•embeddingæå–é€Ÿåº¦
    times = []
    for _ in range(5):
        start_time = time.time()
        embedding = embedding_compressor.extract_text_embedding(test_text)
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = np.mean(times)
    print(f"  Embeddingæå–å¹³å‡æ—¶é—´: {avg_time:.4f}s")
    print(f"  æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"  å‹ç¼©æ¯”: 1ä¸ªå‘é‡ vs ~{len(test_text)//4} tokens")
    
    # å†…å­˜ä½¿ç”¨ä¼°ç®—
    embedding_size = embedding.numel() * 4  # float32
    text_size = len(test_text.encode('utf-8'))
    
    print(f"  å‘é‡å†…å­˜: {embedding_size} bytes")
    print(f"  æ–‡æœ¬å†…å­˜: {text_size} bytes") 
    print(f"  å†…å­˜æ•ˆç‡: {embedding_size/text_size:.2f}x")
    
    print("âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ\n")

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Embeddingå‹ç¼©å™¨ç»¼åˆæµ‹è¯•\n")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        model_manager.load_models()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿›è¡Œæµ‹è¯•...")
    
    print("\n" + "=" * 50)
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_embedding_extraction()
    compressed_data = test_dialog_compression()
    test_similarity_retrieval(compressed_data)
    test_context_generation()
    test_clustering_analysis(compressed_data)
    test_performance_comparison()
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š Embeddingå‹ç¼©å™¨ç‰¹ç‚¹æ€»ç»“:")
    print("âœ… å›ºå®šé•¿åº¦å‘é‡è¡¨ç¤ºï¼ŒèŠ‚çœtoken")
    print("âœ… è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œæ™ºèƒ½åŒ¹é…")
    print("âœ… èšç±»åˆ†æï¼Œå‘ç°å¯¹è¯ä¸»é¢˜")
    print("âœ… å¿«é€Ÿå‹ç¼©ï¼Œæ— éœ€æ–‡æœ¬ç”Ÿæˆ")
    print("âœ… å¯æ‰©å±•å­˜å‚¨ï¼ŒæŒä¹…åŒ–ä¿å­˜")

if __name__ == "__main__":
    run_comprehensive_test() 