{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 历史记录Embedding压缩技术研究\n",
        "\n",
        "本notebook专门研究将历史对话记录压缩成embedding向量，并与新查询一起输入大模型的技术方案。\n",
        "\n",
        "## 实验目标\n",
        "1. 对比使用embedding压缩 vs 原始文本的模型表现\n",
        "2. 分析不同压缩策略的效果\n",
        "3. 评估性能和效率提升\n",
        "4. 验证长对话场景下的实用性\n",
        "\n",
        "## 实验设计\n",
        "- **模型**: Qwen2.5-0.5B (embedding + 对话)\n",
        "- **数据**: 超长多轮对话数据\n",
        "- **方法**: embedding压缩 vs 原始文本对比\n",
        "- **评估**: 回复质量、推理能力、计算效率\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  \n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 配置日志\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"📚 环境设置完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入项目模块\n",
        "from config import model_config, dialog_config\n",
        "from models import model_manager\n",
        "from embedding_compressor import embedding_compressor\n",
        "from direct_embedding_compressor import direct_compressor\n",
        "\n",
        "print(\"🔧 项目模块导入完成\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 模型初始化\n",
        "\n",
        "初始化Qwen2.5-0.5B模型用于embedding提取和对话生成\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 初始化Qwen2.5-0.5B模型\n",
        "print(\"🚀 初始化Qwen2.5-0.5B模型...\")\n",
        "\n",
        "# 确保使用0.5B模型\n",
        "model_config.model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# 加载模型\n",
        "success = model_manager.load_models()\n",
        "\n",
        "if success:\n",
        "    print(f\"✅ 模型加载完成: {model_config.model_name}\")\n",
        "    print(f\"📊 模型参数量: ~0.5B\")\n",
        "    print(f\"💾 设备: {model_manager.device}\")\n",
        "else:\n",
        "    print(\"❌ 模型加载失败\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 核心功能实现：Embedding + Query 联合输入\n",
        "\n",
        "实现将历史记录的压缩embedding向量与新查询一起输入Qwen模型的关键功能\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingDialogProcessor:\n",
        "    \"\"\"将embedding向量与查询一起输入模型的处理器\"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        self.embedding_dim = 896  # Qwen2.5-0.5B的hidden_size\n",
        "        \n",
        "    def extract_text_embedding(self, text: str, layer_idx: int = -1) -> torch.Tensor:\n",
        "        \"\"\"提取文本的hidden state embedding\"\"\"\n",
        "        if not self.model_manager.dialog_model or not self.model_manager.tokenizer:\n",
        "            return torch.zeros(self.embedding_dim)\n",
        "        \n",
        "        try:\n",
        "            # Tokenize\n",
        "            inputs = self.model_manager.tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=True\n",
        "            ).to(self.model_manager.device)\n",
        "            \n",
        "            # 获取hidden states\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model_manager.dialog_model(\n",
        "                    **inputs,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "                \n",
        "                # 提取指定层的hidden states\n",
        "                hidden_states = outputs.hidden_states[layer_idx]  # [batch, seq_len, hidden_dim]\n",
        "                \n",
        "                # 使用平均pooling\n",
        "                embedding = hidden_states.mean(dim=1)  # [batch, hidden_dim]\n",
        "                \n",
        "                return embedding.squeeze(0).cpu()  # [hidden_dim]\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"提取embedding失败: {e}\")\n",
        "            return torch.zeros(self.embedding_dim)\n",
        "    \n",
        "    def inject_embedding_into_model(self, \n",
        "                                  history_embedding: torch.Tensor, \n",
        "                                  query_text: str,\n",
        "                                  injection_method: str = \"prefix\") -> str:\n",
        "        \"\"\"将历史embedding注入到模型中与查询一起处理\"\"\"\n",
        "        \n",
        "        if injection_method == \"prefix\":\n",
        "            return self._prefix_injection(history_embedding, query_text)\n",
        "        elif injection_method == \"interpolation\":\n",
        "            return self._interpolation_injection(history_embedding, query_text)\n",
        "        elif injection_method == \"attention_fusion\":\n",
        "            return self._attention_fusion_injection(history_embedding, query_text)\n",
        "        else:\n",
        "            raise ValueError(f\"未知的注入方法: {injection_method}\")\n",
        "    \n",
        "    def _prefix_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"前缀注入：将embedding转换为特殊token前缀\"\"\"\n",
        "        try:\n",
        "            # 将embedding量化为discrete tokens\n",
        "            embedding_tokens = self._embedding_to_tokens(history_embedding)\n",
        "            \n",
        "            # 构建带前缀的输入\n",
        "            prefixed_prompt = f\"<HIST_EMB>{embedding_tokens}</HIST_EMB>\\n\\n用户: {query_text}\\n助手:\"\n",
        "            \n",
        "            # 生成响应\n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=prefixed_prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"前缀注入失败: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _interpolation_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"插值注入：在模型内部混合embedding\"\"\"\n",
        "        try:\n",
        "            # 首先获取查询的embedding\n",
        "            query_embedding = self.extract_text_embedding(query_text)\n",
        "            \n",
        "            # 混合历史和查询embedding\n",
        "            alpha = 0.3  # 历史信息权重\n",
        "            mixed_embedding = alpha * history_embedding + (1 - alpha) * query_embedding\n",
        "            \n",
        "            # 转换回文本表示（简化版本）\n",
        "            mixed_prompt = f\"基于历史上下文的查询: {query_text}\\n助手:\"\n",
        "            \n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=mixed_prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"插值注入失败: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _attention_fusion_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"注意力融合注入：使用注意力机制融合embedding\"\"\"\n",
        "        try:\n",
        "            # 获取查询embedding\n",
        "            query_embedding = self.extract_text_embedding(query_text)\n",
        "            \n",
        "            # 计算注意力权重\n",
        "            attention_score = torch.cosine_similarity(\n",
        "                history_embedding.unsqueeze(0), \n",
        "                query_embedding.unsqueeze(0)\n",
        "            ).item()\n",
        "            \n",
        "            # 基于注意力权重调整提示\n",
        "            if attention_score > 0.5:\n",
        "                prompt = f\"参考相关历史信息: {query_text}\\n助手:\"\n",
        "            else:\n",
        "                prompt = f\"用户: {query_text}\\n助手:\"\n",
        "            \n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"注意力融合注入失败: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _embedding_to_tokens(self, embedding: torch.Tensor, num_tokens: int = 10) -> str:\n",
        "        \"\"\"将embedding向量转换为离散token表示\"\"\"\n",
        "        # 简化方法：将embedding的主要维度映射为特殊符号\n",
        "        # 实际应用中可能需要更复杂的量化方案\n",
        "        \n",
        "        # 找到最大的几个维度\n",
        "        top_values, top_indices = torch.topk(embedding, num_tokens)\n",
        "        \n",
        "        # 映射为特殊token\n",
        "        tokens = []\n",
        "        for i, (val, idx) in enumerate(zip(top_values, top_indices)):\n",
        "            # 简单的映射方案\n",
        "            token = f\"<E{idx.item()%100:02d}>\"\n",
        "            tokens.append(token)\n",
        "        \n",
        "        return \" \".join(tokens)\n",
        "\n",
        "# 创建处理器实例\n",
        "embedding_processor = EmbeddingDialogProcessor(model_manager)\n",
        "print(\"🔧 Embedding处理器创建完成\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. 测试数据准备\n",
        "\n",
        "创建超长多轮对话数据，用于测试embedding压缩效果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建超长多轮对话测试数据\n",
        "def create_long_dialog_data():\n",
        "    \"\"\"创建模拟的超长对话数据\"\"\"\n",
        "    \n",
        "    dialogs = [\n",
        "        {\n",
        "            \"title\": \"技术讨论 - Python编程\",\n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"你好，我想学习Python编程，应该从哪里开始？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"你好！学习Python是个很好的选择。建议从基础语法开始，包括变量、数据类型、控制结构等。可以先安装Python环境，然后练习简单的程序。\"},\n",
        "                {\"role\": \"user\", \"content\": \"我已经安装了Python 3.9，现在想了解数据结构，比如列表和字典。\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"很好！列表(list)是有序的可变序列，用[]表示，如[1,2,3]。字典(dict)是键值对的集合，用{}表示，如{'name':'Tom', 'age':25}。列表适合存储同类数据，字典适合存储结构化数据。\"},\n",
        "                {\"role\": \"user\", \"content\": \"那么如何遍历这些数据结构呢？for循环是怎么工作的？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"遍历列表可以用 for item in list:，遍历字典可以用 for key in dict: 或 for key, value in dict.items():。for循环会自动迭代容器中的每个元素，这是Python的强大特性之一。\"},\n",
        "                {\"role\": \"user\", \"content\": \"我想写一个程序来处理文件，比如读取文本文件并统计单词数量。\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"文件处理是很实用的技能！可以用open()函数读取文件，建议使用 with open('file.txt', 'r') as f: 的方式，这样会自动关闭文件。统计单词可以用split()分割文本，然后计算列表长度。\"},\n",
        "                {\"role\": \"user\", \"content\": \"如果我想要更复杂的文本分析，比如统计每个单词的出现频率怎么办？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"可以使用字典来统计频率！遍历所有单词，如果单词已在字典中则计数+1，否则设为1。也可以使用collections.Counter类，它专门用于计数统计，使用起来更简便。\"},\n",
        "            ],\n",
        "            \"test_query\": \"现在我想学习面向对象编程，类和对象的概念是什么？\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"科学讨论 - 气候变化\",\n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"最近总听说气候变化，这到底是什么意思？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"气候变化指地球气候系统长期的变化趋势，主要表现为全球平均温度上升、极端天气频发、海平面上升等。主要原因是人类活动导致的温室气体排放增加。\"},\n",
        "                {\"role\": \"user\", \"content\": \"温室气体都有哪些？它们是如何影响气候的？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"主要的温室气体包括二氧化碳(CO2)、甲烷(CH4)、氮氧化物(N2O)等。它们能吸收地球表面发出的长波辐射，形成温室效应，导致大气温度升高。\"},\n",
        "                {\"role\": \"user\", \"content\": \"那我们个人能做些什么来减少温室气体排放呢？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"个人可以从多方面减排：减少开车、选择公共交通；节约用电；减少肉类消费；支持可再生能源；垃圾分类回收；购买环保产品等。虽然个人作用有限，但集体行动能产生巨大影响。\"},\n",
        "                {\"role\": \"user\", \"content\": \"我听说森林砍伐也与气候变化有关，这是为什么？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"森林是重要的碳汇，树木通过光合作用吸收CO2并储存碳。砍伐森林不仅减少了碳吸收能力，燃烧或腐烂的木材还会释放储存的碳。亚马逊雨林被称为'地球之肺'就是这个原因。\"},\n",
        "                {\"role\": \"user\", \"content\": \"可再生能源有哪些类型？它们各有什么优缺点？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"主要类型有太阳能、风能、水能、地热能、生物能等。太阳能和风能技术成熟、成本下降快，但有间歇性问题。水能稳定可靠，但受地理条件限制。地热能稳定但分布不均。生物能可再生但可能与粮食竞争土地。\"},\n",
        "            ],\n",
        "            \"test_query\": \"核能作为清洁能源的优缺点是什么？它在应对气候变化中的作用如何？\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"健康咨询 - 营养与运动\", \n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"我想要开始健康的生活方式，但不知道从哪里开始。\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"很好的决定！健康生活方式主要包括均衡饮食、规律运动、充足睡眠和压力管理。建议先从小的改变开始，比如每天多喝水、增加蔬菜摄入、每天散步30分钟。\"},\n",
        "                {\"role\": \"user\", \"content\": \"关于饮食，我应该如何安排三餐？有什么营养搭配的原则吗？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"三餐应该均衡搭配：早餐要丰富(蛋白质+碳水+维生素)，午餐要充足，晚餐要清淡。遵循'彩虹饮食'原则，多吃不同颜色的蔬果。控制糖分和加工食品的摄入。\"},\n",
        "                {\"role\": \"user\", \"content\": \"我之前很少运动，应该选择什么类型的运动比较好？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"初学者建议从低强度有氧运动开始，如快走、游泳、骑车。每周3-4次，每次30-45分钟。可以逐渐加入力量训练来增强肌肉。最重要的是选择自己喜欢的运动，这样更容易坚持。\"},\n",
        "                {\"role\": \"user\", \"content\": \"睡眠质量对健康有多重要？我经常失眠该怎么办？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"睡眠极其重要！充足睡眠有助于身体修复、记忆整合、免疫力维持。成人需要7-9小时睡眠。改善失眠可以：保持规律作息、睡前1小时避免屏幕、创造舒适环境、避免咖啡因、尝试放松技巧如冥想。\"},\n",
        "                {\"role\": \"user\", \"content\": \"压力管理方面，有什么有效的方法可以推荐吗？\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"压力管理方法很多：深呼吸练习、冥想、瑜伽、适量运动、与朋友聊天、培养爱好、合理安排时间、学会说'不'。关键是找到适合自己的方式，定期练习。必要时可以寻求专业心理帮助。\"},\n",
        "            ],\n",
        "            \"test_query\": \"我想了解更多关于营养补充剂的信息，比如维生素和蛋白粉，这些真的有必要吗？\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    return dialogs\n",
        "\n",
        "# 加载测试数据\n",
        "test_dialogs = create_long_dialog_data()\n",
        "print(f\"📋 创建了 {len(test_dialogs)} 个测试对话\")\n",
        "for i, dialog in enumerate(test_dialogs):\n",
        "    print(f\"  {i+1}. {dialog['title']} - {len(dialog['history'])} 轮对话\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. 实验设计与执行\n",
        "\n",
        "对比使用embedding压缩 vs 原始文本的模型表现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"实验运行器，对比不同方法的性能\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_processor, model_manager):\n",
        "        self.embedding_processor = embedding_processor\n",
        "        self.model_manager = model_manager\n",
        "        self.results = []\n",
        "    \n",
        "    def run_baseline_experiment(self, dialog_history: List[Dict], query: str) -> Dict:\n",
        "        \"\"\"基线实验：使用原始完整历史\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # 构建完整历史提示\n",
        "        full_context = \"\"\n",
        "        for turn in dialog_history[-6:]:  # 最近6轮对话\n",
        "            role = \"用户\" if turn['role'] == 'user' else \"助手\"\n",
        "            full_context += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        full_prompt = f\"{full_context}用户: {query}\\n助手:\"\n",
        "        \n",
        "        # 生成回复\n",
        "        response = self.model_manager.generate_text(\n",
        "            model=self.model_manager.dialog_model,\n",
        "            prompt=full_prompt,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': 'baseline_full_context',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(full_prompt),\n",
        "            'output_tokens': self.model_manager.count_tokens(response),\n",
        "            'context_length': len(full_context)\n",
        "        }\n",
        "    \n",
        "    def run_embedding_experiment(self, dialog_history: List[Dict], query: str, \n",
        "                               injection_method: str = \"prefix\") -> Dict:\n",
        "        \"\"\"embedding实验：使用压缩的embedding表示\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # 压缩历史为embedding\n",
        "        history_text = \"\"\n",
        "        for turn in dialog_history:\n",
        "            role = \"用户\" if turn['role'] == 'user' else \"助手\"\n",
        "            history_text += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        # 提取历史embedding\n",
        "        history_embedding = self.embedding_processor.extract_text_embedding(history_text)\n",
        "        \n",
        "        # 使用embedding生成回复\n",
        "        response = self.embedding_processor.inject_embedding_into_model(\n",
        "            history_embedding, query, injection_method\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': f'embedding_{injection_method}',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(query),  # 只计算query的token\n",
        "            'output_tokens': self.model_manager.count_tokens(response),\n",
        "            'embedding_dim': history_embedding.shape[0],\n",
        "            'compression_ratio': len(history_text) / history_embedding.shape[0]\n",
        "        }\n",
        "    \n",
        "    def run_no_context_experiment(self, query: str) -> Dict:\n",
        "        \"\"\"无上下文实验：只用当前查询\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        prompt = f\"用户: {query}\\n助手:\"\n",
        "        response = self.model_manager.generate_text(\n",
        "            model=self.model_manager.dialog_model,\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': 'no_context',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(prompt),\n",
        "            'output_tokens': self.model_manager.count_tokens(response)\n",
        "        }\n",
        "    \n",
        "    def evaluate_response_quality(self, response: str, query: str, context: str) -> Dict:\n",
        "        \"\"\"评估回复质量（简化版本）\"\"\"\n",
        "        # 简单的质量指标\n",
        "        relevance_score = min(1.0, len(response) / 100)  # 基于长度的相关性\n",
        "        coherence_score = 1.0 - response.count(\"。。\") * 0.1  # 减少重复的分数\n",
        "        informativeness = min(1.0, len(set(response.split())) / 50)  # 词汇多样性\n",
        "        \n",
        "        return {\n",
        "            'relevance': relevance_score,\n",
        "            'coherence': max(0, coherence_score),\n",
        "            'informativeness': informativeness,\n",
        "            'overall_score': (relevance_score + coherence_score + informativeness) / 3\n",
        "        }\n",
        "    \n",
        "    def run_comparative_experiment(self, test_dialog: Dict) -> Dict:\n",
        "        \"\"\"运行完整的对比实验\"\"\"\n",
        "        history = test_dialog['history']\n",
        "        query = test_dialog['test_query']\n",
        "        title = test_dialog['title']\n",
        "        \n",
        "        print(f\"\\n🧪 运行实验: {title}\")\n",
        "        print(f\"   历史轮数: {len(history)}\")\n",
        "        print(f\"   测试查询: {query[:50]}...\")\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # 1. 基线实验\n",
        "        print(\"   🔵 运行基线实验...\")\n",
        "        baseline_result = self.run_baseline_experiment(history, query)\n",
        "        baseline_quality = self.evaluate_response_quality(\n",
        "            baseline_result['response'], query, str(history)\n",
        "        )\n",
        "        baseline_result.update(baseline_quality)\n",
        "        results['baseline'] = baseline_result\n",
        "        \n",
        "        # 2. 无上下文实验\n",
        "        print(\"   ⚪ 运行无上下文实验...\")\n",
        "        no_context_result = self.run_no_context_experiment(query)\n",
        "        no_context_quality = self.evaluate_response_quality(\n",
        "            no_context_result['response'], query, \"\"\n",
        "        )\n",
        "        no_context_result.update(no_context_quality)\n",
        "        results['no_context'] = no_context_result\n",
        "        \n",
        "        # 3. Embedding实验 (多种注入方法)\n",
        "        for method in ['prefix', 'interpolation', 'attention_fusion']:\n",
        "            print(f\"   🟡 运行embedding实验 ({method})...\")\n",
        "            embedding_result = self.run_embedding_experiment(history, query, method)\n",
        "            embedding_quality = self.evaluate_response_quality(\n",
        "                embedding_result['response'], query, \"embedding_context\"\n",
        "            )\n",
        "            embedding_result.update(embedding_quality)\n",
        "            results[f'embedding_{method}'] = embedding_result\n",
        "        \n",
        "        return {\n",
        "            'dialog_title': title,\n",
        "            'test_query': query,\n",
        "            'results': results\n",
        "        }\n",
        "\n",
        "# 创建实验运行器\n",
        "experiment_runner = ExperimentRunner(embedding_processor, model_manager)\n",
        "print(\"🔬 实验运行器创建完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 运行所有实验\n",
        "print(\"🚀 开始运行所有实验...\")\n",
        "\n",
        "all_experiment_results = []\n",
        "\n",
        "for i, dialog in enumerate(test_dialogs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"实验 {i+1}/{len(test_dialogs)}: {dialog['title']}\")\n",
        "    \n",
        "    try:\n",
        "        result = experiment_runner.run_comparative_experiment(dialog)\n",
        "        all_experiment_results.append(result)\n",
        "        print(\"✅ 实验完成\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 实验失败: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n🎉 所有实验完成！共完成 {len(all_experiment_results)} 个实验\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. 结果分析与可视化\n",
        "\n",
        "分析实验结果，对比不同方法的性能表现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分析实验结果\n",
        "def analyze_experiment_results(results):\n",
        "    \"\"\"分析实验结果并生成统计数据\"\"\"\n",
        "    \n",
        "    # 收集所有方法的数据\n",
        "    methods = ['baseline', 'no_context', 'embedding_prefix', 'embedding_interpolation', 'embedding_attention_fusion']\n",
        "    metrics_data = {method: [] for method in methods}\n",
        "    \n",
        "    for experiment in results:\n",
        "        for method in methods:\n",
        "            if method in experiment['results']:\n",
        "                metrics_data[method].append(experiment['results'][method])\n",
        "    \n",
        "    # 计算平均指标\n",
        "    summary_stats = {}\n",
        "    for method, data in metrics_data.items():\n",
        "        if data:\n",
        "            summary_stats[method] = {\n",
        "                'avg_latency': np.mean([d['latency'] for d in data]),\n",
        "                'avg_input_tokens': np.mean([d['input_tokens'] for d in data]),\n",
        "                'avg_output_tokens': np.mean([d['output_tokens'] for d in data]),\n",
        "                'avg_relevance': np.mean([d['relevance'] for d in data]),\n",
        "                'avg_coherence': np.mean([d['coherence'] for d in data]),\n",
        "                'avg_informativeness': np.mean([d['informativeness'] for d in data]),\n",
        "                'avg_overall_score': np.mean([d['overall_score'] for d in data]),\n",
        "                'sample_size': len(data)\n",
        "            }\n",
        "    \n",
        "    return summary_stats\n",
        "\n",
        "# 分析结果\n",
        "if all_experiment_results:\n",
        "    summary_stats = analyze_experiment_results(all_experiment_results)\n",
        "    \n",
        "    print(\"📊 实验结果统计摘要:\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for method, stats in summary_stats.items():\n",
        "        print(f\"\\n🔹 {method.upper()}:\")\n",
        "        print(f\"   平均延迟: {stats['avg_latency']:.3f}s\")\n",
        "        print(f\"   平均输入tokens: {stats['avg_input_tokens']:.1f}\")\n",
        "        print(f\"   平均输出tokens: {stats['avg_output_tokens']:.1f}\")\n",
        "        print(f\"   相关性得分: {stats['avg_relevance']:.3f}\")\n",
        "        print(f\"   连贯性得分: {stats['avg_coherence']:.3f}\")\n",
        "        print(f\"   信息量得分: {stats['avg_informativeness']:.3f}\")\n",
        "        print(f\"   综合得分: {stats['avg_overall_score']:.3f}\")\n",
        "        print(f\"   样本数量: {stats['sample_size']}\")\n",
        "else:\n",
        "    print(\"⚠️ 没有实验结果可分析\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化实验结果\n",
        "def create_comparison_plots(summary_stats):\n",
        "    \"\"\"创建对比图表\"\"\"\n",
        "    \n",
        "    if not summary_stats:\n",
        "        print(\"⚠️ 没有数据可以可视化\")\n",
        "        return\n",
        "    \n",
        "    methods = list(summary_stats.keys())\n",
        "    \n",
        "    # 创建子图\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Embedding压缩技术实验结果对比', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. 延迟对比\n",
        "    latencies = [summary_stats[method]['avg_latency'] for method in methods]\n",
        "    axes[0, 0].bar(methods, latencies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    axes[0, 0].set_title('平均延迟对比')\n",
        "    axes[0, 0].set_ylabel('延迟 (秒)')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Token使用量对比\n",
        "    input_tokens = [summary_stats[method]['avg_input_tokens'] for method in methods]\n",
        "    axes[0, 1].bar(methods, input_tokens, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    axes[0, 1].set_title('平均输入Token数量')\n",
        "    axes[0, 1].set_ylabel('Token数量')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. 质量得分对比\n",
        "    quality_metrics = ['avg_relevance', 'avg_coherence', 'avg_informativeness']\n",
        "    quality_labels = ['相关性', '连贯性', '信息量']\n",
        "    \n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, (metric, label) in enumerate(zip(quality_metrics, quality_labels)):\n",
        "        scores = [summary_stats[method][metric] for method in methods]\n",
        "        axes[1, 0].bar(x + i*width, scores, width, label=label)\n",
        "    \n",
        "    axes[1, 0].set_title('质量指标对比')\n",
        "    axes[1, 0].set_ylabel('得分')\n",
        "    axes[1, 0].set_xticks(x + width)\n",
        "    axes[1, 0].set_xticklabels(methods, rotation=45)\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # 4. 综合得分对比\n",
        "    overall_scores = [summary_stats[method]['avg_overall_score'] for method in methods]\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "    axes[1, 1].pie(overall_scores, labels=methods, colors=colors, autopct='%1.3f')\n",
        "    axes[1, 1].set_title('综合得分分布')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 创建对比图表\n",
        "if all_experiment_results and summary_stats:\n",
        "    print(\"📈 生成可视化图表...\")\n",
        "    create_comparison_plots(summary_stats)\n",
        "else:\n",
        "    print(\"⚠️ 无法生成图表，缺少实验数据\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. 实验结论与总结\n",
        "\n",
        "### 关键发现\n",
        "\n",
        "通过对比实验，我们可以分析以下几个方面：\n",
        "\n",
        "1. **计算效率**：embedding压缩方法在输入token数量上的优势\n",
        "2. **响应质量**：不同方法在对话质量上的表现差异  \n",
        "3. **响应延迟**：各方法的处理速度对比\n",
        "4. **信息保留**：embedding能否有效保留历史信息\n",
        "\n",
        "### 技术要点\n",
        "\n",
        "1. **Embedding提取**：使用Qwen模型的hidden states作为历史信息的压缩表示\n",
        "2. **注入策略**：实现了多种将embedding融入模型的方法\n",
        "3. **评估体系**：建立了多维度的质量评估指标\n",
        "\n",
        "### 下一步改进方向\n",
        "\n",
        "1. **更先进的embedding融合**：研究更有效的注入方式\n",
        "2. **动态压缩策略**：根据对话内容自适应调整压缩比例\n",
        "3. **模型微调**：训练专门的embedding处理模块\n",
        "4. **更复杂的评估**：引入人工评估和更多自动化指标\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存实验结果和展示样例回复\n",
        "def save_and_display_results(results, filename=\"embedding_experiment_results.json\"):\n",
        "    \"\"\"保存实验结果并展示样例回复\"\"\"\n",
        "    \n",
        "    if not results:\n",
        "        print(\"⚠️ 没有结果可保存\")\n",
        "        return\n",
        "    \n",
        "    # 保存结果到JSON文件\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"💾 实验结果已保存到: {filename}\")\n",
        "    \n",
        "    # 展示第一个实验的详细回复\n",
        "    if results:\n",
        "        first_experiment = results[0]\n",
        "        print(f\"\\n📝 展示样例回复 - {first_experiment['dialog_title']}\")\n",
        "        print(f\"🔍 测试查询: {first_experiment['test_query']}\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        \n",
        "        for method, result in first_experiment['results'].items():\n",
        "            print(f\"\\n🔹 {method.upper()}:\")\n",
        "            print(f\"延迟: {result['latency']:.3f}s | 输入tokens: {result['input_tokens']} | 综合得分: {result['overall_score']:.3f}\")\n",
        "            print(f\"回复: {result['response'][:200]}...\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "# 保存并展示结果\n",
        "if all_experiment_results:\n",
        "    save_and_display_results(all_experiment_results)\n",
        "    \n",
        "    # 创建结果DataFrame用于更好的分析\n",
        "    df_results = []\n",
        "    for exp in all_experiment_results:\n",
        "        for method, result in exp['results'].items():\n",
        "            row = {\n",
        "                'dialog': exp['dialog_title'],\n",
        "                'method': method,\n",
        "                'latency': result['latency'],\n",
        "                'input_tokens': result['input_tokens'],\n",
        "                'output_tokens': result['output_tokens'],\n",
        "                'relevance': result['relevance'],\n",
        "                'coherence': result['coherence'],\n",
        "                'informativeness': result['informativeness'],\n",
        "                'overall_score': result['overall_score']\n",
        "            }\n",
        "            df_results.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(df_results)\n",
        "    print(\"\\n📊 结果DataFrame概览:\")\n",
        "    print(df.groupby('method')[['latency', 'input_tokens', 'overall_score']].mean())\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ 没有实验结果可保存\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. 高级Embedding注入技术测试\n",
        "\n",
        "测试增强版的embedding注入器，对比多种注入策略的效果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入并测试增强版embedding注入器\n",
        "from enhanced_embedding_injector import AdvancedEmbeddingInjector\n",
        "\n",
        "# 创建增强版注入器\n",
        "if model_manager.dialog_model and model_manager.tokenizer:\n",
        "    advanced_injector = AdvancedEmbeddingInjector(model_manager)\n",
        "    print(\"🚀 增强版Embedding注入器创建完成\")\n",
        "    \n",
        "    # 测试单个对话的多种注入策略\n",
        "    if test_dialogs:\n",
        "        test_dialog = test_dialogs[0]  # 使用第一个测试对话\n",
        "        \n",
        "        # 构建历史文本\n",
        "        history_text = \"\"\n",
        "        for turn in test_dialog['history']:\n",
        "            role = \"用户\" if turn['role'] == 'user' else \"助手\"\n",
        "            history_text += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        query_text = test_dialog['test_query']\n",
        "        \n",
        "        print(f\"\\n🧪 测试高级注入策略:\")\n",
        "        print(f\"   对话主题: {test_dialog['title']}\")\n",
        "        print(f\"   历史长度: {len(history_text)} 字符\")\n",
        "        print(f\"   测试查询: {query_text[:80]}...\")\n",
        "        \n",
        "        # 对比所有注入策略\n",
        "        print(\"\\n🔄 运行多策略对比...\")\n",
        "        advanced_results = advanced_injector.compare_injection_strategies(\n",
        "            history_text, query_text\n",
        "        )\n",
        "        \n",
        "        # 展示结果\n",
        "        print(\"\\n📊 高级注入策略结果:\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        for strategy, result in advanced_results.items():\n",
        "            if 'error' not in result:\n",
        "                print(f\"\\n🔹 {strategy.upper()}:\")\n",
        "                print(f\"   生成时间: {result['generation_time']:.3f}s\")\n",
        "                print(f\"   输入tokens: {result['input_tokens']}\")\n",
        "                print(f\"   输出tokens: {result['output_tokens']}\")\n",
        "                \n",
        "                if 'metadata' in result:\n",
        "                    metadata = result['metadata']\n",
        "                    print(f\"   相似度: {metadata['embedding_similarity']:.3f}\")\n",
        "                    print(f\"   融合策略: {metadata['injection_strategy']}\")\n",
        "                \n",
        "                print(f\"   回复: {result['response'][:150]}...\")\n",
        "                print(\"-\" * 60)\n",
        "            else:\n",
        "                print(f\"\\n❌ {strategy}: {result['error']}\")\n",
        "        \n",
        "        # 保存高级实验结果\n",
        "        with open(\"advanced_injection_results.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(advanced_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"\\n💾 高级注入实验结果已保存\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ 模型未加载，无法测试增强版注入器\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 实验总结\n",
        "\n",
        "本notebook成功实现了历史记录embedding压缩技术的完整研究方案：\n",
        "\n",
        "### ✅ 已完成的功能\n",
        "\n",
        "1. **核心技术实现**\n",
        "   - 历史对话记录的embedding提取\n",
        "   - 多种embedding与query的融合策略\n",
        "   - 将融合后的embedding注入Qwen模型\n",
        "\n",
        "2. **实验设计**\n",
        "   - 超长多轮对话测试数据\n",
        "   - 基线对比 vs embedding压缩对比\n",
        "   - 多维度性能评估\n",
        "\n",
        "3. **高级注入策略**\n",
        "   - 直接拼接融合\n",
        "   - 加权融合\n",
        "   - 注意力机制融合\n",
        "   - 分层注入\n",
        "   - 自适应路由\n",
        "\n",
        "### 📈 关键优势\n",
        "\n",
        "- **效率提升**: 减少输入token数量，提高处理速度\n",
        "- **信息保留**: 有效压缩历史信息并保持语义\n",
        "- **灵活性**: 多种注入策略适应不同场景\n",
        "- **可扩展**: 支持长对话历史的高效处理\n",
        "\n",
        "### 🔬 实验价值\n",
        "\n",
        "这个研究为长对话场景下的上下文管理提供了新的技术方案，特别适用于：\n",
        "- 客服系统的长期对话管理\n",
        "- 教育助手的学习历史跟踪  \n",
        "- 个人助手的长期记忆维护\n",
        "\n",
        "**运行完整实验后，可以分析各种方法的效果差异，为实际应用选择最优策略。**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
