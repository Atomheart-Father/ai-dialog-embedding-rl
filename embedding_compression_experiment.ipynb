{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# å†å²è®°å½•Embeddingå‹ç¼©æŠ€æœ¯ç ”ç©¶\n",
        "\n",
        "æœ¬notebookä¸“é—¨ç ”ç©¶å°†å†å²å¯¹è¯è®°å½•å‹ç¼©æˆembeddingå‘é‡ï¼Œå¹¶ä¸æ–°æŸ¥è¯¢ä¸€èµ·è¾“å…¥å¤§æ¨¡å‹çš„æŠ€æœ¯æ–¹æ¡ˆã€‚\n",
        "\n",
        "## å®éªŒç›®æ ‡\n",
        "1. å¯¹æ¯”ä½¿ç”¨embeddingå‹ç¼© vs åŸå§‹æ–‡æœ¬çš„æ¨¡å‹è¡¨ç°\n",
        "2. åˆ†æä¸åŒå‹ç¼©ç­–ç•¥çš„æ•ˆæœ\n",
        "3. è¯„ä¼°æ€§èƒ½å’Œæ•ˆç‡æå‡\n",
        "4. éªŒè¯é•¿å¯¹è¯åœºæ™¯ä¸‹çš„å®ç”¨æ€§\n",
        "\n",
        "## å®éªŒè®¾è®¡\n",
        "- **æ¨¡å‹**: Qwen2.5-0.5B (embedding + å¯¹è¯)\n",
        "- **æ•°æ®**: è¶…é•¿å¤šè½®å¯¹è¯æ•°æ®\n",
        "- **æ–¹æ³•**: embeddingå‹ç¼© vs åŸå§‹æ–‡æœ¬å¯¹æ¯”\n",
        "- **è¯„ä¼°**: å›å¤è´¨é‡ã€æ¨ç†èƒ½åŠ›ã€è®¡ç®—æ•ˆç‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  \n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ğŸ“š ç¯å¢ƒè®¾ç½®å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
        "from config import model_config, dialog_config\n",
        "from models import model_manager\n",
        "from embedding_compressor import embedding_compressor\n",
        "from direct_embedding_compressor import direct_compressor\n",
        "\n",
        "print(\"ğŸ”§ é¡¹ç›®æ¨¡å—å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. æ¨¡å‹åˆå§‹åŒ–\n",
        "\n",
        "åˆå§‹åŒ–Qwen2.5-0.5Bæ¨¡å‹ç”¨äºembeddingæå–å’Œå¯¹è¯ç”Ÿæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå§‹åŒ–Qwen2.5-0.5Bæ¨¡å‹\n",
        "print(\"ğŸš€ åˆå§‹åŒ–Qwen2.5-0.5Bæ¨¡å‹...\")\n",
        "\n",
        "# ç¡®ä¿ä½¿ç”¨0.5Bæ¨¡å‹\n",
        "model_config.model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# åŠ è½½æ¨¡å‹\n",
        "success = model_manager.load_models()\n",
        "\n",
        "if success:\n",
        "    print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_config.model_name}\")\n",
        "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°é‡: ~0.5B\")\n",
        "    print(f\"ğŸ’¾ è®¾å¤‡: {model_manager.device}\")\n",
        "else:\n",
        "    print(\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. æ ¸å¿ƒåŠŸèƒ½å®ç°ï¼šEmbedding + Query è”åˆè¾“å…¥\n",
        "\n",
        "å®ç°å°†å†å²è®°å½•çš„å‹ç¼©embeddingå‘é‡ä¸æ–°æŸ¥è¯¢ä¸€èµ·è¾“å…¥Qwenæ¨¡å‹çš„å…³é”®åŠŸèƒ½\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingDialogProcessor:\n",
        "    \"\"\"å°†embeddingå‘é‡ä¸æŸ¥è¯¢ä¸€èµ·è¾“å…¥æ¨¡å‹çš„å¤„ç†å™¨\"\"\"\n",
        "    \n",
        "    def __init__(self, model_manager):\n",
        "        self.model_manager = model_manager\n",
        "        self.embedding_dim = 896  # Qwen2.5-0.5Bçš„hidden_size\n",
        "        \n",
        "    def extract_text_embedding(self, text: str, layer_idx: int = -1) -> torch.Tensor:\n",
        "        \"\"\"æå–æ–‡æœ¬çš„hidden state embedding\"\"\"\n",
        "        if not self.model_manager.dialog_model or not self.model_manager.tokenizer:\n",
        "            return torch.zeros(self.embedding_dim)\n",
        "        \n",
        "        try:\n",
        "            # Tokenize\n",
        "            inputs = self.model_manager.tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=True\n",
        "            ).to(self.model_manager.device)\n",
        "            \n",
        "            # è·å–hidden states\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model_manager.dialog_model(\n",
        "                    **inputs,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "                \n",
        "                # æå–æŒ‡å®šå±‚çš„hidden states\n",
        "                hidden_states = outputs.hidden_states[layer_idx]  # [batch, seq_len, hidden_dim]\n",
        "                \n",
        "                # ä½¿ç”¨å¹³å‡pooling\n",
        "                embedding = hidden_states.mean(dim=1)  # [batch, hidden_dim]\n",
        "                \n",
        "                return embedding.squeeze(0).cpu()  # [hidden_dim]\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"æå–embeddingå¤±è´¥: {e}\")\n",
        "            return torch.zeros(self.embedding_dim)\n",
        "    \n",
        "    def inject_embedding_into_model(self, \n",
        "                                  history_embedding: torch.Tensor, \n",
        "                                  query_text: str,\n",
        "                                  injection_method: str = \"prefix\") -> str:\n",
        "        \"\"\"å°†å†å²embeddingæ³¨å…¥åˆ°æ¨¡å‹ä¸­ä¸æŸ¥è¯¢ä¸€èµ·å¤„ç†\"\"\"\n",
        "        \n",
        "        if injection_method == \"prefix\":\n",
        "            return self._prefix_injection(history_embedding, query_text)\n",
        "        elif injection_method == \"interpolation\":\n",
        "            return self._interpolation_injection(history_embedding, query_text)\n",
        "        elif injection_method == \"attention_fusion\":\n",
        "            return self._attention_fusion_injection(history_embedding, query_text)\n",
        "        else:\n",
        "            raise ValueError(f\"æœªçŸ¥çš„æ³¨å…¥æ–¹æ³•: {injection_method}\")\n",
        "    \n",
        "    def _prefix_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"å‰ç¼€æ³¨å…¥ï¼šå°†embeddingè½¬æ¢ä¸ºç‰¹æ®Štokenå‰ç¼€\"\"\"\n",
        "        try:\n",
        "            # å°†embeddingé‡åŒ–ä¸ºdiscrete tokens\n",
        "            embedding_tokens = self._embedding_to_tokens(history_embedding)\n",
        "            \n",
        "            # æ„å»ºå¸¦å‰ç¼€çš„è¾“å…¥\n",
        "            prefixed_prompt = f\"<HIST_EMB>{embedding_tokens}</HIST_EMB>\\n\\nç”¨æˆ·: {query_text}\\nåŠ©æ‰‹:\"\n",
        "            \n",
        "            # ç”Ÿæˆå“åº”\n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=prefixed_prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"å‰ç¼€æ³¨å…¥å¤±è´¥: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _interpolation_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"æ’å€¼æ³¨å…¥ï¼šåœ¨æ¨¡å‹å†…éƒ¨æ··åˆembedding\"\"\"\n",
        "        try:\n",
        "            # é¦–å…ˆè·å–æŸ¥è¯¢çš„embedding\n",
        "            query_embedding = self.extract_text_embedding(query_text)\n",
        "            \n",
        "            # æ··åˆå†å²å’ŒæŸ¥è¯¢embedding\n",
        "            alpha = 0.3  # å†å²ä¿¡æ¯æƒé‡\n",
        "            mixed_embedding = alpha * history_embedding + (1 - alpha) * query_embedding\n",
        "            \n",
        "            # è½¬æ¢å›æ–‡æœ¬è¡¨ç¤ºï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\n",
        "            mixed_prompt = f\"åŸºäºå†å²ä¸Šä¸‹æ–‡çš„æŸ¥è¯¢: {query_text}\\nåŠ©æ‰‹:\"\n",
        "            \n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=mixed_prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"æ’å€¼æ³¨å…¥å¤±è´¥: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _attention_fusion_injection(self, history_embedding: torch.Tensor, query_text: str) -> str:\n",
        "        \"\"\"æ³¨æ„åŠ›èåˆæ³¨å…¥ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èåˆembedding\"\"\"\n",
        "        try:\n",
        "            # è·å–æŸ¥è¯¢embedding\n",
        "            query_embedding = self.extract_text_embedding(query_text)\n",
        "            \n",
        "            # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n",
        "            attention_score = torch.cosine_similarity(\n",
        "                history_embedding.unsqueeze(0), \n",
        "                query_embedding.unsqueeze(0)\n",
        "            ).item()\n",
        "            \n",
        "            # åŸºäºæ³¨æ„åŠ›æƒé‡è°ƒæ•´æç¤º\n",
        "            if attention_score > 0.5:\n",
        "                prompt = f\"å‚è€ƒç›¸å…³å†å²ä¿¡æ¯: {query_text}\\nåŠ©æ‰‹:\"\n",
        "            else:\n",
        "                prompt = f\"ç”¨æˆ·: {query_text}\\nåŠ©æ‰‹:\"\n",
        "            \n",
        "            response = self.model_manager.generate_text(\n",
        "                model=self.model_manager.dialog_model,\n",
        "                prompt=prompt,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"æ³¨æ„åŠ›èåˆæ³¨å…¥å¤±è´¥: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def _embedding_to_tokens(self, embedding: torch.Tensor, num_tokens: int = 10) -> str:\n",
        "        \"\"\"å°†embeddingå‘é‡è½¬æ¢ä¸ºç¦»æ•£tokenè¡¨ç¤º\"\"\"\n",
        "        # ç®€åŒ–æ–¹æ³•ï¼šå°†embeddingçš„ä¸»è¦ç»´åº¦æ˜ å°„ä¸ºç‰¹æ®Šç¬¦å·\n",
        "        # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é‡åŒ–æ–¹æ¡ˆ\n",
        "        \n",
        "        # æ‰¾åˆ°æœ€å¤§çš„å‡ ä¸ªç»´åº¦\n",
        "        top_values, top_indices = torch.topk(embedding, num_tokens)\n",
        "        \n",
        "        # æ˜ å°„ä¸ºç‰¹æ®Štoken\n",
        "        tokens = []\n",
        "        for i, (val, idx) in enumerate(zip(top_values, top_indices)):\n",
        "            # ç®€å•çš„æ˜ å°„æ–¹æ¡ˆ\n",
        "            token = f\"<E{idx.item()%100:02d}>\"\n",
        "            tokens.append(token)\n",
        "        \n",
        "        return \" \".join(tokens)\n",
        "\n",
        "# åˆ›å»ºå¤„ç†å™¨å®ä¾‹\n",
        "embedding_processor = EmbeddingDialogProcessor(model_manager)\n",
        "print(\"ğŸ”§ Embeddingå¤„ç†å™¨åˆ›å»ºå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. æµ‹è¯•æ•°æ®å‡†å¤‡\n",
        "\n",
        "åˆ›å»ºè¶…é•¿å¤šè½®å¯¹è¯æ•°æ®ï¼Œç”¨äºæµ‹è¯•embeddingå‹ç¼©æ•ˆæœ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºè¶…é•¿å¤šè½®å¯¹è¯æµ‹è¯•æ•°æ®\n",
        "def create_long_dialog_data():\n",
        "    \"\"\"åˆ›å»ºæ¨¡æ‹Ÿçš„è¶…é•¿å¯¹è¯æ•°æ®\"\"\"\n",
        "    \n",
        "    dialogs = [\n",
        "        {\n",
        "            \"title\": \"æŠ€æœ¯è®¨è®º - Pythonç¼–ç¨‹\",\n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ä½ å¥½ï¼å­¦ä¹ Pythonæ˜¯ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚å»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼ŒåŒ…æ‹¬å˜é‡ã€æ•°æ®ç±»å‹ã€æ§åˆ¶ç»“æ„ç­‰ã€‚å¯ä»¥å…ˆå®‰è£…Pythonç¯å¢ƒï¼Œç„¶åç»ƒä¹ ç®€å•çš„ç¨‹åºã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘å·²ç»å®‰è£…äº†Python 3.9ï¼Œç°åœ¨æƒ³äº†è§£æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚åˆ—è¡¨å’Œå­—å…¸ã€‚\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"å¾ˆå¥½ï¼åˆ—è¡¨(list)æ˜¯æœ‰åºçš„å¯å˜åºåˆ—ï¼Œç”¨[]è¡¨ç¤ºï¼Œå¦‚[1,2,3]ã€‚å­—å…¸(dict)æ˜¯é”®å€¼å¯¹çš„é›†åˆï¼Œç”¨{}è¡¨ç¤ºï¼Œå¦‚{'name':'Tom', 'age':25}ã€‚åˆ—è¡¨é€‚åˆå­˜å‚¨åŒç±»æ•°æ®ï¼Œå­—å…¸é€‚åˆå­˜å‚¨ç»“æ„åŒ–æ•°æ®ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"é‚£ä¹ˆå¦‚ä½•éå†è¿™äº›æ•°æ®ç»“æ„å‘¢ï¼Ÿforå¾ªç¯æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"éå†åˆ—è¡¨å¯ä»¥ç”¨ for item in list:ï¼Œéå†å­—å…¸å¯ä»¥ç”¨ for key in dict: æˆ– for key, value in dict.items():ã€‚forå¾ªç¯ä¼šè‡ªåŠ¨è¿­ä»£å®¹å™¨ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œè¿™æ˜¯Pythonçš„å¼ºå¤§ç‰¹æ€§ä¹‹ä¸€ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘æƒ³å†™ä¸€ä¸ªç¨‹åºæ¥å¤„ç†æ–‡ä»¶ï¼Œæ¯”å¦‚è¯»å–æ–‡æœ¬æ–‡ä»¶å¹¶ç»Ÿè®¡å•è¯æ•°é‡ã€‚\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"æ–‡ä»¶å¤„ç†æ˜¯å¾ˆå®ç”¨çš„æŠ€èƒ½ï¼å¯ä»¥ç”¨open()å‡½æ•°è¯»å–æ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨ with open('file.txt', 'r') as f: çš„æ–¹å¼ï¼Œè¿™æ ·ä¼šè‡ªåŠ¨å…³é—­æ–‡ä»¶ã€‚ç»Ÿè®¡å•è¯å¯ä»¥ç”¨split()åˆ†å‰²æ–‡æœ¬ï¼Œç„¶åè®¡ç®—åˆ—è¡¨é•¿åº¦ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"å¦‚æœæˆ‘æƒ³è¦æ›´å¤æ‚çš„æ–‡æœ¬åˆ†æï¼Œæ¯”å¦‚ç»Ÿè®¡æ¯ä¸ªå•è¯çš„å‡ºç°é¢‘ç‡æ€ä¹ˆåŠï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"å¯ä»¥ä½¿ç”¨å­—å…¸æ¥ç»Ÿè®¡é¢‘ç‡ï¼éå†æ‰€æœ‰å•è¯ï¼Œå¦‚æœå•è¯å·²åœ¨å­—å…¸ä¸­åˆ™è®¡æ•°+1ï¼Œå¦åˆ™è®¾ä¸º1ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨collections.Counterç±»ï¼Œå®ƒä¸“é—¨ç”¨äºè®¡æ•°ç»Ÿè®¡ï¼Œä½¿ç”¨èµ·æ¥æ›´ç®€ä¾¿ã€‚\"},\n",
        "            ],\n",
        "            \"test_query\": \"ç°åœ¨æˆ‘æƒ³å­¦ä¹ é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œç±»å’Œå¯¹è±¡çš„æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"ç§‘å­¦è®¨è®º - æ°”å€™å˜åŒ–\",\n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"æœ€è¿‘æ€»å¬è¯´æ°”å€™å˜åŒ–ï¼Œè¿™åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"æ°”å€™å˜åŒ–æŒ‡åœ°çƒæ°”å€™ç³»ç»Ÿé•¿æœŸçš„å˜åŒ–è¶‹åŠ¿ï¼Œä¸»è¦è¡¨ç°ä¸ºå…¨çƒå¹³å‡æ¸©åº¦ä¸Šå‡ã€æç«¯å¤©æ°”é¢‘å‘ã€æµ·å¹³é¢ä¸Šå‡ç­‰ã€‚ä¸»è¦åŸå› æ˜¯äººç±»æ´»åŠ¨å¯¼è‡´çš„æ¸©å®¤æ°”ä½“æ’æ”¾å¢åŠ ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"æ¸©å®¤æ°”ä½“éƒ½æœ‰å“ªäº›ï¼Ÿå®ƒä»¬æ˜¯å¦‚ä½•å½±å“æ°”å€™çš„ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ä¸»è¦çš„æ¸©å®¤æ°”ä½“åŒ…æ‹¬äºŒæ°§åŒ–ç¢³(CO2)ã€ç”²çƒ·(CH4)ã€æ°®æ°§åŒ–ç‰©(N2O)ç­‰ã€‚å®ƒä»¬èƒ½å¸æ”¶åœ°çƒè¡¨é¢å‘å‡ºçš„é•¿æ³¢è¾å°„ï¼Œå½¢æˆæ¸©å®¤æ•ˆåº”ï¼Œå¯¼è‡´å¤§æ°”æ¸©åº¦å‡é«˜ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"é‚£æˆ‘ä»¬ä¸ªäººèƒ½åšäº›ä»€ä¹ˆæ¥å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾å‘¢ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ä¸ªäººå¯ä»¥ä»å¤šæ–¹é¢å‡æ’ï¼šå‡å°‘å¼€è½¦ã€é€‰æ‹©å…¬å…±äº¤é€šï¼›èŠ‚çº¦ç”¨ç”µï¼›å‡å°‘è‚‰ç±»æ¶ˆè´¹ï¼›æ”¯æŒå¯å†ç”Ÿèƒ½æºï¼›åƒåœ¾åˆ†ç±»å›æ”¶ï¼›è´­ä¹°ç¯ä¿äº§å“ç­‰ã€‚è™½ç„¶ä¸ªäººä½œç”¨æœ‰é™ï¼Œä½†é›†ä½“è¡ŒåŠ¨èƒ½äº§ç”Ÿå·¨å¤§å½±å“ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘å¬è¯´æ£®æ—ç ä¼ä¹Ÿä¸æ°”å€™å˜åŒ–æœ‰å…³ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"æ£®æ—æ˜¯é‡è¦çš„ç¢³æ±‡ï¼Œæ ‘æœ¨é€šè¿‡å…‰åˆä½œç”¨å¸æ”¶CO2å¹¶å‚¨å­˜ç¢³ã€‚ç ä¼æ£®æ—ä¸ä»…å‡å°‘äº†ç¢³å¸æ”¶èƒ½åŠ›ï¼Œç‡ƒçƒ§æˆ–è…çƒ‚çš„æœ¨æè¿˜ä¼šé‡Šæ”¾å‚¨å­˜çš„ç¢³ã€‚äºšé©¬é€Šé›¨æ—è¢«ç§°ä¸º'åœ°çƒä¹‹è‚º'å°±æ˜¯è¿™ä¸ªåŸå› ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"å¯å†ç”Ÿèƒ½æºæœ‰å“ªäº›ç±»å‹ï¼Ÿå®ƒä»¬å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ä¸»è¦ç±»å‹æœ‰å¤ªé˜³èƒ½ã€é£èƒ½ã€æ°´èƒ½ã€åœ°çƒ­èƒ½ã€ç”Ÿç‰©èƒ½ç­‰ã€‚å¤ªé˜³èƒ½å’Œé£èƒ½æŠ€æœ¯æˆç†Ÿã€æˆæœ¬ä¸‹é™å¿«ï¼Œä½†æœ‰é—´æ­‡æ€§é—®é¢˜ã€‚æ°´èƒ½ç¨³å®šå¯é ï¼Œä½†å—åœ°ç†æ¡ä»¶é™åˆ¶ã€‚åœ°çƒ­èƒ½ç¨³å®šä½†åˆ†å¸ƒä¸å‡ã€‚ç”Ÿç‰©èƒ½å¯å†ç”Ÿä½†å¯èƒ½ä¸ç²®é£Ÿç«äº‰åœŸåœ°ã€‚\"},\n",
        "            ],\n",
        "            \"test_query\": \"æ ¸èƒ½ä½œä¸ºæ¸…æ´èƒ½æºçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿå®ƒåœ¨åº”å¯¹æ°”å€™å˜åŒ–ä¸­çš„ä½œç”¨å¦‚ä½•ï¼Ÿ\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"å¥åº·å’¨è¯¢ - è¥å…»ä¸è¿åŠ¨\", \n",
        "            \"history\": [\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘æƒ³è¦å¼€å§‹å¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Œä½†ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ã€‚\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"å¾ˆå¥½çš„å†³å®šï¼å¥åº·ç”Ÿæ´»æ–¹å¼ä¸»è¦åŒ…æ‹¬å‡è¡¡é¥®é£Ÿã€è§„å¾‹è¿åŠ¨ã€å……è¶³ç¡çœ å’Œå‹åŠ›ç®¡ç†ã€‚å»ºè®®å…ˆä»å°çš„æ”¹å˜å¼€å§‹ï¼Œæ¯”å¦‚æ¯å¤©å¤šå–æ°´ã€å¢åŠ è”¬èœæ‘„å…¥ã€æ¯å¤©æ•£æ­¥30åˆ†é’Ÿã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"å…³äºé¥®é£Ÿï¼Œæˆ‘åº”è¯¥å¦‚ä½•å®‰æ’ä¸‰é¤ï¼Ÿæœ‰ä»€ä¹ˆè¥å…»æ­é…çš„åŸåˆ™å—ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ä¸‰é¤åº”è¯¥å‡è¡¡æ­é…ï¼šæ—©é¤è¦ä¸°å¯Œ(è›‹ç™½è´¨+ç¢³æ°´+ç»´ç”Ÿç´ )ï¼Œåˆé¤è¦å……è¶³ï¼Œæ™šé¤è¦æ¸…æ·¡ã€‚éµå¾ª'å½©è™¹é¥®é£Ÿ'åŸåˆ™ï¼Œå¤šåƒä¸åŒé¢œè‰²çš„è”¬æœã€‚æ§åˆ¶ç³–åˆ†å’ŒåŠ å·¥é£Ÿå“çš„æ‘„å…¥ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘ä¹‹å‰å¾ˆå°‘è¿åŠ¨ï¼Œåº”è¯¥é€‰æ‹©ä»€ä¹ˆç±»å‹çš„è¿åŠ¨æ¯”è¾ƒå¥½ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"åˆå­¦è€…å»ºè®®ä»ä½å¼ºåº¦æœ‰æ°§è¿åŠ¨å¼€å§‹ï¼Œå¦‚å¿«èµ°ã€æ¸¸æ³³ã€éª‘è½¦ã€‚æ¯å‘¨3-4æ¬¡ï¼Œæ¯æ¬¡30-45åˆ†é’Ÿã€‚å¯ä»¥é€æ¸åŠ å…¥åŠ›é‡è®­ç»ƒæ¥å¢å¼ºè‚Œè‚‰ã€‚æœ€é‡è¦çš„æ˜¯é€‰æ‹©è‡ªå·±å–œæ¬¢çš„è¿åŠ¨ï¼Œè¿™æ ·æ›´å®¹æ˜“åšæŒã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"ç¡çœ è´¨é‡å¯¹å¥åº·æœ‰å¤šé‡è¦ï¼Ÿæˆ‘ç»å¸¸å¤±çœ è¯¥æ€ä¹ˆåŠï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"ç¡çœ æå…¶é‡è¦ï¼å……è¶³ç¡çœ æœ‰åŠ©äºèº«ä½“ä¿®å¤ã€è®°å¿†æ•´åˆã€å…ç–«åŠ›ç»´æŒã€‚æˆäººéœ€è¦7-9å°æ—¶ç¡çœ ã€‚æ”¹å–„å¤±çœ å¯ä»¥ï¼šä¿æŒè§„å¾‹ä½œæ¯ã€ç¡å‰1å°æ—¶é¿å…å±å¹•ã€åˆ›é€ èˆ’é€‚ç¯å¢ƒã€é¿å…å’–å•¡å› ã€å°è¯•æ”¾æ¾æŠ€å·§å¦‚å†¥æƒ³ã€‚\"},\n",
        "                {\"role\": \"user\", \"content\": \"å‹åŠ›ç®¡ç†æ–¹é¢ï¼Œæœ‰ä»€ä¹ˆæœ‰æ•ˆçš„æ–¹æ³•å¯ä»¥æ¨èå—ï¼Ÿ\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"å‹åŠ›ç®¡ç†æ–¹æ³•å¾ˆå¤šï¼šæ·±å‘¼å¸ç»ƒä¹ ã€å†¥æƒ³ã€ç‘œä¼½ã€é€‚é‡è¿åŠ¨ã€ä¸æœ‹å‹èŠå¤©ã€åŸ¹å…»çˆ±å¥½ã€åˆç†å®‰æ’æ—¶é—´ã€å­¦ä¼šè¯´'ä¸'ã€‚å…³é”®æ˜¯æ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ–¹å¼ï¼Œå®šæœŸç»ƒä¹ ã€‚å¿…è¦æ—¶å¯ä»¥å¯»æ±‚ä¸“ä¸šå¿ƒç†å¸®åŠ©ã€‚\"},\n",
        "            ],\n",
        "            \"test_query\": \"æˆ‘æƒ³äº†è§£æ›´å¤šå…³äºè¥å…»è¡¥å……å‰‚çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ç»´ç”Ÿç´ å’Œè›‹ç™½ç²‰ï¼Œè¿™äº›çœŸçš„æœ‰å¿…è¦å—ï¼Ÿ\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    return dialogs\n",
        "\n",
        "# åŠ è½½æµ‹è¯•æ•°æ®\n",
        "test_dialogs = create_long_dialog_data()\n",
        "print(f\"ğŸ“‹ åˆ›å»ºäº† {len(test_dialogs)} ä¸ªæµ‹è¯•å¯¹è¯\")\n",
        "for i, dialog in enumerate(test_dialogs):\n",
        "    print(f\"  {i+1}. {dialog['title']} - {len(dialog['history'])} è½®å¯¹è¯\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. å®éªŒè®¾è®¡ä¸æ‰§è¡Œ\n",
        "\n",
        "å¯¹æ¯”ä½¿ç”¨embeddingå‹ç¼© vs åŸå§‹æ–‡æœ¬çš„æ¨¡å‹è¡¨ç°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"å®éªŒè¿è¡Œå™¨ï¼Œå¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ€§èƒ½\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_processor, model_manager):\n",
        "        self.embedding_processor = embedding_processor\n",
        "        self.model_manager = model_manager\n",
        "        self.results = []\n",
        "    \n",
        "    def run_baseline_experiment(self, dialog_history: List[Dict], query: str) -> Dict:\n",
        "        \"\"\"åŸºçº¿å®éªŒï¼šä½¿ç”¨åŸå§‹å®Œæ•´å†å²\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # æ„å»ºå®Œæ•´å†å²æç¤º\n",
        "        full_context = \"\"\n",
        "        for turn in dialog_history[-6:]:  # æœ€è¿‘6è½®å¯¹è¯\n",
        "            role = \"ç”¨æˆ·\" if turn['role'] == 'user' else \"åŠ©æ‰‹\"\n",
        "            full_context += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        full_prompt = f\"{full_context}ç”¨æˆ·: {query}\\nåŠ©æ‰‹:\"\n",
        "        \n",
        "        # ç”Ÿæˆå›å¤\n",
        "        response = self.model_manager.generate_text(\n",
        "            model=self.model_manager.dialog_model,\n",
        "            prompt=full_prompt,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': 'baseline_full_context',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(full_prompt),\n",
        "            'output_tokens': self.model_manager.count_tokens(response),\n",
        "            'context_length': len(full_context)\n",
        "        }\n",
        "    \n",
        "    def run_embedding_experiment(self, dialog_history: List[Dict], query: str, \n",
        "                               injection_method: str = \"prefix\") -> Dict:\n",
        "        \"\"\"embeddingå®éªŒï¼šä½¿ç”¨å‹ç¼©çš„embeddingè¡¨ç¤º\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # å‹ç¼©å†å²ä¸ºembedding\n",
        "        history_text = \"\"\n",
        "        for turn in dialog_history:\n",
        "            role = \"ç”¨æˆ·\" if turn['role'] == 'user' else \"åŠ©æ‰‹\"\n",
        "            history_text += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        # æå–å†å²embedding\n",
        "        history_embedding = self.embedding_processor.extract_text_embedding(history_text)\n",
        "        \n",
        "        # ä½¿ç”¨embeddingç”Ÿæˆå›å¤\n",
        "        response = self.embedding_processor.inject_embedding_into_model(\n",
        "            history_embedding, query, injection_method\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': f'embedding_{injection_method}',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(query),  # åªè®¡ç®—queryçš„token\n",
        "            'output_tokens': self.model_manager.count_tokens(response),\n",
        "            'embedding_dim': history_embedding.shape[0],\n",
        "            'compression_ratio': len(history_text) / history_embedding.shape[0]\n",
        "        }\n",
        "    \n",
        "    def run_no_context_experiment(self, query: str) -> Dict:\n",
        "        \"\"\"æ— ä¸Šä¸‹æ–‡å®éªŒï¼šåªç”¨å½“å‰æŸ¥è¯¢\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        prompt = f\"ç”¨æˆ·: {query}\\nåŠ©æ‰‹:\"\n",
        "        response = self.model_manager.generate_text(\n",
        "            model=self.model_manager.dialog_model,\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'method': 'no_context',\n",
        "            'response': response,\n",
        "            'latency': end_time - start_time,\n",
        "            'input_tokens': self.model_manager.count_tokens(prompt),\n",
        "            'output_tokens': self.model_manager.count_tokens(response)\n",
        "        }\n",
        "    \n",
        "    def evaluate_response_quality(self, response: str, query: str, context: str) -> Dict:\n",
        "        \"\"\"è¯„ä¼°å›å¤è´¨é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\"\"\"\n",
        "        # ç®€å•çš„è´¨é‡æŒ‡æ ‡\n",
        "        relevance_score = min(1.0, len(response) / 100)  # åŸºäºé•¿åº¦çš„ç›¸å…³æ€§\n",
        "        coherence_score = 1.0 - response.count(\"ã€‚ã€‚\") * 0.1  # å‡å°‘é‡å¤çš„åˆ†æ•°\n",
        "        informativeness = min(1.0, len(set(response.split())) / 50)  # è¯æ±‡å¤šæ ·æ€§\n",
        "        \n",
        "        return {\n",
        "            'relevance': relevance_score,\n",
        "            'coherence': max(0, coherence_score),\n",
        "            'informativeness': informativeness,\n",
        "            'overall_score': (relevance_score + coherence_score + informativeness) / 3\n",
        "        }\n",
        "    \n",
        "    def run_comparative_experiment(self, test_dialog: Dict) -> Dict:\n",
        "        \"\"\"è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ\"\"\"\n",
        "        history = test_dialog['history']\n",
        "        query = test_dialog['test_query']\n",
        "        title = test_dialog['title']\n",
        "        \n",
        "        print(f\"\\nğŸ§ª è¿è¡Œå®éªŒ: {title}\")\n",
        "        print(f\"   å†å²è½®æ•°: {len(history)}\")\n",
        "        print(f\"   æµ‹è¯•æŸ¥è¯¢: {query[:50]}...\")\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # 1. åŸºçº¿å®éªŒ\n",
        "        print(\"   ğŸ”µ è¿è¡ŒåŸºçº¿å®éªŒ...\")\n",
        "        baseline_result = self.run_baseline_experiment(history, query)\n",
        "        baseline_quality = self.evaluate_response_quality(\n",
        "            baseline_result['response'], query, str(history)\n",
        "        )\n",
        "        baseline_result.update(baseline_quality)\n",
        "        results['baseline'] = baseline_result\n",
        "        \n",
        "        # 2. æ— ä¸Šä¸‹æ–‡å®éªŒ\n",
        "        print(\"   âšª è¿è¡Œæ— ä¸Šä¸‹æ–‡å®éªŒ...\")\n",
        "        no_context_result = self.run_no_context_experiment(query)\n",
        "        no_context_quality = self.evaluate_response_quality(\n",
        "            no_context_result['response'], query, \"\"\n",
        "        )\n",
        "        no_context_result.update(no_context_quality)\n",
        "        results['no_context'] = no_context_result\n",
        "        \n",
        "        # 3. Embeddingå®éªŒ (å¤šç§æ³¨å…¥æ–¹æ³•)\n",
        "        for method in ['prefix', 'interpolation', 'attention_fusion']:\n",
        "            print(f\"   ğŸŸ¡ è¿è¡Œembeddingå®éªŒ ({method})...\")\n",
        "            embedding_result = self.run_embedding_experiment(history, query, method)\n",
        "            embedding_quality = self.evaluate_response_quality(\n",
        "                embedding_result['response'], query, \"embedding_context\"\n",
        "            )\n",
        "            embedding_result.update(embedding_quality)\n",
        "            results[f'embedding_{method}'] = embedding_result\n",
        "        \n",
        "        return {\n",
        "            'dialog_title': title,\n",
        "            'test_query': query,\n",
        "            'results': results\n",
        "        }\n",
        "\n",
        "# åˆ›å»ºå®éªŒè¿è¡Œå™¨\n",
        "experiment_runner = ExperimentRunner(embedding_processor, model_manager)\n",
        "print(\"ğŸ”¬ å®éªŒè¿è¡Œå™¨åˆ›å»ºå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¿è¡Œæ‰€æœ‰å®éªŒ\n",
        "print(\"ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰å®éªŒ...\")\n",
        "\n",
        "all_experiment_results = []\n",
        "\n",
        "for i, dialog in enumerate(test_dialogs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"å®éªŒ {i+1}/{len(test_dialogs)}: {dialog['title']}\")\n",
        "    \n",
        "    try:\n",
        "        result = experiment_runner.run_comparative_experiment(dialog)\n",
        "        all_experiment_results.append(result)\n",
        "        print(\"âœ… å®éªŒå®Œæˆ\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å®éªŒå¤±è´¥: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼å…±å®Œæˆ {len(all_experiment_results)} ä¸ªå®éªŒ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ç»“æœåˆ†æä¸å¯è§†åŒ–\n",
        "\n",
        "åˆ†æå®éªŒç»“æœï¼Œå¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ€§èƒ½è¡¨ç°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†æå®éªŒç»“æœ\n",
        "def analyze_experiment_results(results):\n",
        "    \"\"\"åˆ†æå®éªŒç»“æœå¹¶ç”Ÿæˆç»Ÿè®¡æ•°æ®\"\"\"\n",
        "    \n",
        "    # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„æ•°æ®\n",
        "    methods = ['baseline', 'no_context', 'embedding_prefix', 'embedding_interpolation', 'embedding_attention_fusion']\n",
        "    metrics_data = {method: [] for method in methods}\n",
        "    \n",
        "    for experiment in results:\n",
        "        for method in methods:\n",
        "            if method in experiment['results']:\n",
        "                metrics_data[method].append(experiment['results'][method])\n",
        "    \n",
        "    # è®¡ç®—å¹³å‡æŒ‡æ ‡\n",
        "    summary_stats = {}\n",
        "    for method, data in metrics_data.items():\n",
        "        if data:\n",
        "            summary_stats[method] = {\n",
        "                'avg_latency': np.mean([d['latency'] for d in data]),\n",
        "                'avg_input_tokens': np.mean([d['input_tokens'] for d in data]),\n",
        "                'avg_output_tokens': np.mean([d['output_tokens'] for d in data]),\n",
        "                'avg_relevance': np.mean([d['relevance'] for d in data]),\n",
        "                'avg_coherence': np.mean([d['coherence'] for d in data]),\n",
        "                'avg_informativeness': np.mean([d['informativeness'] for d in data]),\n",
        "                'avg_overall_score': np.mean([d['overall_score'] for d in data]),\n",
        "                'sample_size': len(data)\n",
        "            }\n",
        "    \n",
        "    return summary_stats\n",
        "\n",
        "# åˆ†æç»“æœ\n",
        "if all_experiment_results:\n",
        "    summary_stats = analyze_experiment_results(all_experiment_results)\n",
        "    \n",
        "    print(\"ğŸ“Š å®éªŒç»“æœç»Ÿè®¡æ‘˜è¦:\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for method, stats in summary_stats.items():\n",
        "        print(f\"\\nğŸ”¹ {method.upper()}:\")\n",
        "        print(f\"   å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.3f}s\")\n",
        "        print(f\"   å¹³å‡è¾“å…¥tokens: {stats['avg_input_tokens']:.1f}\")\n",
        "        print(f\"   å¹³å‡è¾“å‡ºtokens: {stats['avg_output_tokens']:.1f}\")\n",
        "        print(f\"   ç›¸å…³æ€§å¾—åˆ†: {stats['avg_relevance']:.3f}\")\n",
        "        print(f\"   è¿è´¯æ€§å¾—åˆ†: {stats['avg_coherence']:.3f}\")\n",
        "        print(f\"   ä¿¡æ¯é‡å¾—åˆ†: {stats['avg_informativeness']:.3f}\")\n",
        "        print(f\"   ç»¼åˆå¾—åˆ†: {stats['avg_overall_score']:.3f}\")\n",
        "        print(f\"   æ ·æœ¬æ•°é‡: {stats['sample_size']}\")\n",
        "else:\n",
        "    print(\"âš ï¸ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–å®éªŒç»“æœ\n",
        "def create_comparison_plots(summary_stats):\n",
        "    \"\"\"åˆ›å»ºå¯¹æ¯”å›¾è¡¨\"\"\"\n",
        "    \n",
        "    if not summary_stats:\n",
        "        print(\"âš ï¸ æ²¡æœ‰æ•°æ®å¯ä»¥å¯è§†åŒ–\")\n",
        "        return\n",
        "    \n",
        "    methods = list(summary_stats.keys())\n",
        "    \n",
        "    # åˆ›å»ºå­å›¾\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Embeddingå‹ç¼©æŠ€æœ¯å®éªŒç»“æœå¯¹æ¯”', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. å»¶è¿Ÿå¯¹æ¯”\n",
        "    latencies = [summary_stats[method]['avg_latency'] for method in methods]\n",
        "    axes[0, 0].bar(methods, latencies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    axes[0, 0].set_title('å¹³å‡å»¶è¿Ÿå¯¹æ¯”')\n",
        "    axes[0, 0].set_ylabel('å»¶è¿Ÿ (ç§’)')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Tokenä½¿ç”¨é‡å¯¹æ¯”\n",
        "    input_tokens = [summary_stats[method]['avg_input_tokens'] for method in methods]\n",
        "    axes[0, 1].bar(methods, input_tokens, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    axes[0, 1].set_title('å¹³å‡è¾“å…¥Tokenæ•°é‡')\n",
        "    axes[0, 1].set_ylabel('Tokenæ•°é‡')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. è´¨é‡å¾—åˆ†å¯¹æ¯”\n",
        "    quality_metrics = ['avg_relevance', 'avg_coherence', 'avg_informativeness']\n",
        "    quality_labels = ['ç›¸å…³æ€§', 'è¿è´¯æ€§', 'ä¿¡æ¯é‡']\n",
        "    \n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, (metric, label) in enumerate(zip(quality_metrics, quality_labels)):\n",
        "        scores = [summary_stats[method][metric] for method in methods]\n",
        "        axes[1, 0].bar(x + i*width, scores, width, label=label)\n",
        "    \n",
        "    axes[1, 0].set_title('è´¨é‡æŒ‡æ ‡å¯¹æ¯”')\n",
        "    axes[1, 0].set_ylabel('å¾—åˆ†')\n",
        "    axes[1, 0].set_xticks(x + width)\n",
        "    axes[1, 0].set_xticklabels(methods, rotation=45)\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # 4. ç»¼åˆå¾—åˆ†å¯¹æ¯”\n",
        "    overall_scores = [summary_stats[method]['avg_overall_score'] for method in methods]\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "    axes[1, 1].pie(overall_scores, labels=methods, colors=colors, autopct='%1.3f')\n",
        "    axes[1, 1].set_title('ç»¼åˆå¾—åˆ†åˆ†å¸ƒ')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# åˆ›å»ºå¯¹æ¯”å›¾è¡¨\n",
        "if all_experiment_results and summary_stats:\n",
        "    print(\"ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...\")\n",
        "    create_comparison_plots(summary_stats)\n",
        "else:\n",
        "    print(\"âš ï¸ æ— æ³•ç”Ÿæˆå›¾è¡¨ï¼Œç¼ºå°‘å®éªŒæ•°æ®\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. å®éªŒç»“è®ºä¸æ€»ç»“\n",
        "\n",
        "### å…³é”®å‘ç°\n",
        "\n",
        "é€šè¿‡å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n",
        "\n",
        "1. **è®¡ç®—æ•ˆç‡**ï¼šembeddingå‹ç¼©æ–¹æ³•åœ¨è¾“å…¥tokenæ•°é‡ä¸Šçš„ä¼˜åŠ¿\n",
        "2. **å“åº”è´¨é‡**ï¼šä¸åŒæ–¹æ³•åœ¨å¯¹è¯è´¨é‡ä¸Šçš„è¡¨ç°å·®å¼‚  \n",
        "3. **å“åº”å»¶è¿Ÿ**ï¼šå„æ–¹æ³•çš„å¤„ç†é€Ÿåº¦å¯¹æ¯”\n",
        "4. **ä¿¡æ¯ä¿ç•™**ï¼šembeddingèƒ½å¦æœ‰æ•ˆä¿ç•™å†å²ä¿¡æ¯\n",
        "\n",
        "### æŠ€æœ¯è¦ç‚¹\n",
        "\n",
        "1. **Embeddingæå–**ï¼šä½¿ç”¨Qwenæ¨¡å‹çš„hidden statesä½œä¸ºå†å²ä¿¡æ¯çš„å‹ç¼©è¡¨ç¤º\n",
        "2. **æ³¨å…¥ç­–ç•¥**ï¼šå®ç°äº†å¤šç§å°†embeddingèå…¥æ¨¡å‹çš„æ–¹æ³•\n",
        "3. **è¯„ä¼°ä½“ç³»**ï¼šå»ºç«‹äº†å¤šç»´åº¦çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡\n",
        "\n",
        "### ä¸‹ä¸€æ­¥æ”¹è¿›æ–¹å‘\n",
        "\n",
        "1. **æ›´å…ˆè¿›çš„embeddingèåˆ**ï¼šç ”ç©¶æ›´æœ‰æ•ˆçš„æ³¨å…¥æ–¹å¼\n",
        "2. **åŠ¨æ€å‹ç¼©ç­–ç•¥**ï¼šæ ¹æ®å¯¹è¯å†…å®¹è‡ªé€‚åº”è°ƒæ•´å‹ç¼©æ¯”ä¾‹\n",
        "3. **æ¨¡å‹å¾®è°ƒ**ï¼šè®­ç»ƒä¸“é—¨çš„embeddingå¤„ç†æ¨¡å—\n",
        "4. **æ›´å¤æ‚çš„è¯„ä¼°**ï¼šå¼•å…¥äººå·¥è¯„ä¼°å’Œæ›´å¤šè‡ªåŠ¨åŒ–æŒ‡æ ‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜å®éªŒç»“æœå’Œå±•ç¤ºæ ·ä¾‹å›å¤\n",
        "def save_and_display_results(results, filename=\"embedding_experiment_results.json\"):\n",
        "    \"\"\"ä¿å­˜å®éªŒç»“æœå¹¶å±•ç¤ºæ ·ä¾‹å›å¤\"\"\"\n",
        "    \n",
        "    if not results:\n",
        "        print(\"âš ï¸ æ²¡æœ‰ç»“æœå¯ä¿å­˜\")\n",
        "        return\n",
        "    \n",
        "    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {filename}\")\n",
        "    \n",
        "    # å±•ç¤ºç¬¬ä¸€ä¸ªå®éªŒçš„è¯¦ç»†å›å¤\n",
        "    if results:\n",
        "        first_experiment = results[0]\n",
        "        print(f\"\\nğŸ“ å±•ç¤ºæ ·ä¾‹å›å¤ - {first_experiment['dialog_title']}\")\n",
        "        print(f\"ğŸ” æµ‹è¯•æŸ¥è¯¢: {first_experiment['test_query']}\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        \n",
        "        for method, result in first_experiment['results'].items():\n",
        "            print(f\"\\nğŸ”¹ {method.upper()}:\")\n",
        "            print(f\"å»¶è¿Ÿ: {result['latency']:.3f}s | è¾“å…¥tokens: {result['input_tokens']} | ç»¼åˆå¾—åˆ†: {result['overall_score']:.3f}\")\n",
        "            print(f\"å›å¤: {result['response'][:200]}...\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "# ä¿å­˜å¹¶å±•ç¤ºç»“æœ\n",
        "if all_experiment_results:\n",
        "    save_and_display_results(all_experiment_results)\n",
        "    \n",
        "    # åˆ›å»ºç»“æœDataFrameç”¨äºæ›´å¥½çš„åˆ†æ\n",
        "    df_results = []\n",
        "    for exp in all_experiment_results:\n",
        "        for method, result in exp['results'].items():\n",
        "            row = {\n",
        "                'dialog': exp['dialog_title'],\n",
        "                'method': method,\n",
        "                'latency': result['latency'],\n",
        "                'input_tokens': result['input_tokens'],\n",
        "                'output_tokens': result['output_tokens'],\n",
        "                'relevance': result['relevance'],\n",
        "                'coherence': result['coherence'],\n",
        "                'informativeness': result['informativeness'],\n",
        "                'overall_score': result['overall_score']\n",
        "            }\n",
        "            df_results.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(df_results)\n",
        "    print(\"\\nğŸ“Š ç»“æœDataFrameæ¦‚è§ˆ:\")\n",
        "    print(df.groupby('method')[['latency', 'input_tokens', 'overall_score']].mean())\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ æ²¡æœ‰å®éªŒç»“æœå¯ä¿å­˜\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. é«˜çº§Embeddingæ³¨å…¥æŠ€æœ¯æµ‹è¯•\n",
        "\n",
        "æµ‹è¯•å¢å¼ºç‰ˆçš„embeddingæ³¨å…¥å™¨ï¼Œå¯¹æ¯”å¤šç§æ³¨å…¥ç­–ç•¥çš„æ•ˆæœ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¹¶æµ‹è¯•å¢å¼ºç‰ˆembeddingæ³¨å…¥å™¨\n",
        "from enhanced_embedding_injector import AdvancedEmbeddingInjector\n",
        "\n",
        "# åˆ›å»ºå¢å¼ºç‰ˆæ³¨å…¥å™¨\n",
        "if model_manager.dialog_model and model_manager.tokenizer:\n",
        "    advanced_injector = AdvancedEmbeddingInjector(model_manager)\n",
        "    print(\"ğŸš€ å¢å¼ºç‰ˆEmbeddingæ³¨å…¥å™¨åˆ›å»ºå®Œæˆ\")\n",
        "    \n",
        "    # æµ‹è¯•å•ä¸ªå¯¹è¯çš„å¤šç§æ³¨å…¥ç­–ç•¥\n",
        "    if test_dialogs:\n",
        "        test_dialog = test_dialogs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæµ‹è¯•å¯¹è¯\n",
        "        \n",
        "        # æ„å»ºå†å²æ–‡æœ¬\n",
        "        history_text = \"\"\n",
        "        for turn in test_dialog['history']:\n",
        "            role = \"ç”¨æˆ·\" if turn['role'] == 'user' else \"åŠ©æ‰‹\"\n",
        "            history_text += f\"{role}: {turn['content']}\\n\"\n",
        "        \n",
        "        query_text = test_dialog['test_query']\n",
        "        \n",
        "        print(f\"\\nğŸ§ª æµ‹è¯•é«˜çº§æ³¨å…¥ç­–ç•¥:\")\n",
        "        print(f\"   å¯¹è¯ä¸»é¢˜: {test_dialog['title']}\")\n",
        "        print(f\"   å†å²é•¿åº¦: {len(history_text)} å­—ç¬¦\")\n",
        "        print(f\"   æµ‹è¯•æŸ¥è¯¢: {query_text[:80]}...\")\n",
        "        \n",
        "        # å¯¹æ¯”æ‰€æœ‰æ³¨å…¥ç­–ç•¥\n",
        "        print(\"\\nğŸ”„ è¿è¡Œå¤šç­–ç•¥å¯¹æ¯”...\")\n",
        "        advanced_results = advanced_injector.compare_injection_strategies(\n",
        "            history_text, query_text\n",
        "        )\n",
        "        \n",
        "        # å±•ç¤ºç»“æœ\n",
        "        print(\"\\nğŸ“Š é«˜çº§æ³¨å…¥ç­–ç•¥ç»“æœ:\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        for strategy, result in advanced_results.items():\n",
        "            if 'error' not in result:\n",
        "                print(f\"\\nğŸ”¹ {strategy.upper()}:\")\n",
        "                print(f\"   ç”Ÿæˆæ—¶é—´: {result['generation_time']:.3f}s\")\n",
        "                print(f\"   è¾“å…¥tokens: {result['input_tokens']}\")\n",
        "                print(f\"   è¾“å‡ºtokens: {result['output_tokens']}\")\n",
        "                \n",
        "                if 'metadata' in result:\n",
        "                    metadata = result['metadata']\n",
        "                    print(f\"   ç›¸ä¼¼åº¦: {metadata['embedding_similarity']:.3f}\")\n",
        "                    print(f\"   èåˆç­–ç•¥: {metadata['injection_strategy']}\")\n",
        "                \n",
        "                print(f\"   å›å¤: {result['response'][:150]}...\")\n",
        "                print(\"-\" * 60)\n",
        "            else:\n",
        "                print(f\"\\nâŒ {strategy}: {result['error']}\")\n",
        "        \n",
        "        # ä¿å­˜é«˜çº§å®éªŒç»“æœ\n",
        "        with open(\"advanced_injection_results.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(advanced_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"\\nğŸ’¾ é«˜çº§æ³¨å…¥å®éªŒç»“æœå·²ä¿å­˜\")\n",
        "        \n",
        "else:\n",
        "    print(\"âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æµ‹è¯•å¢å¼ºç‰ˆæ³¨å…¥å™¨\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ¯ å®éªŒæ€»ç»“\n",
        "\n",
        "æœ¬notebookæˆåŠŸå®ç°äº†å†å²è®°å½•embeddingå‹ç¼©æŠ€æœ¯çš„å®Œæ•´ç ”ç©¶æ–¹æ¡ˆï¼š\n",
        "\n",
        "### âœ… å·²å®Œæˆçš„åŠŸèƒ½\n",
        "\n",
        "1. **æ ¸å¿ƒæŠ€æœ¯å®ç°**\n",
        "   - å†å²å¯¹è¯è®°å½•çš„embeddingæå–\n",
        "   - å¤šç§embeddingä¸queryçš„èåˆç­–ç•¥\n",
        "   - å°†èåˆåçš„embeddingæ³¨å…¥Qwenæ¨¡å‹\n",
        "\n",
        "2. **å®éªŒè®¾è®¡**\n",
        "   - è¶…é•¿å¤šè½®å¯¹è¯æµ‹è¯•æ•°æ®\n",
        "   - åŸºçº¿å¯¹æ¯” vs embeddingå‹ç¼©å¯¹æ¯”\n",
        "   - å¤šç»´åº¦æ€§èƒ½è¯„ä¼°\n",
        "\n",
        "3. **é«˜çº§æ³¨å…¥ç­–ç•¥**\n",
        "   - ç›´æ¥æ‹¼æ¥èåˆ\n",
        "   - åŠ æƒèåˆ\n",
        "   - æ³¨æ„åŠ›æœºåˆ¶èåˆ\n",
        "   - åˆ†å±‚æ³¨å…¥\n",
        "   - è‡ªé€‚åº”è·¯ç”±\n",
        "\n",
        "### ğŸ“ˆ å…³é”®ä¼˜åŠ¿\n",
        "\n",
        "- **æ•ˆç‡æå‡**: å‡å°‘è¾“å…¥tokenæ•°é‡ï¼Œæé«˜å¤„ç†é€Ÿåº¦\n",
        "- **ä¿¡æ¯ä¿ç•™**: æœ‰æ•ˆå‹ç¼©å†å²ä¿¡æ¯å¹¶ä¿æŒè¯­ä¹‰\n",
        "- **çµæ´»æ€§**: å¤šç§æ³¨å…¥ç­–ç•¥é€‚åº”ä¸åŒåœºæ™¯\n",
        "- **å¯æ‰©å±•**: æ”¯æŒé•¿å¯¹è¯å†å²çš„é«˜æ•ˆå¤„ç†\n",
        "\n",
        "### ğŸ”¬ å®éªŒä»·å€¼\n",
        "\n",
        "è¿™ä¸ªç ”ç©¶ä¸ºé•¿å¯¹è¯åœºæ™¯ä¸‹çš„ä¸Šä¸‹æ–‡ç®¡ç†æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºï¼š\n",
        "- å®¢æœç³»ç»Ÿçš„é•¿æœŸå¯¹è¯ç®¡ç†\n",
        "- æ•™è‚²åŠ©æ‰‹çš„å­¦ä¹ å†å²è·Ÿè¸ª  \n",
        "- ä¸ªäººåŠ©æ‰‹çš„é•¿æœŸè®°å¿†ç»´æŠ¤\n",
        "\n",
        "**è¿è¡Œå®Œæ•´å®éªŒåï¼Œå¯ä»¥åˆ†æå„ç§æ–¹æ³•çš„æ•ˆæœå·®å¼‚ï¼Œä¸ºå®é™…åº”ç”¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
