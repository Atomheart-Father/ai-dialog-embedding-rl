#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ å¯¹è¯ç³»ç»Ÿå®éªŒè¿è¡Œå™¨
å¿«é€Ÿæ‰§è¡Œå››ä¸ªå¯¹ç…§ç»„å®éªŒå¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import json
import os
import time
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import matplotlib.pyplot as plt
    import pandas as pd
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("Warning: matplotlib/pandas not available. Some plotting features will be disabled.")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from dialog_manager import DialogManager
    from rl_trainer import RLTrainer
    from reward_calculator import RewardCalculator
    from models import model_manager
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–æ¨¡å—éƒ½å·²æ­£ç¡®å®‰è£…")
    exit(1)

class QuickExperiment:
    """å¿«é€Ÿå®éªŒè¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_datasets = self._create_test_datasets()
        self.results = {}
        
    def _create_test_datasets(self) -> Dict[str, List[str]]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        return {
            'technical_discussion': [
                "ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ",
                "Q-learningå’ŒActor-Criticæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "ç»éªŒå›æ”¾æœºåˆ¶æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
                "Îµ-è´ªå¿ƒç­–ç•¥çš„æ¢ç´¢å’Œåˆ©ç”¨å¹³è¡¡æ€ä¹ˆå®ç°ï¼Ÿ",
                "DDPGç®—æ³•å¯¹è¿ç»­åŠ¨ä½œç©ºé—´æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "ä½ åˆšæ‰æåˆ°çš„Q-learningèƒ½å¤„ç†è¿ç»­çŠ¶æ€ç©ºé—´å—ï¼Ÿ",
                "ä»·å€¼å‡½æ•°è¿‘ä¼¼æœ‰ä»€ä¹ˆæŒ‘æˆ˜ï¼Ÿ",
                "å›åˆ°ç»éªŒå›æ”¾ï¼Œä¼˜å…ˆç»éªŒå›æ”¾æ˜¯å¦‚ä½•æ”¹è¿›çš„ï¼Ÿ",
                "Double DQNè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "å¼ºåŒ–å­¦ä¹ åœ¨NLPä¸­çš„åº”ç”¨å‰æ™¯å¦‚ä½•ï¼Ÿ"
            ],
            'problem_solving': [
                "æˆ‘çš„Pythonç¨‹åºå¾ˆæ…¢ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ",
                "æˆ‘åœ¨å¤„ç†100ä¸‡è¡ŒCSVæ–‡ä»¶",
                "æ•°æ®è¯»å–å°±å¾ˆæ…¢ï¼Œä½ è§‰å¾—æœ€å¯èƒ½çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¯•äº†chunkingè¯»å–è¿˜æ˜¯æ…¢ï¼Œè¿˜æœ‰ä»€ä¹ˆä¼˜åŒ–æ–¹æ³•ï¼Ÿ",
                "æ•°æ®ç±»å‹ä¼˜åŒ–å…·ä½“æ€ä¹ˆåšï¼Ÿ",
                "æŒ‰ä½ è¯´çš„ä¼˜åŒ–äº†æ•°æ®ç±»å‹ï¼Œè¿˜æƒ³æ›´å¿«",
                "å›åˆ°chunkingï¼Œchunk sizeæ€ä¹ˆé€‰æ‹©æœ€ä¼˜ï¼Ÿ",
                "è€ƒè™‘æ¢ç”¨Polarsåº“ï¼Œä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ",
                "èƒ½ç»™æˆ‘ä¸€ä¸ªä¼˜åŒ–çš„ä¼˜å…ˆçº§æ’åºå—ï¼Ÿ",
                "èƒ½æ€»ç»“ä¸€ä¸‹å®Œæ•´çš„è§£å†³æ–¹æ¡ˆå—ï¼Ÿ"
            ],
            'casual_conversation': [
                "ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œä½ å–œæ¬¢æ™´å¤©å—ï¼Ÿ",
                "æˆ‘æœ€è¿‘åœ¨å­¦æ‘„å½±ï¼Œä»€ä¹ˆå¤©æ°”æœ€é€‚åˆæ‹ç…§ï¼Ÿ",
                "åˆšæ‰ä½ æåˆ°æ™´å¤©ï¼Œä½ æ‹è¿‡æ—¥å‡ºæˆ–æ—¥è½å—ï¼Ÿ",
                "æˆ‘æƒ³å»æµ·è¾¹æ‹æ—¥å‡ºï¼Œéœ€è¦å‡†å¤‡ä»€ä¹ˆå™¨æï¼Ÿ",
                "é™¤äº†æ‘„å½±æˆ‘è¿˜å–œæ¬¢æ—…è¡Œï¼Œä½ æœ‰æ¨èå—ï¼Ÿ",
                "è¯´åˆ°æ—…è¡Œï¼Œä½ ä¹‹å‰æåˆ°çš„æ‹ç…§æŠ€å·§èƒ½è¯¦ç»†è¯´è¯´å—ï¼Ÿ",
                "æˆ‘è®¡åˆ’å»è¥¿è—ï¼Œé‚£é‡Œçš„å…‰çº¿æ¡ä»¶ä½ äº†è§£å—ï¼Ÿ",
                "æˆ‘ä»¬åˆšå¼€å§‹èŠå¤©æ°”ï¼Œé˜´å¤©é€‚åˆæ‹ä»€ä¹ˆï¼Ÿ",
                "é™¤äº†è¥¿è—ï¼Œè¿˜æœ‰å“ªäº›åœ°æ–¹é€‚åˆæ‘„å½±ï¼Ÿ",
                "ç»¼åˆå¤©æ°”ã€æ—…è¡Œå’Œæ‘„å½±ï¼Œç»™æˆ‘ä¸ªå®Œæ•´å»ºè®®ï¼Ÿ"
            ]
        }
    
    def run_mock_experiment(self, dataset_name: str) -> Dict:
        """è¿è¡Œæ¨¡æ‹Ÿå®éªŒï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        print(f"ğŸ§ª è¿è¡Œæ•°æ®é›†: {dataset_name}")
        
        # æ¨¡æ‹Ÿå››ä¸ªå¯¹ç…§ç»„çš„ç»“æœ
        groups = {
            'baseline': {'compression': False, 'rl_trained': False},
            'compression_only': {'compression': True, 'rl_trained': False},
            'rl_only': {'compression': False, 'rl_trained': True},
            'full_system': {'compression': True, 'rl_trained': True}
        }
        
        results = {}
        
        for group_name, config in groups.items():
            print(f"  ğŸ“Š æµ‹è¯• {group_name}...")
            
            # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
            base_scores = {
                'baseline': [0.6, 0.65, 0.6, 0.7],
                'compression_only': [0.7, 0.75, 0.65, 0.85],
                'rl_only': [0.75, 0.8, 0.78, 0.75],
                'full_system': [0.85, 0.9, 0.88, 0.92]
            }
            
            scores = base_scores[group_name]
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            metrics = []
            timing = []
            responses = []
            
            for i, user_input in enumerate(self.test_datasets[dataset_name]):
                # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
                noise = np.random.normal(0, 0.05, 4)
                turn_scores = [max(0, min(1, s + noise[j])) for j, s in enumerate(scores)]
                
                metrics.append({
                    'relevance_score': turn_scores[0],
                    'coherence_score': turn_scores[1],
                    'fluency_score': turn_scores[2],
                    'context_preservation': turn_scores[3],
                    'response_length': np.random.randint(40, 80)
                })
                
                # æ¨¡æ‹Ÿå“åº”æ—¶é—´
                base_time = 0.5
                if config['compression']:
                    base_time -= 0.1  # å‹ç¼©å‡å°‘æ—¶é—´
                if config['rl_trained']:
                    base_time += 0.05  # RLè®­ç»ƒå¢åŠ å°‘é‡æ—¶é—´
                
                timing.append(max(0.1, base_time + np.random.normal(0, 0.05)))
                
                responses.append({
                    'turn': i + 1,
                    'user_input': user_input,
                    'response': f"è¿™æ˜¯{group_name}ç»„å¯¹é—®é¢˜{i+1}çš„å›ç­”",
                    'response_time': timing[-1]
                })
            
            results[group_name] = {
                'responses': responses,
                'metrics': metrics,
                'timing': timing,
                'compression_stats': [] if not config['compression'] else [{'ratio': 0.3, 'tokens_saved': 500}] * len(metrics)
            }
        
        return results
    
    def analyze_results(self, results: Dict) -> Dict:
        """åˆ†æå®éªŒç»“æœ"""
        analysis = {}
        
        for group_name, group_data in results.items():
            metrics = group_data['metrics']
            timing = group_data['timing']
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„ç»Ÿè®¡å€¼
            relevance_scores = [m['relevance_score'] for m in metrics]
            coherence_scores = [m['coherence_score'] for m in metrics]
            fluency_scores = [m['fluency_score'] for m in metrics]
            context_scores = [m['context_preservation'] for m in metrics]
            
            analysis[group_name] = {
                'relevance': {'mean': np.mean(relevance_scores), 'std': np.std(relevance_scores)},
                'coherence': {'mean': np.mean(coherence_scores), 'std': np.std(coherence_scores)},
                'fluency': {'mean': np.mean(fluency_scores), 'std': np.std(fluency_scores)},
                'context_preservation': {'mean': np.mean(context_scores), 'std': np.std(context_scores)},
                'response_time': {'mean': np.mean(timing), 'std': np.std(timing)},
                'overall_score': np.mean([
                    np.mean(relevance_scores),
                    np.mean(coherence_scores),
                    np.mean(fluency_scores),
                    np.mean(context_scores)
                ])
            }
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        baseline_score = analysis.get('baseline', {}).get('overall_score', 0)
        improvements = {}
        for group_name, group_analysis in analysis.items():
            if group_name != 'baseline' and baseline_score > 0:
                improvement = ((group_analysis['overall_score'] - baseline_score) / baseline_score * 100)
                improvements[group_name] = improvement
        
        analysis['improvements'] = improvements
        
        return analysis
    
    def plot_results(self, analysis: Dict, dataset_name: str, save_path: Optional[str] = None):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        if not PLOTTING_AVAILABLE:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            return
            
        groups = [name for name in analysis.keys() if name != 'improvements']
        metrics = ['relevance', 'coherence', 'fluency', 'context_preservation']
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{dataset_name} æ•°æ®é›†æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        axes = [ax1, ax2, ax3, ax4]
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']
        
        for i, metric in enumerate(metrics):
            ax = axes[i]
            
            # å‡†å¤‡æ•°æ®
            means = [analysis[group][metric]['mean'] for group in groups]
            stds = [analysis[group][metric]['std'] for group in groups]
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars = ax.bar(groups, means, yerr=stds, capsize=5, color=colors, alpha=0.7)
            
            # ç¾åŒ–å›¾è¡¨
            ax.set_title(f'{metric.replace("_", " ").title()}')
            ax.set_ylabel('å¾—åˆ†')
            ax.set_ylim(0, 1)
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, mean in zip(bars, means):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{mean:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def print_summary(self, analysis: Dict, dataset_name: str):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print(f"\nğŸ“Š {dataset_name} æ•°æ®é›†å®éªŒç»“æœæ‘˜è¦")
        print("=" * 60)
        
        # å„ç»„æ€§èƒ½å¯¹æ¯”
        groups = [name for name in analysis.keys() if name != 'improvements']
        for group in groups:
            group_data = analysis[group]
            print(f"\nğŸ”¬ {group.upper()}:")
            print(f"  æ•´ä½“å¾—åˆ†: {group_data['overall_score']:.3f}")
            print(f"  ç›¸å…³æ€§: {group_data['relevance']['mean']:.3f} Â± {group_data['relevance']['std']:.3f}")
            print(f"  è¿è´¯æ€§: {group_data['coherence']['mean']:.3f} Â± {group_data['coherence']['std']:.3f}")
            print(f"  æµç•…æ€§: {group_data['fluency']['mean']:.3f} Â± {group_data['fluency']['std']:.3f}")
            print(f"  ä¸Šä¸‹æ–‡ä¿æŒ: {group_data['context_preservation']['mean']:.3f} Â± {group_data['context_preservation']['std']:.3f}")
            print(f"  å“åº”æ—¶é—´: {group_data['response_time']['mean']:.3f} Â± {group_data['response_time']['std']:.3f} ç§’")
        
        # æ”¹è¿›å¹…åº¦
        if 'improvements' in analysis:
            print(f"\nğŸ“ˆ ç›¸å¯¹åŸºçº¿ç»„çš„æ”¹è¿›:")
            for group, improvement in analysis['improvements'].items():
                print(f"  {group.upper()}: {improvement:+.1f}%")
        
        # æ’å
        sorted_groups = sorted(groups, key=lambda x: analysis[x]['overall_score'], reverse=True)
        print(f"\nğŸ† æ€§èƒ½æ’å:")
        for i, group in enumerate(sorted_groups, 1):
            score = analysis[group]['overall_score']
            print(f"  ç¬¬{i}å: {group.upper()} ({score:.3f})")
    
    def run_full_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸš€ å¼€å§‹å¼ºåŒ–å­¦ä¹ å¯¹è¯ç³»ç»Ÿå®éªŒ")
        print("=" * 50)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = f"experiment_results_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)
        
        all_results = {}
        all_analysis = {}
        
        for dataset_name in self.test_datasets.keys():
            print(f"\nğŸ“ æ•°æ®é›†: {dataset_name}")
            print("-" * 30)
            
            # è¿è¡Œå®éªŒ
            results = self.run_mock_experiment(dataset_name)
            analysis = self.analyze_results(results)
            
            # ä¿å­˜ç»“æœ
            all_results[dataset_name] = results
            all_analysis[dataset_name] = analysis
            
            # ç”Ÿæˆå›¾è¡¨
            plot_path = os.path.join(results_dir, f"{dataset_name}_performance.png")
            self.plot_results(analysis, dataset_name, plot_path)
            
            # æ‰“å°æ‘˜è¦
            self.print_summary(analysis, dataset_name)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = os.path.join(results_dir, "experiment_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        analysis_file = os.path.join(results_dir, "experiment_analysis.json")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(all_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        print(f"ğŸ“Š å®éªŒæ•°æ®: {results_file}")
        print(f"ğŸ“ˆ åˆ†ææŠ¥å‘Š: {analysis_file}")
        
        # ç”Ÿæˆæ€»ä½“å¯¹æ¯”
        self._generate_overall_comparison(all_analysis, results_dir)
        
        return all_results, all_analysis
    
    def _generate_overall_comparison(self, all_analysis: Dict, results_dir: str):
        """ç”Ÿæˆæ€»ä½“å¯¹æ¯”åˆ†æ"""
        print(f"\nğŸ¯ æ€»ä½“å®éªŒç»“è®º")
        print("=" * 50)
        
        # è®¡ç®—å„ç»„åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡è¡¨ç°
        groups = ['baseline', 'compression_only', 'rl_only', 'full_system']
        overall_scores = {}
        
        for group in groups:
            scores = []
            for dataset, analysis in all_analysis.items():
                if group in analysis:
                    scores.append(analysis[group]['overall_score'])
            overall_scores[group] = np.mean(scores) if scores else 0
        
        # æ’å
        sorted_groups = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
        
        print("ğŸ† æ€»ä½“æ€§èƒ½æ’å:")
        for i, (group, score) in enumerate(sorted_groups, 1):
            print(f"  ç¬¬{i}å: {group.upper()} - {score:.3f}")
        
        # æ”¹è¿›åˆ†æ
        baseline_score = overall_scores.get('baseline', 0)
        if baseline_score > 0:
            print(f"\nğŸ“ˆ å¹³å‡æ”¹è¿›å¹…åº¦:")
            for group, score in overall_scores.items():
                if group != 'baseline':
                    improvement = (score - baseline_score) / baseline_score * 100
                    print(f"  {group.upper()}: {improvement:+.1f}%")
        
        # ç”Ÿæˆæ€»ä½“å¯¹æ¯”å›¾
        self._plot_overall_comparison(overall_scores, results_dir)
    
    def _plot_overall_comparison(self, overall_scores: Dict, results_dir: str):
        """ç»˜åˆ¶æ€»ä½“å¯¹æ¯”å›¾"""
        if not PLOTTING_AVAILABLE:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡æ€»ä½“å¯¹æ¯”å›¾ç”Ÿæˆ")
            return
            
        groups = list(overall_scores.keys())
        scores = list(overall_scores.values())
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(groups, scores, color=colors, alpha=0.7)
        
        plt.title('å››ä¸ªå¯¹ç…§ç»„æ€»ä½“æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.ylabel('å¹³å‡å¾—åˆ†')
        plt.ylim(0, 1)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # ç¾åŒ–
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        overall_plot_path = os.path.join(results_dir, "overall_comparison.png")
        plt.savefig(overall_plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š æ€»ä½“å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {overall_plot_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å¼ºåŒ–å­¦ä¹ å¯¹è¯ç³»ç»Ÿå®éªŒå¯åŠ¨å™¨")
    print("=" * 50)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = QuickExperiment()
    
    # è¿è¡Œå®éªŒ
    try:
        results, analysis = experiment.run_full_experiment()
        print(f"\nâœ… å®éªŒå®Œæˆï¼")
        print("ğŸ“‹ å®éªŒæŠ¥å‘ŠåŒ…å«:")
        print("  - å››ä¸ªå¯¹ç…§ç»„æ€§èƒ½å¯¹æ¯”")
        print("  - å¤šä¸ªæ•°æ®é›†æµ‹è¯•ç»“æœ")
        print("  - è¯¦ç»†çš„é‡åŒ–åˆ†æ")
        print("  - å¯è§†åŒ–å›¾è¡¨")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ä¾èµ–æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…")

if __name__ == "__main__":
    main() 