"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
å®ç°å‹ç¼©æ¨¡å‹å’Œä¸»å¯¹è¯æ¨¡å‹çš„è”åˆè®­ç»ƒæ¶æ„
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from collections import deque, namedtuple
from typing import List, Dict, Tuple, Optional
import logging
import json
import os
from datetime import datetime

from config import rl_config, model_config, dialog_config
from models import model_manager
from reward_calculator import RewardCalculator

logger = logging.getLogger(__name__)

# å®šä¹‰ç»éªŒå›æ”¾ä¸­çš„è½¬æ¢
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class CompressionActionSpace:
    """å‹ç¼©åŠ¨ä½œç©ºé—´å®šä¹‰"""
    
    def __init__(self):
        # å®šä¹‰å‹ç¼©ç­–ç•¥çš„åŠ¨ä½œç©ºé—´
        self.compression_ratios = [0.2, 0.3, 0.4, 0.5]  # å‹ç¼©æ¯”ä¾‹é€‰æ‹©
        self.focus_strategies = [
            'recent_focus',      # ä¾§é‡æœ€è¿‘å¯¹è¯
            'topic_focus',       # ä¾§é‡ä¸»é¢˜ç›¸å…³
            'entity_focus',      # ä¾§é‡å®ä½“ä¿¡æ¯
            'balanced',          # å¹³è¡¡ç­–ç•¥
        ]
        self.action_dim = len(self.compression_ratios) * len(self.focus_strategies)
    
    def decode_action(self, action_idx: int) -> Dict:
        """å°†åŠ¨ä½œç´¢å¼•è§£ç ä¸ºå…·ä½“ç­–ç•¥"""
        ratio_idx = action_idx // len(self.focus_strategies)
        strategy_idx = action_idx % len(self.focus_strategies)
        
        return {
            'compression_ratio': self.compression_ratios[ratio_idx],
            'focus_strategy': self.focus_strategies[strategy_idx],
            'action_idx': action_idx
        }
    
    def sample_random_action(self) -> int:
        """éšæœºé‡‡æ ·åŠ¨ä½œ"""
        return random.randint(0, self.action_dim - 1)

class DialogState:
    """å¯¹è¯çŠ¶æ€è¡¨ç¤º"""
    
    def __init__(self, history: List[Dict], compressed_summary: str = ""):
        self.history = history.copy()
        self.compressed_summary = compressed_summary
        self.token_count = self._count_tokens()
        self.turn_count = len([h for h in history if h['role'] == 'user'])
        
    def _count_tokens(self) -> int:
        """è®¡ç®—æ€»tokenæ•°"""
        total = 0
        for turn in self.history:
            total += model_manager.count_tokens(turn['content'])
        if self.compressed_summary:
            total += model_manager.count_tokens(self.compressed_summary)
        return total
    
    def to_tensor(self, max_length: int = 2048) -> torch.Tensor:
        """å°†çŠ¶æ€è½¬æ¢ä¸ºtensorè¡¨ç¤º"""
        # ç®€åŒ–çš„çŠ¶æ€è¡¨ç¤ºï¼šä½¿ç”¨tokenåµŒå…¥çš„å¹³å‡å€¼
        all_text = ""
        if self.compressed_summary:
            all_text += self.compressed_summary + " "
        
        for turn in self.history[-5:]:  # åªå–æœ€è¿‘5è½®
            all_text += f"{turn['role']}: {turn['content']} "
        
        # ç®€åŒ–çš„çŠ¶æ€è¡¨ç¤ºï¼šä½¿ç”¨å›ºå®šé•¿åº¦å‘é‡
        # åŸºäºæ–‡æœ¬é•¿åº¦å’Œè½®æ¬¡æ•°ç”Ÿæˆç‰¹å¾å‘é‡
        features = [
            len(all_text) / 1000.0,  # æ–‡æœ¬é•¿åº¦ç‰¹å¾
            self.turn_count / 10.0,   # è½®æ¬¡æ•°ç‰¹å¾
            self.token_count / 2000.0  # tokenæ•°ç‰¹å¾
        ]
        
        # æ‰©å±•åˆ°æŒ‡å®šé•¿åº¦
        feature_vector = torch.zeros(max_length)
        for i, feat in enumerate(features[:max_length]):
            feature_vector[i] = feat
        
        return feature_vector

class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.action_space = CompressionActionSpace()
        self.reward_calculator = RewardCalculator()
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=10000)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.current_episode = 0
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = rl_config.epsilon_start
        
        # åˆ›å»ºè®­ç»ƒç›®å½•
        self.training_dir = "rl_training"
        os.makedirs(self.training_dir, exist_ok=True)
        
        logger.info("ğŸ¤– RLè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def select_compression_action(self, state: DialogState, training: bool = True) -> int:
        """é€‰æ‹©å‹ç¼©åŠ¨ä½œï¼ˆä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥ï¼‰"""
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return self.action_space.sample_random_action()
        else:
            # åˆ©ç”¨ï¼šåŸºäºå½“å‰ç­–ç•¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            return self._select_optimal_action(state)
    
    def _select_optimal_action(self, state: DialogState) -> int:
        """é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç®€åŒ–çš„ç­–ç•¥ï¼šåŸºäºå†å²é•¿åº¦å’Œtokenæ•°é€‰æ‹©
        if state.token_count > 2000:
            # é•¿å¯¹è¯ï¼Œé€‰æ‹©æ›´é«˜å‹ç¼©æ¯”
            return 0  # 0.2å‹ç¼©æ¯” + recent_focus
        elif state.token_count > 1500:
            # ä¸­ç­‰é•¿åº¦ï¼Œå¹³è¡¡å‹ç¼©
            return 7  # 0.3å‹ç¼©æ¯” + balanced
        else:
            # çŸ­å¯¹è¯ï¼Œè½»åº¦å‹ç¼©
            return 11  # 0.4å‹ç¼©æ¯” + recent_focus
    
    def execute_compression_action(self, state: DialogState, action: int) -> Tuple[str, DialogState]:
        """æ‰§è¡Œå‹ç¼©åŠ¨ä½œ"""
        action_params = self.action_space.decode_action(action)
        
        # æ„å»ºä¸“é—¨çš„å‹ç¼©æç¤º
        compression_prompt = self._build_compression_prompt(
            state.history, 
            action_params
        )
        
        # ä½¿ç”¨å‹ç¼©æ¨¡å‹ç”Ÿæˆæ‘˜è¦
        if model_manager.compressor_model is None:
            compressed_summary = "å‹ç¼©æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
        else:
            compressed_summary = model_manager.generate_text(
                model=model_manager.compressor_model,
                prompt=compression_prompt,
                max_new_tokens=int(len(state.history) * action_params['compression_ratio'] * 10)
            )
        
        # åˆ›å»ºæ–°çŠ¶æ€ï¼ˆå‹ç¼©åä¿ç•™æœ€è¿‘å‡ è½®ï¼‰
        recent_history = state.history[-dialog_config.keep_recent_turns * 2:]
        new_state = DialogState(recent_history, compressed_summary)
        
        return compressed_summary, new_state
    
    def _build_compression_prompt(self, history: List[Dict], action_params: Dict) -> str:
        """æ„å»ºå‹ç¼©æç¤º"""
        focus_instructions = {
            'recent_focus': "é‡ç‚¹ä¿ç•™æœ€è¿‘çš„å¯¹è¯å†…å®¹å’Œç”¨æˆ·å…³åˆ‡",
            'topic_focus': "é‡ç‚¹ä¿ç•™ä¸»è¦è¯é¢˜å’Œå…³é”®æ¦‚å¿µ",
            'entity_focus': "é‡ç‚¹ä¿ç•™äººåã€åœ°åã€ä¸“ä¸šæœ¯è¯­ç­‰å®ä½“ä¿¡æ¯",
            'balanced': "å¹³è¡¡ä¿ç•™å„ç±»ä¿¡æ¯ï¼Œç¡®ä¿æ‘˜è¦å®Œæ•´æ€§"
        }
        
        history_text = ""
        for turn in history:
            role = "ç”¨æˆ·" if turn['role'] == 'user' else "åŠ©æ‰‹"
            history_text += f"{role}: {turn['content']}\n"
        
        return f"""è¯·å¯¹ä»¥ä¸‹å¯¹è¯å†å²è¿›è¡Œæ™ºèƒ½å‹ç¼©æ‘˜è¦ï¼š

å¯¹è¯å†å²ï¼š
{history_text}

å‹ç¼©è¦æ±‚ï¼š
- å‹ç¼©æ¯”ä¾‹ï¼š{action_params['compression_ratio']*100:.0f}%
- ç­–ç•¥é‡ç‚¹ï¼š{focus_instructions[action_params['focus_strategy']]}
- ä¿æŒè¯­ä¹‰è¿è´¯æ€§å’Œå…³é”®ä¿¡æ¯

æ‘˜è¦ï¼š"""
    
    def calculate_reward(self, 
                        original_state: DialogState,
                        action: int,
                        compressed_summary: str,
                        dialog_response: str,
                        user_input: str) -> float:
        """è®¡ç®—å¥–åŠ±"""
        return self.reward_calculator.calculate_total_reward(
            original_state=original_state,
            action=action,
            compressed_summary=compressed_summary,
            dialog_response=dialog_response,
            user_input=user_input
        )
    
    def store_transition(self, state: DialogState, action: int, next_state: DialogState, reward: float):
        """å­˜å‚¨è½¬æ¢åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº"""
        transition = Transition(state, action, next_state, reward)
        self.memory.append(transition)
    
    def train_step(self) -> float:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        if len(self.memory) < 100:  # ç­‰å¾…è¶³å¤Ÿçš„ç»éªŒ
            return 0.0
        
        # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·
        batch_size = min(32, len(self.memory))
        transitions = random.sample(self.memory, batch_size)
        batch = Transition(*zip(*transitions))
        
        # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        rewards = torch.tensor(batch.reward, dtype=torch.float32)
        loss = F.mse_loss(rewards, torch.zeros_like(rewards))  # ç®€åŒ–çš„æŸå¤±è®¡ç®—
        
        # æ›´æ–°epsilon
        self._update_epsilon()
        
        return loss.item()
    
    def _update_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        self.epsilon = max(
            rl_config.epsilon_end,
            rl_config.epsilon_start - (self.current_episode / rl_config.epsilon_decay)
        )
    
    def train_episode(self, simulate_user_inputs: List[str]) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        episode_reward = 0.0
        episode_steps = 0
        
        # åˆå§‹åŒ–å¯¹è¯çŠ¶æ€
        current_state = DialogState([])
        
        logger.info(f"ğŸ® å¼€å§‹è®­ç»ƒEpisode {self.current_episode}")
        
        for user_input in simulate_user_inputs:
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°çŠ¶æ€
            current_state.history.append({
                'role': 'user',
                'content': user_input,
                'timestamp': datetime.now().isoformat()
            })
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©
            if current_state.token_count > dialog_config.trigger_compression_tokens:
                # é€‰æ‹©å‹ç¼©åŠ¨ä½œ
                action = self.select_compression_action(current_state, training=True)
                
                # æ‰§è¡Œå‹ç¼©
                compressed_summary, compressed_state = self.execute_compression_action(
                    current_state, action
                )
                
                # ä½¿ç”¨å‹ç¼©åçŠ¶æ€ç”Ÿæˆå›å¤
                dialog_response = self._generate_dialog_response(compressed_state, user_input)
                
                # è®¡ç®—å¥–åŠ±
                reward = self.calculate_reward(
                    current_state, action, compressed_summary, dialog_response, user_input
                )
                
                # å­˜å‚¨ç»éªŒ
                self.store_transition(current_state, action, compressed_state, reward)
                
                # æ›´æ–°çŠ¶æ€
                current_state = compressed_state
                episode_reward += reward
            else:
                # æ— éœ€å‹ç¼©ï¼Œç›´æ¥å¯¹è¯
                dialog_response = self._generate_dialog_response(current_state, user_input)
                # å°çš„æ­£å¥–åŠ±ç”¨äºæ— å‹ç¼©æƒ…å†µ
                reward = 0.1
                episode_reward += reward
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            current_state.history.append({
                'role': 'assistant',
                'content': dialog_response,
                'timestamp': datetime.now().isoformat()
            })
            
            episode_steps += 1
        
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        loss = self.train_step()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards.append(episode_reward)
        self.training_losses.append(loss)
        self.current_episode += 1
        
        episode_stats = {
            'episode': self.current_episode,
            'total_reward': episode_reward,
            'steps': episode_steps,
            'loss': loss,
            'epsilon': self.epsilon,
            'memory_size': len(self.memory)
        }
        
        logger.info(f"âœ… Episode {self.current_episode} å®Œæˆ: å¥–åŠ±={episode_reward:.3f}, æŸå¤±={loss:.6f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.current_episode % rl_config.save_checkpoint_interval == 0:
            self.save_checkpoint()
        
        return episode_stats
    
    def _generate_dialog_response(self, state: DialogState, user_input: str) -> str:
        """ç”Ÿæˆå¯¹è¯å›å¤"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = ""
        if state.compressed_summary:
            context += f"å†å²æ‘˜è¦ï¼š{state.compressed_summary}\n\n"
        
        # æ·»åŠ æœ€è¿‘å¯¹è¯
        for turn in state.history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
            role = "ç”¨æˆ·" if turn['role'] == 'user' else "åŠ©æ‰‹"
            context += f"{role}: {turn['content']}\n"
        
        context += f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
        
        # ç”Ÿæˆå›å¤
        if model_manager.dialog_model is None:
            response = "å¯¹è¯æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›å¤"
        else:
            response = model_manager.generate_text(
                model=model_manager.dialog_model,
                prompt=context,
                max_new_tokens=512
            )
        
        return response
    
    def save_checkpoint(self):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'episode': self.current_episode,
            'epsilon': self.epsilon,
            'episode_rewards': self.episode_rewards,
            'training_losses': self.training_losses,
            'memory_size': len(self.memory),
            'config': {
                'learning_rate': rl_config.learning_rate,
                'discount_factor': rl_config.discount_factor
            }
        }
        
        checkpoint_path = os.path.join(
            self.training_dir, 
            f"checkpoint_episode_{self.current_episode}.json"
        )
        
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}
        
        recent_rewards = self.episode_rewards[-100:]  # æœ€è¿‘100ä¸ªepisode
        
        return {
            'total_episodes': self.current_episode,
            'current_epsilon': self.epsilon,
            'avg_reward_recent': np.mean(recent_rewards),
            'max_reward': max(self.episode_rewards),
            'memory_utilization': len(self.memory) / 10000,
            'recent_loss': self.training_losses[-1] if self.training_losses else 0.0
        }

# å…¨å±€RLè®­ç»ƒå™¨å®ä¾‹
rl_trainer = RLTrainer() 