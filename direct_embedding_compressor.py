"""
ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹å†…éƒ¨çŠ¶æ€çš„ä¸Šä¸‹æ–‡å‹ç¼©å™¨
é€šè¿‡æå–å’Œèåˆhidden stateså®ç°é«˜æ•ˆçš„å†å²ä¿¡æ¯å‹ç¼©
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional, Union
import logging
from datetime import datetime
import json

from config import model_config, dialog_config
from models import model_manager

logger = logging.getLogger(__name__)

class HiddenStateBank:
    """å†å²hidden stateå­˜å‚¨é“¶è¡Œ"""
    
    def __init__(self, max_states: int = 50, state_dim: int = 768):
        self.max_states = max_states
        self.state_dim = state_dim
        
        # å­˜å‚¨å†å²stateså’Œå¯¹åº”çš„metadata
        self.state_bank: List[Dict] = []
        self.attention_weights = None
        
    def add_state(self, hidden_state: torch.Tensor, metadata: Dict):
        """æ·»åŠ æ–°çš„hidden state"""
        state_entry = {
            'state': hidden_state.detach().clone(),
            'metadata': metadata,
            'timestamp': datetime.now().isoformat(),
            'access_count': 0
        }
        
        self.state_bank.append(state_entry)
        
        # ç»´æŒæœ€å¤§å®¹é‡
        if len(self.state_bank) > self.max_states:
            # ç§»é™¤æœ€å°‘è®¿é—®çš„state
            self.state_bank.sort(key=lambda x: x['access_count'])
            self.state_bank = self.state_bank[1:]
    
    def retrieve_relevant_states(self, query_state: torch.Tensor, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ä¸queryæœ€ç›¸å…³çš„historical states"""
        if not self.state_bank:
            return []
        
        similarities = []
        for state_entry in self.state_bank:
            sim = F.cosine_similarity(
                query_state.unsqueeze(0), 
                state_entry['state'].unsqueeze(0)
            ).item()
            similarities.append((sim, state_entry))
        
        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(key=lambda x: x[0], reverse=True)
        relevant_states = [entry for _, entry in similarities[:top_k]]
        
        # æ›´æ–°è®¿é—®è®¡æ•°
        for entry in relevant_states:
            entry['access_count'] += 1
        
        return relevant_states
    
    def get_state_summary(self) -> Dict:
        """è·å–state bankçš„ç»Ÿè®¡æ‘˜è¦"""
        if not self.state_bank:
            return {}
        
        return {
            'total_states': len(self.state_bank),
            'avg_access_count': np.mean([s['access_count'] for s in self.state_bank]),
            'state_dimension': self.state_dim,
            'memory_usage_mb': len(self.state_bank) * self.state_dim * 4 / (1024*1024)
        }

class DirectEmbeddingCompressor:
    """ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹å†…éƒ¨çŠ¶æ€çš„å‹ç¼©å™¨"""
    
    def __init__(self):
        self.state_bank = HiddenStateBank()
        
        # ä¸Šä¸‹æ–‡èåˆç­–ç•¥
        self.fusion_strategies = {
            'attention': self._attention_fusion,
            'weighted_sum': self._weighted_sum_fusion,
            'concatenation': self._concatenation_fusion,
            'interpolation': self._interpolation_fusion
        }
        
        # å½“å‰ä½¿ç”¨çš„èåˆç­–ç•¥
        self.current_strategy = 'attention'
        
        logger.info("ğŸ¯ ç›´æ¥Embeddingå‹ç¼©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_dialog_hidden_states(self, dialog_text: str, 
                                   extract_layers: List[int] = [-1, -2, -3]) -> Dict[str, torch.Tensor]:
        """æå–å¯¹è¯æ–‡æœ¬çš„å¤šå±‚hidden states"""
        if not model_manager.dialog_model or not model_manager.tokenizer:
            logger.warning("æ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›é›¶å‘é‡")
            return {'combined': torch.zeros(768)}
        
        try:
            # Tokenizeè¾“å…¥
            tokenizer = model_manager.tokenizer
            inputs = tokenizer(  # type: ignore
                dialog_text,
                return_tensors="pt",
                truncation=True,
                max_length=1024,
                padding=True
            )
            
            # è·å–å¤šå±‚hidden states
            with torch.no_grad():
                model = model_manager.dialog_model
                outputs = model(  # type: ignore
                    **inputs,
                    output_hidden_states=True
                )
                
                hidden_states = outputs.hidden_states
                extracted_states = {}
                
                # æå–æŒ‡å®šå±‚çš„states
                for layer_idx in extract_layers:
                    layer_state = hidden_states[layer_idx]  # [batch, seq_len, hidden_dim]
                    
                    # å¤šç§poolingç­–ç•¥
                    mean_pooled = layer_state.mean(dim=1)  # [batch, hidden_dim]
                    max_pooled = layer_state.max(dim=1)[0]  # [batch, hidden_dim]
                    cls_token = layer_state[:, 0, :]  # [batch, hidden_dim] (first token)
                    
                    extracted_states[f'layer_{layer_idx}_mean'] = mean_pooled.squeeze(0)
                    extracted_states[f'layer_{layer_idx}_max'] = max_pooled.squeeze(0)
                    extracted_states[f'layer_{layer_idx}_cls'] = cls_token.squeeze(0)
                
                # åˆ›å»ºç»„åˆè¡¨ç¤º
                all_states = list(extracted_states.values())
                if all_states:
                    # æ–¹æ¡ˆ1: ç®€å•å¹³å‡
                    combined_state = torch.stack(all_states).mean(dim=0)
                    extracted_states['combined'] = combined_state
                    
                    # æ–¹æ¡ˆ2: åŠ æƒç»„åˆ (ç»™æœ€åä¸€å±‚æ›´é«˜æƒé‡)
                    weights = torch.softmax(torch.tensor([3.0, 2.0, 1.0] * 3), dim=0)
                    weighted_state = sum(w * s for w, s in zip(weights, all_states))
                    extracted_states['weighted_combined'] = weighted_state
                
                return extracted_states
                
        except Exception as e:
            logger.error(f"æå–hidden stateså¤±è´¥: {e}")
            return {'combined': torch.zeros(768)}
    
    def compress_history_to_states(self, dialog_history: List[Dict]) -> List[Dict]:
        """å°†å¯¹è¯å†å²å‹ç¼©ä¸ºhidden states"""
        compressed_states = []
        
        # æŒ‰å¯¹è¯è½®æ¬¡å¤„ç†
        for i in range(0, len(dialog_history), 2):
            if i + 1 < len(dialog_history):
                user_turn = dialog_history[i]
                assistant_turn = dialog_history[i + 1]
                
                if user_turn['role'] == 'user' and assistant_turn['role'] == 'assistant':
                    # æ„å»ºå¯¹è¯æ–‡æœ¬
                    dialog_text = f"ç”¨æˆ·: {user_turn['content']}\nåŠ©æ‰‹: {assistant_turn['content']}"
                    
                    # æå–hidden states
                    hidden_states = self.extract_dialog_hidden_states(dialog_text)
                    
                    # åˆ›å»ºçŠ¶æ€æ¡ç›®
                    state_entry = {
                        'states': hidden_states,
                        'user_input': user_turn['content'],
                        'assistant_response': assistant_turn['content'],
                        'turn_index': i // 2,
                        'token_count': len(dialog_text),
                        'summary': user_turn['content'][:50] + "..."
                    }
                    
                    compressed_states.append(state_entry)
                    
                    # æ·»åŠ åˆ°state bank
                    self.state_bank.add_state(
                        hidden_states['combined'],
                        {
                            'turn_index': i // 2,
                            'summary': state_entry['summary'],
                            'user_input': user_turn['content']
                        }
                    )
        
        return compressed_states
    
    def _attention_fusion(self, query_state: torch.Tensor, 
                         context_states: List[torch.Tensor]) -> torch.Tensor:
        """ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èåˆå†å²states"""
        if not context_states:
            return query_state
        
        # Stack context states
        context_matrix = torch.stack(context_states)  # [num_contexts, hidden_dim]
        
        # è®¡ç®—attention weights
        attention_scores = torch.matmul(
            query_state.unsqueeze(0),  # [1, hidden_dim]
            context_matrix.transpose(0, 1)  # [hidden_dim, num_contexts]
        )  # [1, num_contexts]
        
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # åŠ æƒèåˆ
        fused_context = torch.matmul(attention_weights, context_matrix).squeeze(0)
        
        # ä¸query stateç»“åˆ
        enhanced_state = 0.7 * query_state + 0.3 * fused_context
        
        return enhanced_state
    
    def _weighted_sum_fusion(self, query_state: torch.Tensor, 
                           context_states: List[torch.Tensor]) -> torch.Tensor:
        """åŠ æƒæ±‚å’Œèåˆ"""
        if not context_states:
            return query_state
        
        # æ ¹æ®ç›¸ä¼¼åº¦è®¡ç®—æƒé‡
        weights = []
        for ctx_state in context_states:
            sim = F.cosine_similarity(query_state, ctx_state, dim=0)
            weights.append(sim)
        
        weights = F.softmax(torch.tensor(weights), dim=0)
        
        # åŠ æƒæ±‚å’Œ
        weighted_context = sum(w * ctx for w, ctx in zip(weights, context_states))
        
        return 0.6 * query_state + 0.4 * weighted_context
    
    def _concatenation_fusion(self, query_state: torch.Tensor, 
                            context_states: List[torch.Tensor]) -> torch.Tensor:
        """æ‹¼æ¥èåˆï¼ˆéœ€è¦é™ç»´ï¼‰"""
        if not context_states:
            return query_state
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å‡ ä¸ªcontext states
        top_states = context_states[:3]  # æœ€å¤š3ä¸ª
        
        # æ‹¼æ¥
        all_states = [query_state] + top_states
        concatenated = torch.cat(all_states, dim=0)
        
        # é™ç»´åˆ°åŸå§‹ç»´åº¦ï¼ˆç®€å•çš„çº¿æ€§å˜æ¢ï¼‰
        hidden_dim = query_state.shape[0]
        current_dim = concatenated.shape[0]
        
        if current_dim > hidden_dim:
            # ç®€å•çš„åˆ†å—å¹³å‡
            chunks = concatenated.chunk(current_dim // hidden_dim + 1)
            reduced = torch.stack(chunks[:-1] if len(chunks[-1]) < hidden_dim else chunks).mean(dim=0)
            return reduced[:hidden_dim]
        
        return concatenated
    
    def _interpolation_fusion(self, query_state: torch.Tensor, 
                            context_states: List[torch.Tensor]) -> torch.Tensor:
        """æ’å€¼èåˆ"""
        if not context_states:
            return query_state
        
        # è®¡ç®—context statesçš„ä¸­å¿ƒ
        context_center = torch.stack(context_states).mean(dim=0)
        
        # æ’å€¼å‚æ•°åŸºäºç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(query_state, context_center, dim=0)
        alpha = torch.sigmoid(similarity)  # è‡ªé€‚åº”æ’å€¼æƒé‡
        
        return alpha * query_state + (1 - alpha) * context_center
    
    def generate_enhanced_context(self, current_input: str, 
                                max_context_tokens: int = 1500) -> Tuple[str, Dict]:
        """ç”Ÿæˆå¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼ˆèåˆå†å²statesï¼‰"""
        # æå–å½“å‰è¾“å…¥çš„hidden state
        current_text = f"ç”¨æˆ·: {current_input}"
        current_states = self.extract_dialog_hidden_states(current_text)
        query_state = current_states['combined']
        
        # æ£€ç´¢ç›¸å…³çš„å†å²states
        relevant_entries = self.state_bank.retrieve_relevant_states(query_state, top_k=5)
        
        if not relevant_entries:
            # æ— å†å²ä¿¡æ¯ï¼Œç›´æ¥è¿”å›
            return f"ç”¨æˆ·: {current_input}\nåŠ©æ‰‹:", {'fusion_used': False}
        
        # æå–ç›¸å…³çš„states
        context_states = [entry['state'] for entry in relevant_entries]
        
        # ä½¿ç”¨å½“å‰ç­–ç•¥èåˆstates
        fusion_func = self.fusion_strategies[self.current_strategy]
        enhanced_state = fusion_func(query_state, context_states)
        
        # å°†èåˆåçš„stateè½¬æ¢å›è‡ªç„¶è¯­è¨€æè¿°
        context_description = self._state_to_description(enhanced_state, relevant_entries)
        
        # æ„å»ºæœ€ç»ˆçš„prompt
        enhanced_prompt = f"""åŸºäºå†å²ä¸Šä¸‹æ–‡çš„å¯¹è¯ï¼š

{context_description}

å½“å‰å¯¹è¯ï¼š
ç”¨æˆ·: {current_input}
åŠ©æ‰‹:"""
        
        # æ£€æŸ¥é•¿åº¦é™åˆ¶
        if model_manager.count_tokens(enhanced_prompt) > max_context_tokens:
            # ç®€åŒ–ä¸Šä¸‹æ–‡
            simplified_context = self._simplify_context(context_description, max_context_tokens // 2)
            enhanced_prompt = f"""å†å²æ‘˜è¦: {simplified_context}

ç”¨æˆ·: {current_input}
åŠ©æ‰‹:"""
        
        metadata = {
            'fusion_used': True,
            'fusion_strategy': self.current_strategy,
            'num_context_states': len(context_states),
            'relevant_turns': [entry['metadata']['summary'] for entry in relevant_entries],
            'enhanced_state_norm': torch.norm(enhanced_state).item()
        }
        
        return enhanced_prompt, metadata
    
    def _state_to_description(self, fused_state: torch.Tensor, 
                            relevant_entries: List[Dict]) -> str:
        """å°†èåˆçš„stateè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
        # æ–¹æ¡ˆ1: åŸºäºç›¸å…³æ¡ç›®çš„å…ƒæ•°æ®
        summaries = []
        for entry in relevant_entries[:3]:  # æœ€å¤š3ä¸ª
            summary = entry['metadata']['summary']
            summaries.append(f"- {summary}")
        
        # æ–¹æ¡ˆ2: åŸºäºstateçš„"æ¿€æ´»æ¨¡å¼"
        state_activation = torch.sigmoid(fused_state)
        high_activation_dims = (state_activation > 0.7).sum().item()
        activation_pattern = "é«˜åº¦ç›¸å…³" if high_activation_dims > 100 else "éƒ¨åˆ†ç›¸å…³"
        
        description = f"""ç›¸å…³å†å²ä¿¡æ¯ ({activation_pattern}):
{chr(10).join(summaries)}"""
        
        return description
    
    def _simplify_context(self, context: str, max_length: int) -> str:
        """ç®€åŒ–ä¸Šä¸‹æ–‡æè¿°"""
        if len(context) <= max_length:
            return context
        
        lines = context.split('\n')
        simplified = []
        current_length = 0
        
        for line in lines:
            if current_length + len(line) <= max_length:
                simplified.append(line)
                current_length += len(line)
            else:
                break
        
        return '\n'.join(simplified) + "..."
    
    def switch_fusion_strategy(self, strategy: str):
        """åˆ‡æ¢èåˆç­–ç•¥"""
        if strategy in self.fusion_strategies:
            self.current_strategy = strategy
            logger.info(f"åˆ‡æ¢åˆ°èåˆç­–ç•¥: {strategy}")
        else:
            logger.warning(f"æœªçŸ¥çš„èåˆç­–ç•¥: {strategy}")
    
    def get_compression_statistics(self) -> Dict:
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        bank_summary = self.state_bank.get_state_summary()
        
        stats = {
            'state_bank_info': bank_summary,
            'current_fusion_strategy': self.current_strategy,
            'available_strategies': list(self.fusion_strategies.keys()),
            'compression_efficiency': self._calculate_compression_efficiency()
        }
        
        return stats
    
    def _calculate_compression_efficiency(self) -> Dict:
        """è®¡ç®—å‹ç¼©æ•ˆç‡"""
        if not self.state_bank.state_bank:
            return {}
        
        # ä¼°ç®—åŸå§‹æ–‡æœ¬å¤§å° vs stateå¤§å°
        total_original_chars = sum(
            len(entry['metadata'].get('user_input', '')) 
            for entry in self.state_bank.state_bank
        )
        
        total_state_memory = len(self.state_bank.state_bank) * self.state_bank.state_dim * 4  # bytes
        original_memory = total_original_chars * 2  # å‡è®¾UTF-8ç¼–ç 
        
        return {
            'compression_ratio': total_state_memory / original_memory if original_memory > 0 else 1.0,
            'memory_savings_mb': (original_memory - total_state_memory) / (1024*1024),
            'states_per_mb': len(self.state_bank.state_bank) / (total_state_memory / (1024*1024))
        }
    
    def save_state_bank(self, filepath: str):
        """ä¿å­˜state bankåˆ°æ–‡ä»¶"""
        serializable_data = []
        for entry in self.state_bank.state_bank:
            serializable_entry = {
                'state': entry['state'].detach().cpu().numpy().tolist(),
                'metadata': entry['metadata'],
                'timestamp': entry['timestamp'],
                'access_count': entry['access_count']
            }
            serializable_data.append(serializable_entry)
        
        save_data = {
            'state_bank': serializable_data,
            'fusion_strategy': self.current_strategy,
            'save_timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"State bankå·²ä¿å­˜åˆ°: {filepath}")
    
    def load_state_bank(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½state bank"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # é‡å»ºstate bank
            self.state_bank.state_bank = []
            for entry_data in data['state_bank']:
                entry = {
                    'state': torch.tensor(entry_data['state']),
                    'metadata': entry_data['metadata'],
                    'timestamp': entry_data['timestamp'],
                    'access_count': entry_data['access_count']
                }
                self.state_bank.state_bank.append(entry)
            
            # æ¢å¤èåˆç­–ç•¥
            if 'fusion_strategy' in data:
                self.current_strategy = data['fusion_strategy']
            
            logger.info(f"ä» {filepath} åŠ è½½äº† {len(self.state_bank.state_bank)} ä¸ªstates")
        
        except Exception as e:
            logger.error(f"åŠ è½½state bankå¤±è´¥: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
direct_compressor = DirectEmbeddingCompressor() 