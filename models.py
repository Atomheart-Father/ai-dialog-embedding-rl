"""
æ¨¡å‹åŠ è½½å’Œç®¡ç†æ¨¡å—
æ”¯æŒMPSåŠ é€Ÿå’Œé‡åŒ–ä¼˜åŒ–
"""
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    GenerationConfig
)
try:
    from transformers import BitsAndBytesConfig
except ImportError:
    BitsAndBytesConfig = None
from typing import Optional
import os
import logging
from config import model_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼Œè´Ÿè´£åŠ è½½å’Œç®¡ç†åŒæ¨¡å‹"""

    def __init__(self):
        self.device = model_config.device
        self.tokenizer: Optional[AutoTokenizer] = None
        self.compressor_model: Optional[AutoModelForCausalLM] = None
        self.dialog_model: Optional[AutoModelForCausalLM] = None
        self.generation_config = None

    def load_models(self) -> bool:
        """åŠ è½½åŒæ¨¡å‹"""
        try:
            logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")

            # åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
            os.makedirs(model_config.model_cache_dir, exist_ok=True)

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.model_name,
                cache_dir=model_config.model_cache_dir,
                trust_remote_code=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer and hasattr(self.tokenizer, 'pad_token') and self.tokenizer.pad_token is None:
                if hasattr(self.tokenizer, 'eos_token'):
                    self.tokenizer.pad_token = self.tokenizer.eos_token

            # é…ç½®é‡åŒ–
            quantization_config = None
            if model_config.use_quantization and model_config.device != "mps" and BitsAndBytesConfig is not None:
                # MPSæš‚ä¸æ”¯æŒBitsAndBytesConfigï¼Œä½¿ç”¨torché‡åŒ–
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )

            # åŠ è½½å‹ç¼©æ¨¡å‹
            logger.info("ğŸ“¦ åŠ è½½å‹ç¼©æ¨¡å‹...")
            self.compressor_model = AutoModelForCausalLM.from_pretrained(
                model_config.model_name,
                cache_dir=model_config.model_cache_dir,
                torch_dtype=torch.float16 if model_config.device != "cpu" else torch.float32,
                quantization_config=quantization_config,
                trust_remote_code=True,
                device_map="auto" if model_config.device == "cuda" else None,
                low_cpu_mem_usage=True
            )

            # åŠ è½½ä¸»å¯¹è¯æ¨¡å‹
            logger.info("ğŸ’¬ åŠ è½½ä¸»å¯¹è¯æ¨¡å‹...")
            self.dialog_model = AutoModelForCausalLM.from_pretrained(
                model_config.model_name,
                cache_dir=model_config.model_cache_dir,
                torch_dtype=torch.float16 if model_config.device != "cpu" else torch.float32,
                quantization_config=quantization_config,
                trust_remote_code=True,
                device_map="auto" if model_config.device == "cuda" else None,
                low_cpu_mem_usage=True
            )

            # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆé’ˆå¯¹MPSï¼‰
            if model_config.device == "mps":
                if self.compressor_model and hasattr(self.compressor_model, 'to'):
                    self.compressor_model = self.compressor_model.to(self.device)
                if self.dialog_model and hasattr(self.dialog_model, 'to'):
                    self.dialog_model = self.dialog_model.to(self.device)

            # è®¾ç½®ç”Ÿæˆé…ç½® - ä¼˜åŒ–å‚æ•°é¿å…é‡å¤
            if self.tokenizer:
                self.generation_config = GenerationConfig(
                    max_length=model_config.max_length,
                    temperature=0.8,  # ç¨å¾®æé«˜åˆ›é€ æ€§
                    top_p=0.9,
                    top_k=40,
                    do_sample=True,
                    repetition_penalty=1.1,  # å‡å°‘é‡å¤
                    no_repeat_ngram_size=3,  # é¿å…3-gramé‡å¤
                    pad_token_id=getattr(self.tokenizer, 'pad_token_id', None),
                    eos_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                )

            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬tokenæ•°é‡"""
        if not self.tokenizer:
            return 0
        return len(self.tokenizer.encode(text))

    def generate_text(self, model: AutoModelForCausalLM, prompt: str, max_new_tokens: int = 512) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            if not self.tokenizer or not model:
                return ""

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=model_config.max_length - max_new_tokens
            ).to(self.device)

            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    generation_config=self.generation_config,
                    max_new_tokens=max_new_tokens,
                    pad_token_id=getattr(self.tokenizer, 'pad_token_id', None),
                    do_sample=True
                )

            # è§£ç è¾“å‡ºï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            input_length = inputs.input_ids.shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

            return response.strip()

        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def cleanup(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.compressor_model:
            del self.compressor_model
        if self.dialog_model:
            del self.dialog_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            torch.mps.empty_cache()


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()
